<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>炼丹实验室 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/easyml</link><description>会定期更新一些我的深度学习和机器学习相关的实践心得。</description><lastBuildDate>Wed, 21 Sep 2016 14:15:52 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>深度学习模型使用word2vec向量的方法总结</title><link>https://zhuanlan.zhihu.com/p/22018256</link><description>&lt;p&gt;使用word2vec工具在大规模外部文本语料上训练得到的向量，可以比较精确的衡量词之间的相关程度。一个比较简单的应用，就是利用词之间的向量的cos得分，来找相关词。同时word2vec向量，也可以用于深度学习模型的训练，使深度学习模型可以利用这种相关性，从而提高收敛速度和最终结果。但是实际使用的时候，有很多方式可供选择。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;直接用word2vec向量初始化模型embedding,训练的时候允许embedding向量更新。 这个方法最为常用，但是遇到不在训练语料中的词，就不能借助外部word2vec向量了。&lt;/li&gt;&lt;li&gt;word2vec向量，先连接全连接层（可以是多层），转化后的向量再作为模型的embedding,训练的时候，word2vec向量保持不变，允许全连接层的参数更新。 这个方法，哪怕遇到不在训练语料中的词，只要这个词在外部大规模语料中，能得到word2vec向量，那么就没问题。同时因为word2vec向量在训练的时候固定，因此模型训练涉及的参数会大大减少。 因为word2vec向量的分布，和模型实际需要的向量分布，可能存在差异，因此这个全连接层的作用，就是对word2vec向量的分布进行调整，让他尽可能接近模型需要的向量分布。&lt;/li&gt;&lt;li&gt;将word2vec向量拷贝，得到向量A和向量B，训练的时候，向量A保持不变，允许向量B的参数更新，最终embedding向量是A和B的平均。 具体请参考&lt;a href="https://arxiv.org/abs/1408.5882"&gt;https://arxiv.org/abs/1408.5882&lt;/a&gt;这个idea的想法，其实是限制word2vec向量的调整，避免调整的时候，太偏离原始向量。&lt;/li&gt;&lt;li&gt;&lt;ol&gt;&lt;li&gt;第一轮训练，用word2vec向量初始化embedding,对未知词随机初始化embedding,在训练的时候,固定住word2vec初始化的embedding,而允许未知词的embedding进行调整&lt;/li&gt;&lt;li&gt;第二轮训练，允许所有embedding调整,继续训练 这个idea也可以很好的处理未知词，第一轮的时候，因为固定了word2vec向量，因此模型会尽可能基于word2vec向量的分布来调整自己的参数。但是可能分布差异太大，导致模型参数无论怎么调整，都得不到最好结果。因此第二轮的时候，允许word2vec向量进行适当调整。具体请参考&lt;a href="https://arxiv.org/abs/1507.04808"&gt;https://arxiv.org/abs/1507.04808&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;参考论文&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/1507.04808" data-title="2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models" class="" data-editable="true"&gt;2016-AAAI-Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://arxiv.org/abs/1408.5882" data-title="2014-EMNLP-Convolutional Neural Networks for Sentence Classification" class="" data-editable="true"&gt;2014-EMNLP-Convolutional Neural Networks for Sentence Classification&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22018256&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Mon, 15 Aug 2016 12:52:55 GMT</pubDate></item><item><title>深度学习网络调试技巧</title><link>https://zhuanlan.zhihu.com/p/20792837</link><description>&lt;p&gt;神经网络的代码，比一般的代码要难调试不少，和编译错误以及运行时程序崩溃相比，神经网络比较棘手的地方，往往在于程序运行正常，但是结果无法收敛，这个检查起来可要麻烦多了。下面是根据我平时调试神经网络的经验，总结的一些比较通用的调试技巧，后续会再写一篇文章，专门介绍一下theano如何进行调试，希望能对大家调试神经网络有所帮助。&lt;/p&gt;&lt;h1&gt;遇到Nan怎么办？&lt;/h1&gt;&lt;p&gt;Nan问题，我相信大部分人都遇到过，一般可能是下面几个原因造成的：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;除0问题。这里实际上有两种可能，一种是被除数的值是无穷大，即Nan，另一种就是除数的值是0。之前产生的Nan或者0，有可能会被传递下去，造成后面都是Nan。请先检查一下神经网络中有可能会有除法的地方，例如softmax层，再认真的检查一下数据。我有一次帮别人调试代码，甚至还遇到过，训练数据文件中，有些值就是Nan。。。这样读进来以后，开始训练，只要遇到Nan的数据，后面也就Nan了。可以尝试加一些日志，把神经网络的中间结果输出出来，看看哪一步开始出现Nan。后面会介绍Theano的处理办法。&lt;/li&gt;&lt;li&gt;梯度过大，造成更新后的值为Nan。特别是RNN，有可能会有梯度爆炸的问题。一般有以下几个解决办法。&lt;ol&gt;&lt;li&gt;对梯度做clip(梯度裁剪），限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15。&lt;/li&gt;&lt;li&gt;减少学习率。初始学习率过大，也有可能造成这个问题。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;初始参数值过大，也有可能出现Nan问题。输入和输出的值，最好也做一下归一化。具体方法可以参考我之前的一篇文章：&lt;a href="http://zhuanlan.zhihu.com/p/20767428" class="" data-editable="true" data-title="深度学习个人炼丹心得 - 炼丹实验室 - 知乎专栏"&gt;深度学习个人炼丹心得 - 炼丹实验室 - 知乎专栏&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;神经网络学不出东西怎么办？&lt;/h1&gt;&lt;p&gt;可能我们并没有遇到，或者解决了Nan等问题，网络一直在正常的训练，但是cost降不下来，预测的时候，结果不正常。&lt;/p&gt;&lt;ol&gt;&lt;li&gt;请打印出训练集的cost值和测试集上cost值的变化趋势，正常情况应该是训练集的cost值不断下降，最后趋于平缓，或者小范围震荡，测试集的cost值先下降，然后开始震荡或者慢慢上升。如果训练集cost值不下降，有可能是代码有bug，有可能是数据有问题（本身有问题，数据处理有问题等等），有可能是超参（网络大小，层数，学习率等）设置的不合理。 请人工构造10条数据，用神经网络反复训练，看看cost是否下降，如果还不下降，那么可能网络的代码有bug，需要认真检查了。如果cost值下降，在这10条数据上做预测，看看结果是不是符合预期。那么很大可能网络本身是正常的。那么可以试着检查一下超参和数据是不是有问题。&lt;/li&gt;&lt;li&gt;如果神经网络代码，全部是自己实现的，那么强烈建议做梯度检查。确保梯度计算没有错误。&lt;/li&gt;&lt;li&gt;先从最简单的网络开始实验，不要仅仅看cost值，还要看一看神经网络的预测输出是什么样子，确保能跑出预期结果。例如做语言模型实验的时候，先用一层RNN，如果一层RNN正常，再尝试LSTM，再进一步尝试多层LSTM。&lt;/li&gt;&lt;li&gt;如果可能的话，可以输入一条指定数据，然后自己计算出每一步正确的输出结果，再检查一下神经网络每一步的结果，是不是一样的。&lt;/li&gt;&lt;/ol&gt;&lt;h1&gt;参考资料&lt;/h1&gt;&lt;p&gt;&lt;a href="http://russellsstewart.com/notes/0.html" data-editable="true" data-title="russellsstewart.com 的页面"&gt;http://russellsstewart.com/notes/0.html&lt;/a&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20792837&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Sat, 23 Apr 2016 13:26:14 GMT</pubDate></item><item><title>深度学习个人调参训练心得</title><link>https://zhuanlan.zhihu.com/p/20767428</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/1c714563438bfc3cf3f2358cb58d8967_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;新开了一个专栏，为什么叫炼丹实验室呢，因为以后会在这个专栏里分享一些关于深度学习相关的实战心得，而深度学习很多人称它为玄学，犹如炼丹一般。不过即使是炼丹也是可以摸索出一些经验规律的，希望和各位炼丹术士一起多多交流。&lt;/p&gt;训练技巧对深度学习来说是非常重要的，作为一门实验性质很强的科学，同样的网络结构使用不同的训练方法训练，结果可能会有很大的差异。这里我总结了近一年来的炼丹心得，分享给大家，也欢迎大家补充指正。&lt;ol&gt;&lt;li&gt;&lt;p&gt;参数初始化,下面几种方式,随便选一个,结果基本都差不多.&lt;/p&gt;&lt;/li&gt;&lt;ol&gt;&lt;li&gt;uniform W = np.random.uniform(low=-scale, high=scale, size=shape)&lt;/li&gt;&lt;li&gt;glorot_uniform scale = np.sqrt(6. / (shape[0] + shape[1])) np.random.uniform(low=-scale, high=scale, size=shape)&lt;/li&gt;&lt;li&gt;高斯初始化: w = np.random.randn(n) / sqrt(n),n为参数数目 激活函数为relu的话,推荐 w = np.random.randn(n) * sqrt(2.0/n)&lt;/li&gt;&lt;li&gt;svd ,对RNN效果比较好,可以有效提高收敛速度.&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;&lt;p&gt;数据预处理方式&lt;/p&gt;&lt;ol&gt;&lt;li&gt;zero-center ,这个挺常用的.X -= np.mean(X, axis = 0) # zero-center X /= np.std(X, axis = 0) # normalize&lt;/li&gt;&lt;li&gt;PCA whitening,这个用的比较少.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;训练技巧&lt;/p&gt;&lt;ol&gt;&lt;li&gt;要做梯度归一化,即算出来的梯度除以minibatch size&lt;/li&gt;&lt;li&gt;clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值,就算一个衰减系系数,让value的值等于阈值: 5,10,15&lt;/li&gt;&lt;li&gt;dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd在我的大部分实验中，效果提升都非常明显.因此可能的话，建议一定要尝试一下。 dropout的位置比较有讲究, 对于RNN,建议放到输入-&amp;gt;RNN与RNN-&amp;gt;输出的位置.关于RNN如何用dropout,可以参考这篇论文:&lt;a href="https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1409.2329" class="" data-editable="true" data-title="http://arxiv.org/abs/1409.2329"&gt;http://arxiv.org/abs/1409.2329&lt;/a&gt;&lt;/li&gt;&lt;li&gt;adam,adadelta等,在小数据上,我这里实验的效果不如sgd, sgd收敛速度会慢一些，但是最终收敛后的结果，一般都比较好。如果使用sgd的话,可以选择从1.0或者0.1的学习率开始,隔一段时间,在验证集上检查一下,如果cost没有下降,就对学习率减半. 我看过很多论文都这么搞,我自己实验的结果也很好. 当然,也可以先用ada系列先跑,最后快收敛的时候,更换成sgd继续训练.同样也会有提升.&lt;/li&gt;&lt;li&gt;除了gate之类的地方,需要把输出限制成0-1之外,尽量不要用sigmoid,可以用tanh或者relu之类的激活函数.&lt;/li&gt;&lt;li&gt;rnn的dim和embdding size,一般从128上下开始调整. batch size,一般从128左右开始调整.batch size合适最重要,并不是越大越好.&lt;/li&gt;&lt;li&gt;word2vec初始化,在小数据上,不仅可以有效提高收敛速度,也可以可以提高结果.&lt;/li&gt;&lt;li&gt;尽量对数据做shuffle&lt;/li&gt;&lt;li&gt;LSTM 的forget gate的bias,用1.0或者更大的值做初始化,可以取得更好的结果,来自这篇论文:&lt;a href="https://link.zhihu.com/?target=http%3A//jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" class="" data-editable="true" data-title="http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf"&gt;http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf&lt;/a&gt;, 我这里实验设成1.0,可以提高收敛速度.实际使用中,不同的任务,可能需要尝试不同的值.&lt;/li&gt;&lt;li&gt;Batch Normalization据说可以提升效果，不过我没有尝试过，建议作为最后提升模型的手段，参考论文：&lt;a href="http://arxiv.org/abs/1502.03167" class=""&gt;http://arxiv.org/abs/1502.03167&lt;/a&gt;&lt;/li&gt;&lt;li&gt;如果你的模型包含全连接层（MLP），并且输入和输出大小一样，可以考虑将MLP替换成Highway Network,我尝试对结果有一点提升，建议作为最后提升模型的手段，原理很简单，就是给输出加了一个gate来控制信息的流动，详细介绍请参考论文: &lt;a href="http://arxiv.org/abs/1505.00387"&gt;http://arxiv.org/abs/1505.00387&lt;/a&gt;&lt;/li&gt;&lt;li&gt;来自&lt;a href="https://www.zhihu.com/people/00c38786ac1d4d806996ee10e8b2912a" data-hash="00c38786ac1d4d806996ee10e8b2912a" class="member_mention" data-editable="true" data-title="@张馨宇" data-hovercard="p$b$00c38786ac1d4d806996ee10e8b2912a"&gt;@张馨宇&lt;/a&gt;的技巧：一轮加正则，一轮不加正则，反复进行。&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;Ensemble: 论文刷结果的终极核武器,深度学习中一般有以下几种方式&lt;/p&gt;&lt;ol&gt;&lt;li&gt;同样的参数,不同的初始化方式&lt;/li&gt;&lt;li&gt;不同的参数,通过cross-validation,选取最好的几组&lt;/li&gt;&lt;li&gt;同样的参数,模型训练的不同阶段，即不同迭代次数的模型。&lt;/li&gt;&lt;li&gt;不同的模型,进行线性融合. 例如RNN和传统模型.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/20767428&amp;pixel&amp;useReferer"/&gt;</description><author>萧瑟</author><pubDate>Mon, 18 Apr 2016 15:45:37 GMT</pubDate></item></channel></rss>