<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Datartisan数据工匠 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/datartisan</link><description></description><lastBuildDate>Wed, 23 Nov 2016 16:16:49 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>机器学习系列-word2vec篇</title><link>https://zhuanlan.zhihu.com/p/23733638</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-922b676116eecb65136750c1f8b41173_r.png"&gt;&lt;/p&gt;作者：向日葵&lt;h3&gt;开篇&lt;/h3&gt;&lt;p&gt;深度学习方向当下如火如荼，就差跑进楼下大妈的聊天内容了。深度学习的宝藏很多，一个小领域的一段小代码，都可以发出璀璨的光芒。如果你也刚刚踏入这方向，一开始难免有一些彷徨，但慢慢会有，清晨入古寺 初日照高林，那种博大的体验。&lt;/p&gt;&lt;p&gt;word2vec就是这样的一小段代码，如果你对word2vec的代码了如指掌，那你可以直接return。这是一篇关于word2vec介绍的文章，读完以后你会欣喜的发现自己会灵活的使用word2vec，但你也可能会郁闷，因为还是会觉得像是盲人摸象一样，完全对深度学习没有一点头绪。没关系，谁不是这样一点一滴的积累起来的呢。&lt;/p&gt;&lt;h3&gt;从需求入门&lt;/h3&gt;&lt;p&gt;美国大选刚刚落幕，川普胜出。假设反过来想，给你一个川普的词，你会联想到哪些？正常的话，应该是美国、大选、希拉里、奥巴马；也就是相似词语的选取了。对于相识词的选取，算法非常的多。也有许多人用了很简单的办法就能求得两样东西的相似性，比如购物车里物品的相似度，最简单的办法就是看看同时买了这样东西的用户还同时买了什么，用简单的数据结构就很容易实现这样的一个算法。这种算法很简单也很方便，但就是这种简单而使他忽略了很多的问题。例如时间顺序，下面会有提到。&lt;/p&gt;&lt;p&gt;还是回归到相识度的问题。归结到数学问题上，最经常用的是把每个词都归结到一个坐标系下，再用距离公式（如：皮尔逊公式）可方便的求出各个词语之间的相识度。&lt;/p&gt;&lt;p&gt;这也是word2vec的方法，word2vec 通过训练，可以把对文本内容的处理简化为 K 维向量空间中的向量运算，而向量空间上的相似度可以用来表示文本语义上的相似度。 如图，下面是有五维向量空间的单词：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0c7b4498252abb13418c59d22b08a1f2.png" data-rawwidth="976" data-rawheight="443"&gt;算法的关键步骤就是如何求出词语的向量空间。 &lt;/p&gt;&lt;h3&gt;word2vec算法介绍&lt;/h3&gt;&lt;p&gt;word2vec是2013年Google中开源的一款工具。2013年神经网络的各种算法都已经相当的成熟了，word2vec核心是神经网络的方法，采用 CBOW（Continuous Bag-Of-Words，即连续的词袋模型）和 Skip-Gram 两种模型，将词语映像到同一坐标系，得出数值向量的高效工具。&lt;/p&gt;&lt;p&gt;一般来说算法采用神经网络的话，要注意他的输入和输出。因为使用神经网络进行训练需要有输入和输出，输入通过神经网络后，通过和输入对比，进行神经网络的重新调整，达到训练网络的目的。抓住输入输出就能够很好的理解神经网络的算法过程。&lt;/p&gt;&lt;p&gt;语言模型采用神经网络，就要判断什么东西要作为输入，什么东西要作为输出。这是算法可以创新的地方，语言模型有许多种，大部分的原理也是采用根据上下文，来推测这个词的概率。&lt;/p&gt;&lt;p&gt;word2vec输入输出也算是鬼斧神功，算法跟哈夫曼树有关系。哈夫曼树可以比较准确的表达这边文章的结构。&lt;/p&gt;&lt;p&gt;a,b,c,d分别表示不同词，并附加找个词出现的频率，这些词就能有自己的路径和编码。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-922b676116eecb65136750c1f8b41173.png" data-rawwidth="517" data-rawheight="291"&gt;&lt;p&gt;关于哈夫曼树就不仔细详细说明了，他是一种压缩算法，能很好的保持一篇文章的特性。&lt;/p&gt;&lt;p&gt;训练的过程是，把每一段落取出来，每个词都通过哈夫曼树对应的路径和编码。编码是(0和1)，作为神经网络的输出，每个路径初始化一个给定维数的向量，跟自己段落中的每个词作为输入，进行反向的迭代，就可以训练出参数。&lt;/p&gt;&lt;p&gt;这就是算法的整个过程。&lt;/p&gt;&lt;h3&gt;快速入门&lt;/h3&gt;&lt;ol&gt;&lt;li&gt;代码下载： &lt;a href="http://word2vec.googlecode.com/svn/trunk/" data-editable="true" data-title="googlecode.com 的页面" class=""&gt;http://word2vec.googlecode.com/svn/trunk/&lt;/a&gt;&lt;/li&gt;&lt;li&gt;针对个人需求修改 makefile 文件，比如作者使用的 linux 系统就需要把 makefile 编译选项中的-Ofast 要更改为-O2 或者-g（调试时用）， 同时删除编译器无法辨认的-march=native 和-Wno-unused-result 选项。 有些系统可能还需要修改相关的 c 语言头文件，具体网上搜搜应该可以解决。&lt;/li&gt;&lt;li&gt;&lt;p&gt;运行“ make”编译 word2vec 工具。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;运行 demo 脚本： ./demo-word.sh&lt;/p&gt;&lt;p&gt;demo-word.sh主要工作为：&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;&lt;ul&gt;&lt;li&gt;1）编译（ make）&lt;/li&gt;&lt;li&gt;2）下载训练数据 text8，如果不存在。 text8 中为一些空格隔开的英文单词，但不含标点符号，一共有 1600 多万个单词。&lt;/li&gt;&lt;li&gt;3）训练，大概一个小时左右，取决于机器配置&lt;/li&gt;&lt;li&gt;4）调用distance，查找最近的词&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;python版本的命令如下：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Python的命令为python word2vec.py -train tx -model vb -cbow 0 -negative 0 -dim 5&lt;/p&gt;&lt;/blockquote&gt;&lt;h3&gt;应用&lt;/h3&gt;&lt;p&gt;word2vec是根据文章中每个词的上下关系，把每个词的关系映射到同一坐标系下，构成了一个大矩阵，矩阵下反映了每个词的关系。这些词的关系是通过上下文相关得出来的，它具有前后序列性，而Word2vec同时采用了哈夫曼的压缩算法，对是一些热门词进行了很好的降权处理。因此他在做一些相似词，或者词语的扩展都有很好的效果。&lt;/p&gt;&lt;p&gt;这种相识性还可以用在，物品的推荐上，根据用户购买物品的顺序，把每个物品当成一个单词，相当于一门外语了，谁也看不懂而已，但里面放映了上下文的关系，这个是很重要的，也是我们一开头那种普通算法无法做到的，同时对一些热门的物品自然有降权的处理，非常的方便。&lt;/p&gt;&lt;p&gt;word2vec自然规避了两大问题：词语的次序和热门词语的降权处理。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23733638&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 17 Nov 2016 12:59:58 GMT</pubDate></item><item><title>Github上Pandas,Numpy和 Scipy三个库中20个最常用的函数</title><link>https://zhuanlan.zhihu.com/p/22589899</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-798f51341c7d164bd44eaaf38393fc65_r.png"&gt;&lt;/p&gt;&lt;p&gt;几个月前，我看到一篇博客中列出了 Github 网站上 Python 常用库中使用频率最高的一些函数/模块。我在这个基础上做了可视化处理，并撰写了每个库中使用频率前十的函数示例。其中本文中只包含了部分示例，完整的示例可以参见我的 Github。 &lt;/p&gt;&lt;p&gt;首先我利用 requests 和 BeautifulSoup 从原始博客中爬取相关的数据，然后利用 matplotlib 和 seaborn 来绘制条形图，其中函数的排序由包含该函数的资源库(Repositories)数目所决定。比如，虽然 pd.Timestamp 的总频次特别高，但是该函数仅在少量的资源库中出现，所以它的排序相对靠后。&lt;/p&gt;&lt;p&gt;Pandas&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-40d2ea1b8cf1e0dfb2fb365f61a0f2cc.png" data-rawwidth="818" data-rawheight="494"&gt;&lt;h4&gt;DataFrame: 创建一个 dataframe 对象&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-d0a012915b1e7a78427ec3a77492c2c4.png" data-rawwidth="448" data-rawheight="500"&gt;&lt;h4&gt;merge：联结两个 dataframe&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-ee46064bfbfa818849118ef5ac9c7fa7.png" data-rawwidth="454" data-rawheight="403"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-35d684f66095fca3477f57010775b95c.png" data-rawwidth="785" data-rawheight="541"&gt;&lt;h4&gt;Numpy&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-f8b0281e4a7f166298981ddcbb7ae3ac.png" data-rawwidth="776" data-rawheight="526"&gt;&lt;/h4&gt;&lt;h4&gt;arange: 创建某个区间内等间距的序列数组&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-8cadd29490f8d0e1719d9df3e4d41f00.png" data-rawwidth="448" data-rawheight="175"&gt;&lt;h4&gt;mean: 沿着某个轴向计算列表/数组中所有数据的平均数&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-8d3760ef3bfe2c7a0fc9033bae5c67c7.png" data-rawwidth="451" data-rawheight="327"&gt;&lt;h4&gt;Scipy&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-00c7eb8d0f1991e3a245894f6cf34720.png" data-rawwidth="780" data-rawheight="523"&gt;&lt;h4&gt;stats: 常用的统计函数或分布函数&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-1fcddb0f0dadf7fe3db647724ab30f93.png" data-rawwidth="453" data-rawheight="355"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-3312fa73fcca36fd718e9825d0fbf162.png" data-rawwidth="381" data-rawheight="256"&gt;&lt;h4&gt;linalg: 常用的线性代数函数，如逆矩阵(linalg.inv)、行列式(linalg.det)&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1d040fd9df660cd7f7339e0c5fdf423a.png" data-rawwidth="449" data-rawheight="521"&gt;&lt;h4&gt;interpolate: 样条函数和插值函数&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7ee12e43107a108d56f9555bb2e898a7.png" data-rawwidth="451" data-rawheight="218"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-907edbd661c2d653b88acea7b80784b8.png" data-rawwidth="380" data-rawheight="256"&gt;&lt;/h4&gt;&lt;h4&gt;signal: 包含信号处理工具&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-01c14faa35a8e3013cb5251a05c91c6f.png" data-rawwidth="457" data-rawheight="541"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b69e91802f6a38d8e15c648fb5fd731b.png" data-rawwidth="383" data-rawheight="266"&gt;&lt;/h4&gt;&lt;h4&gt;misc: misc.imread 和 misc.imsave 分别用于读取和保存图像数据&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6301e3a3f640f18e6ebbed7a0c590b70.png" data-rawwidth="453" data-rawheight="236"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-798f51341c7d164bd44eaaf38393fc65.png" data-rawwidth="572" data-rawheight="204"&gt;&lt;p&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;原文链接：&lt;a href="https://galeascience.wordpress.com/2016/08/10/top-10-pandas-numpy-and-scipy-functions-on-github/" class="" data-editable="true" data-title="Top 20 Pandas, NumPy and SciPy functions on GitHub"&gt;Top 20 Pandas, NumPy and SciPy functions on GitHub&lt;/a&gt;原文作者：Alexander Galea译者：Fibears&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22589899&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 10 Nov 2016 10:28:14 GMT</pubDate></item><item><title>美国大选Facebook舆情分析——基于R</title><link>https://zhuanlan.zhihu.com/p/22270222</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/adedd738064e6e8f128136380fccdb5d_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;2016年7月27日，希拉里·克林顿顺利成为民主党总统候选人，这也意味着她将和之前成为共和党总统候选人的唐纳德·特朗普在11月份进行最终角逐。两位候选人在之前的五个月的网络口水仗，也使得各大社交平台异彩纷呈。为了从社交媒体这一渠道对两位候选人的竞选表现和粉丝基础有一个更为具体的了解。我们分析了一年多以来克林顿（大约360万粉丝）和特朗普（大约800万粉丝）的Facebook流量。&lt;/p&gt;&lt;h3&gt;数据获取&lt;/h3&gt;&lt;p&gt;用R包“Rfacebook”爬取了2015年5月1日到2016年5月31日的候选人Facebook官方主页的内容，获得了这段时间内所有的帖子和相应的评论（特朗普大约140万评论而克林顿大约120万评论）。然后用R的文本挖掘包和文本数据定量分析方法，基于LIWC的词条目录对每个评论（不含非英文内容）单独解析它的情绪和心理结构。最后，我们汇总成为日度数据进行分析。&lt;/p&gt;&lt;h3&gt;情绪氛围&lt;/h3&gt;&lt;p&gt;特朗普评论区相较克林顿表现得更为积极：数据显示特朗普的积极评论占比69.46%，消极评论占比30.46%；克林顿的积极评论占比65.94%，消极评论占比33.94％。而第二张图显示特朗普评论区的情绪分化现象更为显著：这里用百分比的变异系数来表示情绪分化程度，特朗普评论的百分比变异系数是122.23，而克林顿的是115.31，这确实很容易理解，特朗普自竞选以来发表了种种言论，要么引来狂热的追捧，要么被人诟病为疯子。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1d2e8019e3e72aa81dbc5ac903a7e90b.png" data-rawwidth="865" data-rawheight="508"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/15a02b9e169c971c733d554dd110da7c.png" data-rawwidth="865" data-rawheight="508"&gt;&lt;h3&gt;乐观程度&lt;/h3&gt;&lt;p&gt;克林顿的拥护者相较而言对竞选前景表现得更为乐观（克林顿31.13％而特朗普是29.57%），并但是可以发现特朗普的评论区有一种向上的积极发展趋势。同样，特朗普评论区的乐观程度分化也更为显著，拥有130.12的变异系数，而克林顿只有126.11。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/93fcfd6e0609e8ed99cf3524c3b902ad.png" data-rawwidth="865" data-rawheight="509"&gt;&lt;h3&gt;拥护者群体的包容度&lt;/h3&gt;&lt;p&gt;第四张图显示了评论区中，拥护者相互包容的的程度。比如说如果评论中更多的“我们”出现，则拥护者内部是更加包容和团结的；如果评论中更多的“我”出现，则拥护者内部是更加独立的。结果显示双方表达方式有很大不同，特朗普的拥护者表述的方式更加独立（36.12%），而克林顿的拥护者相对包容（30.38%），这可能意味着克林顿的拥护者能够更好地凝聚力量支持他们的选择。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a99a686a5295b8d639d4432de9295a51.png" data-rawwidth="865" data-rawheight="508"&gt;七月底民调结果显示克林顿的支持率首次被特朗普反超，但是紧接着由于特朗普的不当言论，克林顿重获优势，但无论下一任世界领袖的归属如何，最终决定这一结果的，还是数据。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22270222&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 03 Nov 2016 10:29:28 GMT</pubDate></item><item><title>跨境电商微信公众号推文热点解析</title><link>https://zhuanlan.zhihu.com/p/23110482</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7abcf006901ac8a465dd12f9b5add773_r.png"&gt;&lt;/p&gt;&lt;h3&gt;作者：数据工匠－SD Cry!!&lt;/h3&gt;&lt;p&gt;近几年来，国内跨境电商发展势头良好，在习惯了由代购到海淘这类进口跨境电子商务模式之后，国内的电商卖家也将目光转向了广阔的全球市场。据中国商务部预测，2016年中国跨境电商进出口额预计增至6.5万亿元人民币，占总进出口贸易额的20%，年增长率将达到30%。毫无疑问，跨境电子商务正逐步成为我国经济新的增长动力，为外贸市场带来新的活力。在此背景下，各类与跨境电商相关的微信公众号也如雨后春笋般地走入了人们的视野，成为了国内跨境电商卖家了解跨境电商行业知识与实事情报的重要渠道。那么，在最近的一段时间内，这些公众号都在关注些什么呢？ &lt;/p&gt;&lt;p&gt;为了了解跨境电商微信公众号推文热点，我们采集了跨境电商相关的微信公众号2016年3月至8月发布的微信推文，总计612篇，各个公众号推文占比如下：   &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7abcf006901ac8a465dd12f9b5add773.png" data-rawwidth="857" data-rawheight="569"&gt;在收集到的微信推文中，“跨境电商”、“电商平台”、“亚马逊平台”、“亚马逊卖家”以及“亚马逊运营”等词出现的次数位列高频词组榜前五，其中三个词组出现了“亚马逊”字样。在词云图中“亚马逊实操”、“什么FBA”也出现在了显眼的位置，由此可以推测亚马逊为现阶段微信公众号最关心的电商平台。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-b53c04db1386cbb21a8621188d767d70.png" data-rawwidth="829" data-rawheight="319"&gt;&lt;p&gt;为了作进一步地说明，我们对比了关键词“亚马逊／amazon”与其他电商平台名出现的频率，通过统计，发现在所有的612篇微信推文中，有79篇谈及Wish，占比12.91%；有131篇谈及速卖通，占比21.41%；有176篇谈及Ebay，占比28.76%；而谈及亚马逊的推文则高达463篇，占比75.65%。&lt;/p&gt;&lt;h2&gt;热点解析&lt;/h2&gt;&lt;h4&gt;一、国际电商政策与平台规则是关注焦点&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-69b1fb6a5611e3af1b5026647c505d25.png" data-rawwidth="865" data-rawheight="356"&gt;&lt;p&gt;在所有推文中，部分主要讨论与中国卖家最直接相关的国际电商政策与平台规则，今年8月，亚马逊美国站全球开店正式恢复，相关政策的改变也成为了受到大多数国内跨境电商卖家关注的焦点。 &lt;/p&gt;&lt;h4&gt;二、公众号在推文中的宣传力度加大&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6938488646f367cc6587d5bfbf780303.png" data-rawwidth="820" data-rawheight="345"&gt;&lt;p&gt;在收集到的推文中，部分内容侧重于与公众号关联的微信群或相关社区推广，随着跨境电商信息平台及相关第三方工具数量的激增，业内竞争压力促使公众号在推文中加大了宣传的力度。 &lt;/p&gt;&lt;h4&gt;三、多角度分析社交媒体对跨境电商的影响&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-0913f85bd3eacc89f60e4dd7ffb4bfa7.png" data-rawwidth="845" data-rawheight="350"&gt;&lt;p&gt;一部分微信推文内容涉及当今网络社交媒体对于跨境电商的影响，谈及了facebook、网络红人以及2016夏季受网民关注较高的奥运会等内容。&lt;/p&gt;&lt;h4&gt;四、“千刀”计划运营日志推广势头强劲&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-914131054170c3803abe01149e307544.png" data-rawwidth="847" data-rawheight="366"&gt;&lt;p&gt;在全部推文中，有一部分均为某跨境电商机构“千刀”活动的运营日志广播或经验交流，相对于其他热点，我们能够在这些推文的词云图中找到“千刀创客”、“Bella女神”这种更具特色的词组，独特的推广交流形式使得这些文章被单独聚为一类，可以看出，在如今的时代，微信公众号在传播信息之外也可能成为“跨境电商明星”诞生的平台。&lt;/p&gt;&lt;h4&gt;五、电商行业大会成为获得行业信息的重要渠道&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7d9e85730fca8be6f749ba5aa4652776.png" data-rawwidth="888" data-rawheight="360"&gt;&lt;p&gt;还有部分推文主要是各类跨境电商集会或电商行业大会的宣传与通知，随着平台基本规则在国内卖家中的逐渐普及，卖家间的线下交流也成为了获得行业信息的重要渠道，各种电商大会的出现很好的印证了这一点，而微信公众号也成为了其宣传推广的重要渠道。 &lt;/p&gt;&lt;h4&gt;六、“广告引流与Listing优化”话题经久不衰&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-86a2e2089d8739b935b921e048b35b9d.png" data-rawwidth="912" data-rawheight="347"&gt;&lt;p&gt;还有部分推文主要讨论产品的广告引流与Listing关键词优化，这是绝大多数跨境电商卖家打造所谓“爆款”的必经之路，也是整个电商行业范围内经久不衰的话题。 &lt;/p&gt;&lt;h4&gt;七、亚马逊物流体系FBA倍受关注&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a87cf0927123370360b7e6ceb8df77e6.png" data-rawwidth="870" data-rawheight="346"&gt;&lt;p&gt;该类推文主要讨论跨境电商交易过程中涉及到的物流与仓储及售后问题，其中亚马逊物流体系FBA倍受关注。&lt;/p&gt;&lt;h4&gt;八、国际电商发展形势不容忽视  &lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-25c15b45afa87cddc9cd2392ac6a6380.png" data-rawwidth="880" data-rawheight="343"&gt;&lt;p&gt;该类推文主要为一系列国际大型电商平台发展情况的行业数据报道，包括亚马逊的阶段性经营业绩、电商平台的行业竞争数据以及国际间的汇率变动等，这些大环境形势的变化直接关系着个人卖家的有关决策，不容忽视。&lt;/p&gt;&lt;h4&gt;九、电商技巧培训等新型业态悄然发展&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-90be3452f7f66e754df3851bfe2c21a9.png" data-rawwidth="867" data-rawheight="339"&gt;&lt;p&gt;该类推文主要内容为跨境电商在线答疑或培训的推广广告，可以看到，跨境电商蓬勃的发展也促使了电商技巧线上答疑培训这一新型业态的产生。&lt;/p&gt;&lt;h4&gt;十、运营培训班颇受欢迎&lt;/h4&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-aeeb42ac05e35bdee11fcdae112cda07.png" data-rawwidth="837" data-rawheight="328"&gt;&lt;p&gt;这其实是一篇亚马逊运营课程报名的推广文，由于重复次数较多，被单独聚类为了一个热点话题。可以看到，除了在线的答疑指导与线下的同业交流，这种贩卖课程体系、专家集中授课的形式也受到了当今跨境电商从业者的欢迎。&lt;/p&gt;&lt;p&gt;根据热点话题发现的结果，本文统计了属于各个热点话题的推文占比，统计结果如下所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-80d2b3215930f82e1608160f900b2026.png" data-rawwidth="761" data-rawheight="363"&gt;&lt;p&gt;可以看到，在所有热点话题中，涉及“国际电商政策与平台规则是关注焦点”这一热点的推文总计187篇，占推文总数的31%，为近期跨境电商相关公众号发布最多的文章类型。同时，值得注意的是， “多角度分析社交媒体对跨境电商的影响”相关文章共计63篇，占比达10%以上，超过了“广告引流与Listing优化”和“亚马逊物流体系FBA”等热点，这昭示着社交媒体对于跨境电商作用的日益明显，可能成为未来影响跨境电商行业风向的又一作用力。&lt;/p&gt;&lt;p&gt;经过以上分析，我们大致了解了近期跨境电商微信公众号的发文内容，可以归结为以下几点：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;相比于其他国际电商平台，亚马逊受到了跨境电商公众号最多的关注，这可能是由于我国出口跨境电商在亚马逊上的发展相对成熟；&lt;/li&gt;&lt;li&gt;微信推文内容鱼龙混杂，不乏大量在线或线下付费培训广告，我国跨境电商的发展不但直接作用于进出口贸易额，也带动着一系列新业态的产生；&lt;/li&gt;&lt;li&gt;“国际电商政策与平台规则”成为近期跨境电商微信公众号关注次数最多的焦点，而“多角度分析社交媒体对跨境电商的影响”的推文占比也相对较大，同样应引起广大电商卖家注意。 &lt;/li&gt;&lt;/ol&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23110482&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 02 Nov 2016 16:16:22 GMT</pubDate></item><item><title>亚马逊美站 hot new releases 榜单分析</title><link>https://zhuanlan.zhihu.com/p/23368799</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-61490b65a4190772729085f08bc39a59_r.png"&gt;&lt;/p&gt;&lt;p&gt;随着现代科技的进步发展，各式各样的移动设备正逐渐成为人们学习工作生活中的必需品，对于跨境电商而言，手机及其配件的市场规模同样不容小觑。上周数据脉数据发掘中心，通过对亚马逊市场 Cell Phones &amp;amp; Accessories 类目商品榜单进行深入挖掘，还提供了一份专业的数据报告。近些年，境外电商的下一个发展点在数据挖掘领域尤为突出，中国许多优秀的境外电商也在慢慢向数据驱动的商业模式靠拢，这样的数据报告在未来将是境外电商主流的决策参考依据。&lt;/p&gt;&lt;p&gt;报告生成要经过许多专业的数据处理工作：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;数据挖掘：数据收集覆盖了亚马逊 Cell Phones &amp;amp; Accessories 类目的相关榜单。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;（下图为报告的部分内容） &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-36b6f8797942516953904318d51d2d05.png" data-rawwidth="659" data-rawheight="265"&gt;数据分析：每个数据图表后面都会附上相对应的数据分析和解读，主要讨论了数据所显示出的主要信息。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-9bc5af55ba06a6bf88ecabe5e00c6c9f.png" data-rawwidth="609" data-rawheight="394"&gt;重要信息提炼：正如下图所示，报告指出上周一款品牌为 Anker 的壁式手机充电器榜单排名增长十分显著。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b94c4f661c962b2fa01cfe2a9ae7218a.png" data-rawwidth="636" data-rawheight="506"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-61490b65a4190772729085f08bc39a59.png" data-rawwidth="630" data-rawheight="422"&gt;以上是对这份报告的简单介绍，希望看到更多更详尽的内容，可以登录数据脉官方网站查看（&lt;a href="https://datartery.com/" data-editable="true" data-title="数据脉 - 亚马逊数据选品运营神器" class=""&gt;数据脉 - 亚马逊数据选品运营神器&lt;/a&gt;），希望所有企业都能享受到数据科学高速发展所带来的成果，用数据脉打造亚马逊上最火的商品！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23368799&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 02 Nov 2016 13:26:41 GMT</pubDate></item><item><title>如何可视化城市的交通便捷性</title><link>https://zhuanlan.zhihu.com/p/22351423</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/960dd13000dace136234afea98703f0d_r.png"&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.walkscore.com/" data-editable="true" data-title="WalkScore"&gt;WalkScore&lt;/a&gt;(&lt;a href="https://www.walkscore.com/" data-editable="true" data-title="Find Apartments for Rent and Rentals"&gt;Find Apartments for Rent and Rentals&lt;/a&gt;) 是一款用于可视化展示街区交通便捷性的工具，即评估从该街区到附近公园、学校或者餐馆等地方是否便利。借助 Python 中的 &lt;a href="https://udst.github.io/pandana/" data-editable="true" data-title="pandana"&gt;pandana&lt;/a&gt;(&lt;a href="https://udst.github.io/pandana/" data-editable="true" data-title="Pandana — pandana 0.1 documentation" class=""&gt;Pandana — pandana 0.1 documentation&lt;/a&gt;) 库，我们可以很轻松地创建一个类似的可视化工具。Pandana(Fletcher Foti 开发的用于社区分析的 pandas 扩展包) 可以快速地查询某个街区的交通便捷性。本文将介绍如何利用它来可视化展示城市的交通便捷性，本文的相关代码和数据都托管在 &lt;a href="https://github.com/gboeing/urban-data-science/tree/master/20-Network-Analysis-Walkability" data-editable="true" data-title="Github"&gt;Github&lt;/a&gt;(&lt;a href="https://github.com/gboeing/urban-data-science/tree/master/20-Network-Analysis-Walkability" data-editable="true" data-title="urban-data-science/20-Network-Analysis-Walkability at master · gboeing/urban-data-science · GitHub" class=""&gt;urban-data-science/20-Network-Analysis-Walkability at master · gboeing/urban-data-science · GitHub&lt;/a&gt;)上。&lt;/p&gt;&lt;p&gt;首先，我利用 pandana 来绘制旧金山地区伯克利/奥克兰的边界区域。接下来我将从开放街景地图数据库中导入道路关系网和便利设施的位置数据。本文主要关心到达餐馆、酒吧和学校的便捷性，你可以根据自己的研究目的设定相应的便利设施库。最后我将计算街道网络中每个街区节点步行到最近便利设施的距离并进行可视化展示。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/960dd13000dace136234afea98703f0d.png" data-rawwidth="571" data-rawheight="412"&gt;&lt;p&gt;上图中展示了街道网络图中每个街区节点步行到一公里内最近的餐馆、酒吧或者学校的可达性。但是衡量交通便捷性的一个更好的指标是该街区附近是否存在较多的便利设施，因此我们考虑绘制每个街区节点到其第五近的便利设施的距离情况：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/bd1331cb65087b56ef2399f18f909ab3.png" data-rawwidth="578" data-rawheight="410"&gt;上图中很清晰地展示了交通较为便捷的奥克兰中心区和伯克利中心区。此外我们还可以单独评估各个社区到附近不同地点的距离。下图展示了街区节点到最近酒吧的可达性：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/8a7cd4355cd28f0ac0746331baf17355.png" data-rawwidth="565" data-rawheight="415"&gt;&lt;p&gt;下图展示了到最近学校的可达性：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/7043b80d500fa79603b39264d43380a0.png" data-rawwidth="586" data-rawheight="419"&gt;不出所料，不同的街区到酒吧和学校的可达性差异较大。而且从上图中我们可以看出，湾区东部有一大块街区附近基本没有学校资源。下一步我们可以进一步分析这些便利设施的布局与当地种族聚集程度、教育水平和收入水平等变量之间的关系。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22351423&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 20 Oct 2016 10:44:31 GMT</pubDate></item><item><title>利用关键词洞察海外亚马逊电商市场商机</title><link>https://zhuanlan.zhihu.com/p/23062345</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-65d9daebf1631b7ddff38af8953450c7_r.jpg"&gt;&lt;/p&gt;作为进阶亚马逊买家，我们需要实时关注市场的动态和走向。“方向比努力更重要”简简单单一句话，不知承载着多少市场风云变幻留下的血泪。  &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-65d9daebf1631b7ddff38af8953450c7.jpg" data-rawwidth="403" data-rawheight="295"&gt;&lt;p&gt;前一阵某个产品突然间火了，你还没来得及弄明白它怎么火的，却深怕赶不上趟。于是你不管三七二十一，或小打小闹或火力全开地在这个市场掺了一脚。但是人生最令人沮丧的事情之一就是当你全速前进的时候突然发现自己走错了方向。很多看似爆火的产品，等你进入的时候，这个市场可能已经被挤得没有利润空间了。&lt;/p&gt;&lt;p&gt;跟风这种事真的和赌博没有什么区别，要不然大家就不会总是想在前面加上盲目两个字变成“盲目跟风”了。作为一个卖家，跟风是很正常的，因为一个正在抢钱的市场，我为什么就不能抢一把呢？一个进阶亚马逊卖家应该做的就是避开盲目，跟得一手好风，甚至要带起一阵风来让别人跟。&lt;/p&gt;&lt;p&gt;所以，弄清楚哪些东西正在持续升温；哪些现在非常火；哪些东西可能已经过气就十分有必要。那这些该如何把握呢？做市场调查吗？当然可以，但是你要是这样做，你要花多大的代价才能得到一些可以支持你判断市场动态的数据呢？&lt;/p&gt;&lt;p&gt;细致入微的商品市场分析固然重要，但我们也没有必要把事情想得太过复杂。有的时候我们只需要利用已知的商品关键词锚定目标市场，把握几个关键数据，便能清晰了解到对应类型商品的火爆程度。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-183a294774f79e632eb17fff4bbe3dc7.png" data-rawwidth="977" data-rawheight="540"&gt;我们可以利用亚马逊搜索功能，搜索关键字，搜索结果前的几项都是比较典型的，值得我们用心研究的结果。所以我们可以利用数据工具把前面的的若干个商品的数据抽取出来， 再利用数据工具把这若干个商品的“月平均销量”、“平均BSR”、“平均评论量”、“平均评分”计算出来。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b13c2ed67a3809b15f9c77ebd798800b.png" data-rawwidth="1314" data-rawheight="919"&gt;上文中的方法也许略显麻烦，我推荐大家使用最近在亚马逊卖家圈里很火的一款产品——“数据脉”&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e8e6c3f427b1b96b35070e3a8be24616.png" data-rawwidth="1217" data-rawheight="879"&gt;利用数据脉里“关键词商机”这个功能。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0c0b1a90afe83372fa2574d1b3525dfb.png" data-rawwidth="233" data-rawheight="278"&gt;进入之后我们将我们要了解的商品关键词输入到搜索框里，选择想要搜索的品类范围，点击“开始分析”。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-38432958c5a4e26acf5508f3fb31947a.png" data-rawwidth="1317" data-rawheight="280"&gt;稍候片刻，待分析完成后，点击“查看结果”。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-7a33264a5b4a08319f9d33272ac4e909.png" data-rawwidth="1288" data-rawheight="541"&gt;我们将得到下图所示的分析报告。如果你对某个结果特别感兴趣的话，可以点击“亚马逊”图标，直接进入该商品的亚马逊页面；或点击跟踪竞品可以对这个商品进行跟踪。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-2a5b09a11bf37f8617738dd90f5d9dbd.png" data-rawwidth="1324" data-rawheight="849"&gt;&lt;p&gt;有了这些数据，我们就可以做更加深入的分析了：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;现在该商品能有多少的月销售量？&lt;/li&gt;&lt;li&gt;BSR大概能排在第几位？&lt;/li&gt;&lt;li&gt;评论量能有多少？&lt;/li&gt;&lt;li&gt;现在的市场饱和了吗？大概还有多少利润空间？&lt;/li&gt;&lt;li&gt;我现在进入市场会不会太晚或者太早？&lt;/li&gt;&lt;li&gt;我要投放的“关键词广告”够不够热，会有很多人搜索和关注吗？&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以上的每一项深入分析都可以成为一个新的话题，对此我们不再展开详细说明了。工具摆在那，希望大家都能用出花样，用出境界，抢先一步抓住关键词中的市场商机。&lt;/p&gt;&lt;p&gt;http://qm.qq.com/cgi-bin/qm/qr?k=y0sgE3_qKUd_DhplBTYiV667XDMPNONR (二维码自动识别)&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23062345&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Wed, 19 Oct 2016 16:48:27 GMT</pubDate></item><item><title>短文本主题建模方法</title><link>https://zhuanlan.zhihu.com/p/22332099</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/45b65581fc651b93dc1f6c9212833f6f_r.png"&gt;&lt;/p&gt;&lt;h2&gt;1. 引言&lt;/h2&gt;&lt;p&gt;许多数据分析应用都会涉及到从短文本中提取出潜在的主题，比如微博、短信、日志文件或者评论数据。一方面，提取出潜在的主题有助于下一步的分析，比如情感评分或者文本分类模型。另一方面，短文本数据存在一定的特殊性，我们无法直接用传统的主题模型算法来处理它。短文本数据的主要难点在于： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;短文本数据中经常存在多词一义的现象[1]，比如 “dollar”, "$", "$$", "fee", "charges" 拥有相同的含义，但是受限于文本篇幅的原因，我们很难直接从短文本数据中提取出这些信息。&lt;/li&gt;&lt;li&gt;与长文档不同的地方在于，短文本数据中通常只包含一个主题。这看似很好处理，但是传统的主题模型算法都假设一篇文档中包含多个主题，这给建模分析带来了不小的麻烦。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;主题提取模型通常包含多个流程，比如文本预处理、文本向量化、主题挖掘和主题表示过程。每个流程中都有多种处理方法，不同的组合方法将会产生不同的建模结果。 &lt;/p&gt;&lt;p&gt;本文将主要从实际操作的角度来介绍不同的短文本主题建模算法的优缺点，更多理论上的探讨可以参考以下文章。 &lt;/p&gt;&lt;p&gt;下文中我将自己创建一个数据集，并利用 Python scikit-learn 来拟合相应的主题模型。 &lt;/p&gt;&lt;h2&gt;2. 主题发现模型&lt;/h2&gt;&lt;p&gt;本文主要介绍三个主题模型, LDA(Latent Dirichlet Allocation), NMF(Non-Negative Matrix Factorization)和SVD(Singular Value Decomposition)。本文主要采用 scikit-learn 来实现这三个模型。 &lt;/p&gt;&lt;p&gt;除了这三个模型外，还有其他一些模型也可以用来发现文档的结构。其中最重要的一个模型就是 KMeans 聚类模型，本文将对比 KMeans 聚类模型和其他主题模型的拟合效果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/dd56a7cb6c13d05cb6001441e6551aec.png" data-rawwidth="444" data-rawheight="217"&gt;&lt;p&gt;首先，我们需要构建文本数据集。本文将以四个自己构建的文本数据集为例来构建主题模型： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;clearcut topics: 该数据集中只包含两个主题—— "berger-lovers" 和 "sandwich-haters"。&lt;/li&gt;&lt;li&gt;unbalanced topics: 该数据集与第一个数据集包含的主题信息一致，但是此数据集的分布是有偏的。&lt;/li&gt;&lt;li&gt;semantic topics: 该数据集包含四个主题，分别是 "berger-lovers"， "berger-haters"，"sandwich-lovers" 和 "sandwich-haters"。此外，该数据集中还包含了两个潜在的主题 “food” 和 “feelings”。&lt;/li&gt;&lt;li&gt;noisy topics: 正如前文所说的，短文本数据中经常存在多词一义的现象，该数据集主要用于模拟两个主题不同类型的文本。该数据集文本的篇幅小于其他三个数据集，这可以用来检验模型是否能够很好地处理短文本数据。&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/00d8dd77ea06f0372f19f7beeca09cb8.png" data-rawwidth="450" data-rawheight="677"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/10a3c69c01a2801ae89c479548073233.png" data-rawwidth="449" data-rawheight="565"&gt;&lt;p&gt;首先，我们需要考虑下如何评估一个主题模型建模效果的好坏程度。多数情况下，每个主题中的关键词有以下两个特征： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;关键词出现的频率得足够大&lt;/li&gt;&lt;li&gt;足以区分不同的主题 &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;一些研究表明：关键词还需具备以下两个特征： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;相同主题的文档中关键词共同出现的频率应该差不多&lt;/li&gt;&lt;li&gt;每个主题中关键词的语义应该十分接近，比如水果主题中的 “apples” 和 “oranges”，或者情感主题中的 “love” 和 “hate”。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;接下来，我们将介绍如何实现上述的四个模型——NMF, SVD, LDA 和 KMEANS。对于每个主题模型，我们将分别采用两种文本向量化的方法—— TF(Term Frequence) 和 TFIDF(Term-frequence-inverse-document-frequence)。通常情况下，如果你的数据集中有许多词语在多篇文档中都频繁出现，那么你应该选择采用 TFIDF 的向量化方法。此时这些频繁出现的词语将被视为噪声数据，这些数据会影响模型的拟合效果。然而对于短文本数据而言，TF和TFIDF方法并没有显著的区别，因为短文本数据集中很难碰到上述情况。如何将文本数据向量化是个非常热门的研究领域，比如 基于word embedding模型的方法——word2vec和doc2vec。 &lt;/p&gt;&lt;p&gt;主题模型将选择主题词语分布中频率最高的词语作为该主题的关键词，但是对于 SVD 和 KMEANS 算法来说，模型得到的主题词语矩阵中既包含正向值也包含负向值，我们很难直接从中准确地提取出主题关键词。为了解决这个问题，我选择从中挑出绝对数值最大的几个词语作为关键词，并且根据正负值的情况加上相应的标签，即对负向词语加上 "^" 的前缀，比如"^bergers"。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/06878f42ddaa8b873ac42f621064c06f.png" data-rawwidth="444" data-rawheight="374"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/69b9594b0bac370b9d24d76eda1d4513.png" data-rawwidth="435" data-rawheight="575"&gt;&lt;h3&gt;2.1 SVD: 正交分解&lt;/h3&gt;&lt;p&gt;sklearn 中的 truncated SVD implementation 类似于主成分分析算法，它们都试图利用正交分解的方法选择出具有最大方差的变量信息。 &lt;/p&gt;&lt;p&gt;对于 clearcut-topic 数据集来说，我们分别利用 TF 和 TFIDF方法来向量化文本数据，并构建 SVD 模型，模型的拟合结果如下所示。正如我们之前所提到的，SVD 模型所提取的关键词中包含正负向词语。为了简单起见， 我们可以理解为该主题包含正向词语，不包含负向的词语。 &lt;/p&gt;&lt;p&gt;比如，对于 "Topic 1: bergers | ^hate | love | ^sandwiches" 来说，该文本的主题中包含 "love bergers" 但是不包含 "hate sandwiches"。 &lt;/p&gt;&lt;p&gt;由于模型的随机效应，所以每次运行模型得到的结果都会存在细微的差异。在 SVD 的拟合结果中我们发现发现 Topic 3: bergers | ^hate | ^love | sandwiches 成功地提取了 “food” 的主题。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/1ecc76e6b9450551513af3752342b792.png" data-rawwidth="437" data-rawheight="467"&gt;在上述的例子中，我们设定了过多的主题数量，这是因为大多数时候我们无法事先知道某个文档包含多少个主题。如果我们令主题个数等于2，可以得到下述结果：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/dc3ec2767306e12c9ed185caa89000dc.png" data-rawwidth="443" data-rawheight="158"&gt;&lt;p&gt;当我们在解释 SVD 模拟的拟合结果时，我们需要对比多个主题的信息。比如上述的模型拟合结果可以解释成：数据集中文档的主要差异是文档中包含 “love bergers” 但不包含 “hate sandwiches”。 &lt;/p&gt;&lt;p&gt;接下来我们将利用 SVD 来拟合 unbalanced topic 数据集，检验该模型处理非平衡数据集的效果。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/62d8b038dc4d11879375779b2611062f.png" data-rawwidth="437" data-rawheight="166"&gt;从下述结果中可以看出，SVD无法处理噪声数据，即无法从中提取出主题信息。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/1598eefc59132b72b2788d2e245bb863.png" data-rawwidth="437" data-rawheight="297"&gt;&lt;/p&gt;&lt;h3&gt;2.2 LDA: 根据词语的共现频率来提取主题&lt;/h3&gt;&lt;p&gt;LDA 是最常用的主题提取模型之一，因为该模型能够处理多种类型的文本数据，而且模拟的拟合效果非常易于解释。 &lt;/p&gt;&lt;p&gt;直观上来看，LDA 根据不同文档中词语的共现频率来提取文本中潜在的主题信息。另一方面，具有相同主题结构的文本之间往往非常相似，因此我们可以根据潜在的主题空间来推断词语之间的相似性和文档之间的相似性。 &lt;/p&gt;&lt;p&gt;LDA 算法中主要有两类参数： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;每个主题中各个关键词的分布参数&lt;/li&gt;&lt;li&gt;每篇文档中各个主题的分布参数&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;接下来我们将研究这些参数是如何影响 LDA 模型的计算过程，人们更多的是根据经验来选择最佳参数。 &lt;/p&gt;&lt;p&gt;与 SVD 模型不同的是，LDA 模型所提取的主题非常好解释。以 clearcut-topics 数据集为例，LDA 模型中每个主题都有明确的关键词，它和SVD主要有以下两个区别： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;LDA 模型中可能存在重复的主题&lt;/li&gt;&lt;li&gt;不同的主题可以共享相同的关键词，比如单词 “we” 在所有的主题中都出现了。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;此外，对 LDA 模型来说，采用不同的文本向量化方法也会得到不同的结果。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2e2974b2b0434b623125f2b209585202.png" data-rawwidth="445" data-rawheight="349"&gt;&lt;p&gt;在 sklearn 中，参数 topic&lt;em&gt;word&lt;/em&gt;prior 和 doc&lt;em&gt;topic&lt;/em&gt;prior 分别用来控制 LDA 模型的两类参数。 &lt;/p&gt;&lt;p&gt;其中 topic&lt;em&gt;word&lt;/em&gt;prior 的默认值是(1/n&lt;em&gt;topics)，这意味着主题中的每个词语服从均匀分布。&lt;/em&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/d3da2eabf4428c7545318776b8a2e8d1.png" data-rawwidth="435" data-rawheight="202"&gt;&lt;em&gt;选择更小的 topic&lt;/em&gt;word_prior 参数值可以提取粒度更小的主题信息，因为每个主题中都会选择更少的词语。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/9d269da811a94d8d603b0924c03f0538.png" data-rawwidth="454" data-rawheight="183"&gt;LDA 模型同样无法很好地处理 noisy topics 数据集，从下述结果中可以看出 LDA 模型提取的主题相当模糊：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/188d61474f54376df385430e96dd6c50.png" data-rawwidth="448" data-rawheight="400"&gt;&lt;h3&gt;2.3 NMF&lt;/h3&gt;&lt;p&gt;NMF 可以视为 LDA模型的特例，从理论上来说，这两个模型之间的联系非常复杂。但是在实际应用中，NMF 经常被视为参数固定且可以获得稀疏解的 LDA 模型。虽然 NMF 模型的灵活性不如 LDA 模型，但是该模型可以很好地处理短文本数据集。 &lt;/p&gt;&lt;p&gt;另一方面，NMF 最大的缺点是拟合结果的不一致——当我们设置过大的主题个数时，NMF 拟合的结果非常糟糕。相比之下，LDA模型的拟合结果更为稳健。 &lt;/p&gt;&lt;p&gt;首先我们来看下 NMF 模型不一致的拟合结果。对于 clearcut topics 数据集来说，当我们设置提取5个主题时，其结果和真实结果非常相似：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/e394d64b6412d6eb8b3d6da6d755d767.png" data-rawwidth="442" data-rawheight="178"&gt;但是当我们增加主题个数时（远大于真实主题数2），NMF 模型将会得到一些奇异的结果：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/956e6a758e5de7218440a057ca1fcde0.png" data-rawwidth="444" data-rawheight="512"&gt;相比之下，LDA模型的结果十分稳健。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c99c3663425c41fc62de71a3b2c52a65.png" data-rawwidth="444" data-rawheight="640"&gt;对于非平衡数据集，设置好合适的主题个数，NMF 可以很好地提取出文档中的主题信息。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/390b3509368b4b3a77a13099939e6ca8.png" data-rawwidth="452" data-rawheight="179"&gt;值得注意的是，NMF 是本文提到的四个模型中唯一一个能够较好地处理 noisy topics 数据的模型：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/85b28398d17a31a72849b3fadc76f446.png" data-rawwidth="441" data-rawheight="296"&gt;&lt;/p&gt;&lt;h3&gt;2.4 KMeans&lt;/h3&gt;&lt;p&gt;类似于 KMeans 模型的聚类方法能够根据文档的向量形式对其进行分组。然而这个模型无法看成是主题模型，因为我们很难解释聚类结果中的关键词信息。 &lt;/p&gt;&lt;p&gt;但是如果结合 TF或TFIDF方法，我们可以将 KMeans 模型的聚类中心视为一堆词语的概率组合：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2e976c586bd65756b7375dd8e0330b90.png" data-rawwidth="446" data-rawheight="548"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/cce452500aeed3f9d6373bd95b837350.png" data-rawwidth="440" data-rawheight="303"&gt;&lt;/p&gt;&lt;h3&gt;2.5 寻找具有高语义相关的主题&lt;/h3&gt;&lt;p&gt;最后，我将简单比较下不同的主题提取模型。大多数情况下，我们倾向于根据文档的主题分布情况对其进行分组，并根据关键词的分布情况来提取主题的信息。 &lt;/p&gt;&lt;p&gt;大多数研究者都认为词语的语义信息是由其上下文信息所决定的，比如 “love” 和 “hate”可以看成是语义相似的词语，因为这两个词都可以用在 “I _ apples” 的语境中。事实上，词向量最重要的一个研究就是如何构建词语、短语或者文档的向量形式，使得新的向量空间中仍然保留着语义信息。 &lt;/p&gt;&lt;p&gt;找寻语义相同的词语不同于计算词语的共现频率。从下述的结果中可以看出，大多数主题提取模型只涉及到词语的共现频率，并没有考虑词语的语义信息，只有 SVD 模型简单涉及到语义信息。 &lt;/p&gt;&lt;p&gt;需要注意的是，本文所采用的数据集是根据一定的规则随机生成的，所以下述结果更多的是用于说明不同模型之间的区别：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/126bf908c01b80419a94181336662b9d.png" data-rawwidth="454" data-rawheight="755"&gt;&lt;h2&gt;3. 总结&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;短文本数据集具有其独特的性质，建模时需要特别注意。&lt;/li&gt;&lt;li&gt;模型的选择依赖于主题的定义（共现频率高或者语义相似性）和主题提取的目的（文档表示或者是异常值检验）&lt;/li&gt;&lt;li&gt;&lt;p&gt;我们可以首先采用 KMeans 或者 NMF 模型来快速获取文档的结构信息：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;主题中词语的分布情况&lt;/li&gt;&lt;li&gt;文档中主题的分布情况&lt;/li&gt;&lt;li&gt;主题个数&lt;/li&gt;&lt;li&gt;每个主题中词语的个数&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;LDA 模型具有很好的灵活性，可以处理多种类型的文本数据。但是调参过程需要很好地理解数据结构，因此如果你想构建 LDA 模型，你最好先构建一个基准模型（KMEANS 或 NMF）&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;SVD 模型可以很好地提取出文本的主要信息。比如 SVD 模型可以很好地分析半结构化的数据（模板数据、截图或者html中的表格数据）。 &lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;http://weixin.qq.com/r/WkMCGqvEoPbfre959xZI (二维码自动识别)&lt;/p&gt;&lt;p&gt;原文链接：&lt;a href="http://nbviewer.jupyter.org/github/dolaameng/tutorials/blob/master/topic-finding-for-short-texts/topics_for_short_texts.ipynb"&gt;http://nbviewer.jupyter.org/github/dolaameng/tutorials/blob/master/topic-finding-for-short-texts/topics_for_short_texts.ipynb&lt;/a&gt;&lt;/p&gt;&lt;p&gt;原文作者：dolaameng  &lt;/p&gt;&lt;p&gt;译者：Fibears  &lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22332099&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 13 Oct 2016 11:51:22 GMT</pubDate></item><item><title>机器学习通用框架</title><link>https://zhuanlan.zhihu.com/p/22833471</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-5ed55fdec803bf29c368e83917832630_r.png"&gt;&lt;/p&gt;每个数据科学家每天都要处理成吨的数据，而他们60%~70%的时间都在进行数据清洗和数据格式调整，将原始数据转变为可以用机器学习所识别的形式。本文主要集中在数据清洗后的过程，也就是机器学习的通用框架。这个框架是我在参加了百余场机器学习竞赛后的一个总结。尽管这个框架是非常笼统和概括的，但是绝对能发挥强大的作用，仍然可以在专业人员的运用下变成复杂、高效的方法。整个过程使用Python来实现。&lt;h2&gt;数据&lt;/h2&gt;&lt;p&gt;在用机器学习的方法之前，我们应该先把数据转变为表格的形式，这个过程是最耗时、最复杂的。我们用下图来表示这一过程。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7e116860a704a65392553fc64eb40615.png" data-rawwidth="800" data-rawheight="222"&gt;&lt;p&gt;这一过程也就是将原始数据的所有的变量量化，进一步转变为含数据（Data）和标签（Labels）的数据框形式。这样处理过的数据就可以用来机器学习建模了。数据框形式的数据是机器学习和数据挖掘中最为通用的数据表现形式，它的行是数据抽样得到的样本，列代表数据的标签Y和特征X，其中标签根据我们要研究的问题不同，有可能是一列或多列。 &lt;/p&gt;&lt;h2&gt;标签的类型&lt;/h2&gt;&lt;p&gt;根据我们要研究的问题，标签的类型也不一： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;单列0-1值(&lt;strong&gt;二分类问题&lt;/strong&gt;，一个样本只属于一类并且一共只有两类）&lt;/li&gt;&lt;li&gt;单列连续值（&lt;strong&gt;单回归问题&lt;/strong&gt;，要预测的值只有一个）&lt;/li&gt;&lt;li&gt;多列0-1值（&lt;strong&gt;多分类问题&lt;/strong&gt;，同样是一个样本只属于一类但是一共有多类）&lt;/li&gt;&lt;li&gt;多列连续值(&lt;strong&gt;多回归问题&lt;/strong&gt;，能够预测多个值）&lt;/li&gt;&lt;li&gt;多标签（&lt;strong&gt;多标签分类问题&lt;/strong&gt;，但是一个样本可以属于多类）&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;评价指标&lt;/h2&gt;&lt;p&gt;对于多个机器学习方法，我们必须找到一个评价指标来衡量它们的好坏。比如一个二元分类的问题我们一般选用AUC ROC或者仅仅用AUC曲线下面的面积来衡量。在多标签和多分类问题上，我们选择交叉熵或对数损失函数。在回归问题上我们选择常用的均方误差(MSE)。&lt;/p&gt;&lt;h2&gt;Python库&lt;/h2&gt;&lt;p&gt;在安装机器学习的几个库之前，应该安装两个基础库：numpy和scipy。 &lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;Pandas&lt;/strong&gt; 处理数据最强大的库&lt;/li&gt;&lt;li&gt;&lt;strong&gt;scikit-learn&lt;/strong&gt; 涵盖机器学习几乎所有方法的库&lt;/li&gt;&lt;li&gt;&lt;strong&gt;xgboost&lt;/strong&gt; 优化了传统的梯度提升算法 &lt;/li&gt;&lt;li&gt;&lt;strong&gt;keras&lt;/strong&gt; 神经网络&lt;/li&gt;&lt;li&gt;&lt;strong&gt;matplotlib&lt;/strong&gt;用来作图的库&lt;/li&gt;&lt;li&gt;&lt;strong&gt;tpdm&lt;/strong&gt; 显示过程&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;机器学习框架&lt;/h2&gt;&lt;p&gt;2015年我想出了一个自动式机器学习的框架，直到今天还在开发阶段但是不久就会发布，本文就是以这个框架作为基础的。下图展示了这个框架：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-5ed55fdec803bf29c368e83917832630.png" data-rawwidth="800" data-rawheight="605"&gt;&lt;p&gt;上面展示的这个框架里面，粉红色的线就是一些通用的步骤。在处理完数据并把数据转为数据框格式后，我们就可以进行机器学习过程了。 &lt;/p&gt;&lt;h3&gt;确定问题&lt;/h3&gt;&lt;p&gt;确定要研究的问题，也就是通过观察标签的类别确定究竟是分类还是回归问题。 &lt;/p&gt;&lt;h3&gt;划分样本&lt;/h3&gt;&lt;p&gt;第二步是将所有的样本划分为&lt;strong&gt;训练集(training data)&lt;/strong&gt;和&lt;strong&gt;验证集(validation data)&lt;/strong&gt;。过程如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-7e153d8877ffb6b6549888cacf8ebea5.png" data-rawwidth="800" data-rawheight="406"&gt;划分样本的这一过程必须要根据标签来做。比如对于一个类别不平衡的分类问题，必须要用分层抽样的方法，比如每种标签抽多少，这样才能保证抽出来的两个样本子集分布类似。在Python中，我们可以用scikit-learn轻松实现。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-fe714934ca0c0c46506a0abf03a34538.png" data-rawwidth="496" data-rawheight="130"&gt;对于回归问题，那么一个简单的K折划分就足够了。但是仍然有一些复杂的方法可以使得验证集和训练集标签的分布接近，这个问题留给读者作为练习。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-274aedb004ffb1123a80f28523812d38.png" data-rawwidth="504" data-rawheight="121"&gt;&lt;p&gt;上面我用了样本全集中的10%作为验证集的规模，当然你可以根据你的样本量做相应的调整。划分完样本以后，我们就把这些数据放在一边。接下来我们使用的任何一种机器学习的方法都要先在训练集上使用然后再用验证集检验效果。验证集和训练集永远都不能掺和在一起。这样才能得到有效的评价得分，否则将会导致过拟合的问题。 &lt;/p&gt;&lt;h3&gt;识别特征&lt;/h3&gt;&lt;p&gt;一个数据集总是带有很多的变量(variables)，或者称之为特征（features)，他们对应着数据框的维度。一般特征的值有三种类型：数值变量、属性变量和文字变量。我们用经典的&lt;a href="https://www.kaggle.com/c/titanic/data" data-editable="true" data-title="泰坦尼克号数据集"&gt;泰坦尼克号数据集&lt;/a&gt;来示例。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-ddcec0bb516d5ac00699f42ae7f6a4d0.png" data-rawwidth="800" data-rawheight="441"&gt;&lt;p&gt;在这里生还（survival）就是标签，船舱等级（pclass）、性别（sex）和登船港口（embarked）是属性变量。而像年龄（age）、船上兄弟姐妹数量（sibsp）、船上父母孩子数量（parch）是数值变量。而姓名（name）这种文字变量我们认为这和生还与否没什么关系，所以我们决定不考虑。首先处理数值型变量，这些变量几乎不需要任何的处理，常见的方式是正规化（normalization)。处理属性变量通常有两步： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;把属性变量转变为标签&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-5f8e47281c1989d51fdfb15f9d46d96e.png" data-rawwidth="533" data-rawheight="111"&gt;&lt;/li&gt;&lt;li&gt;把标签转变为二元数值&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-210a2f9867eb2da40f2f84166b66d9f5.png" data-rawwidth="531" data-rawheight="108"&gt;由于泰坦尼克号数据集没有很好的文字变量来示范，那么我们就制定一个通用的规则来处理文字变量。把所有的文字变量组合到一起，然后用某种算法来处理并转变为数字&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-4b80388cfcf90e904a3ae345a66f680a.png" data-rawwidth="531" data-rawheight="90"&gt;&lt;p&gt;我们可以用CountVectorizer或者TfidfVectorizer来实现&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c0a5835bbe0ca5ccc70e7cb5585c8f29.png" data-rawwidth="530" data-rawheight="92"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1de24e5d3486617db516dcae18e89df4.png" data-rawwidth="530" data-rawheight="91"&gt;一般来说第二种方法往往比较优越，下面代码框中所展示的参数长期以来都取得了良好的效果。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-4576414bd3937f529680b4d1db9547f9.png" data-rawwidth="530" data-rawheight="230"&gt;如果你对训练集数据采用了上述处理方式，那么也要保证对验证及数据做相同处理。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6d7b9382604d0b687aa166b06057c435.png" data-rawwidth="533" data-rawheight="59"&gt;&lt;h3&gt;特征融合&lt;/h3&gt;&lt;p&gt;特征融合是指将不同的特征融合，要区别对待密集型变量和稀疏型变量。 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-89154eb1d05095710ec9d96192474ef6.png" data-rawwidth="611" data-rawheight="134"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-69a7a2a787669ed9a72efd22c36747df.png" data-rawwidth="531" data-rawheight="126"&gt;&lt;p&gt;当我们把特征融合好以后，可以开始机器学习的建模过程了，在这里我们都是选择以决策树为基学习器的集成算法，主要有： &lt;/p&gt;&lt;ul&gt;&lt;li&gt;RandomForestClassifier&lt;/li&gt;&lt;li&gt;RandomForestRegressor&lt;/li&gt;&lt;li&gt;ExtraTreesClassifier&lt;/li&gt;&lt;li&gt;ExtraTreesRegressor&lt;/li&gt;&lt;li&gt;XGBClassifier&lt;/li&gt;&lt;li&gt;XGBRegressor但是不能直接把没有经过规范化的数值变量直接用线性模型拟合，可以用scikitlearn里面的&lt;strong&gt;规范化（Normalized）&lt;/strong&gt;和&lt;strong&gt;标准化（StandardScaler）&lt;/strong&gt;命令分别对密集和稀疏的数据进行相应的处理。&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;特征降维和特征选择&lt;/h3&gt;&lt;p&gt;如果以上方式处理后的数据可以产生一个优秀的模型，那就可以直接进行参数调整了。如果不行则还要继续进行特征降维和特征选择。降维的方法有以下几种：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a994abcea4bab5ec498a5e1010c8890d.png" data-rawwidth="547" data-rawheight="158"&gt;简单起见，这里不考虑LDA和QDA。对于高维数据来说，PCA是常用的降维方式，对于图像数据一般我们选用10~15组主成分，当然如果模型效果会提升的话也可以选择更多的主成分。对于其他类型的数据我们一般选择50~60个主成分。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f88985c9186ec217a9e5fa4f77e87819.png" data-rawwidth="533" data-rawheight="93"&gt;文字变量转变为稀疏矩阵后进行奇异值分解，奇异值分解对应scikit learn库中的TruncatedSVD语句。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-db5aa99256a24a68e67f071e32eec5c1.png" data-rawwidth="535" data-rawheight="110"&gt;一般在TF-IDF中SVD主成分的数目大约在120~200之间，但是也可以采用更多的成分，但是相应的计算成本也会增加。在特征降维之后我们可以进行建模的训练过程了，但是有的时候如果这样降维后的结果仍不理想，可以进行特征选择：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-fd94e51c03ce073dd8a32e8b8be11a88.png" data-rawwidth="543" data-rawheight="156"&gt;特征选择也有很多实用的方法，比如说常用的向前或向后搜索。那就是一个接一个地把特征加入模型训练，如果加入一个新的特征后模型效果不好，那就不加入这一特征。直到选出最好的特征子集。对于这种方法有一个提升的方式是&lt;a href="https://github.com/abhishekkrthakur/greedyFeatureSelection" data-editable="true" data-title="用AUC作为评价指标"&gt;用AUC作为评价指标&lt;/a&gt;，当然这个提升也不是尽善尽美的，还是需要实际应用进行改善和调整的。还有一种特征选择的方式是在建模的过程中就得到了最佳特征子集。比如我们可以观察logit模型的系数或者拟合一个随机森林模型从而直接把这些甄选后的特征用在其它模型中。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-e5ebb4859ca5bfa4fad12bc2c62ae2c9.png" data-rawwidth="527" data-rawheight="107"&gt;在上面的处理中应该选择一个小的estimator数目这样不会导致过拟合。还可以用梯度提升算法来进行特征选择，在这里我们建议用xgboost的库而不是sklearn库里面的梯度提升算法，因为前者速度快且有着更好的延展性。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-e19417a5546f66fad1a8654761c4e953.png" data-rawwidth="534" data-rawheight="114"&gt;对于稀疏的数据集我们可以用随机森林、xgboost或卡方等方式来进行特征选择。下面的例子中我们用了卡方的方法选择了20个特征出来。当然这个参数值20也是可以进一步优化的。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-262c04afcbb8625145ff89273c9c66d9.png" data-rawwidth="537" data-rawheight="111"&gt;&lt;p&gt;同样，以上我们用的所有方法都要记录储存用以交叉验证。 &lt;/p&gt;&lt;h3&gt;模型选择和参数调整&lt;/h3&gt;&lt;p&gt;一般而言，常用的机器学习模型有以下几种，我们将在这些模型中选择最好的模型：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;分类问题&lt;ul&gt;&lt;li&gt;随机森林&lt;/li&gt;&lt;li&gt;梯度提升算法（GBM）&lt;/li&gt;&lt;li&gt;Logistic 回归&lt;/li&gt;&lt;li&gt;朴素贝叶斯分类器&lt;/li&gt;&lt;li&gt;支持向量机&lt;/li&gt;&lt;li&gt;k临近分类器&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;回归问题&lt;ul&gt;&lt;li&gt;随机森林&lt;/li&gt;&lt;li&gt;梯度提升算法（GBM）&lt;/li&gt;&lt;li&gt;线性回归&lt;/li&gt;&lt;li&gt;岭回归&lt;/li&gt;&lt;li&gt;Lasso&lt;/li&gt;&lt;li&gt;支持向量回归&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;下表中展示了每种模型分别需要优化的参数，这其中包含的问题太多太多了。究竟参数取什么值才最优，很多人往往有经验但是不会甘愿把这些秘密分享给别人。但是在这里我会把我的经验跟大家分享。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-fbaebe0e8a9f3ee4230e12a304c8b746.png" data-rawwidth="785" data-rawheight="800"&gt;&lt;blockquote&gt;&lt;p&gt;RS*是指没有一个确切的值提供给大家。 &lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;在我看来上面的这些模型基本会完爆其他的模型，当然这只是我的一家之言。下面是上述过程的一个总结，主要是强调一下要保留训练的结果用来给验证集验证，而不是重新用验证集训练！&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-46a7880b2130eab916ddd2e2d4830c3b.png" data-rawwidth="800" data-rawheight="444"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7ebb886225a95536fac0bc0785d92882.png" data-rawwidth="800" data-rawheight="184"&gt;&lt;/p&gt;&lt;p&gt;在我长时间的实践过程中，我发现这些总结出来的规则和框架还是很有用的，当然在一些极其复杂的工作中这些方法还是力有不逮。生活从来不会完美，我们只能尽自身所能去优化，机器学习也是一样。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;原文作者：Abhishek Thakur原文链接：&lt;a href="http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/?sukey=3997c0719f151520eec92d4e7022d429d473ec41dadc46d670acc94d1f94abd4231fd776bc3c18126eea39e6b5a67350" data-editable="true" data-title="Approaching (Almost) Any Machine Learning Problem" class=""&gt;Approaching (Almost) Any Machine Learning Problem&lt;/a&gt;译者：Cup&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22833471&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Sun, 09 Oct 2016 14:24:17 GMT</pubDate></item><item><title>机器学习系列-Logistic回归：我看你像谁 （下篇）</title><link>https://zhuanlan.zhihu.com/p/22692266</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-cb216179f2b856f69d299982cb5bb153_r.png"&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;作者：向日葵&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;Logistic回归&lt;/h2&gt;&lt;p&gt;书接上回，在我们有了最小二乘法与极大似然估计做基础之后，这样我们就做好了Logistic回归的准备，渐渐的进入到我们的主题Logistic回归。 很多都属于分类的问题了，邮件（垃圾邮件/非垃圾邮件），肿瘤（良性/恶性）。二分类问题，可以用如下形式来定义它： y∈{0,1},其中0属于负例，1属于正例。 现在来构造一种状态，一个向量来代表肿瘤（良性/恶性）和肿瘤大小的关系。 &lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-569e40432a36bb5c6524ec03d501892a.png" data-rawwidth="819" data-rawheight="413"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-56666b6e14927636bd71b71ba81d2e64.png" data-rawwidth="818" data-rawheight="302"&gt;Sigmoid 函数在有个很漂亮的“S"形，如下图所示（引自维基百科）：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b6c0a14d298c4857cabc80bb27aecba1.png" data-rawwidth="738" data-rawheight="492"&gt;综合上述两式，我们得到逻辑回归模型的数学表达式：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-8c8064514b0d41ebd9f3222d4fec169d.png" data-rawwidth="885" data-rawheight="355"&gt;Cost函数和J函数如下，它们是基于最大似然估计推导得到的。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-2d30ec70ae197c7aba524cb38db64134.png" data-rawwidth="884" data-rawheight="197"&gt;下面详细说明推导的过程：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-2a3c65b90fd23715566dee0420567baf.png" data-rawwidth="882" data-rawheight="348"&gt;最大似然估计就是求使l(θ)取最大值时的θ，其实这里可以使用梯度上升法求解，求得的θ就是要求的最佳参数。但是，在Andrew Ng的课程中将J(θ)取为下式，即：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-36071a33031756d36876df600daaa644.png" data-rawwidth="885" data-rawheight="178"&gt;梯度下降法求的最小值 &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-631267a13f028b13e3c75b07a8b963f2.png" data-rawwidth="886" data-rawheight="755"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-d00dbcb3572b291fb75efe1cf84648d3.png" data-rawwidth="885" data-rawheight="133"&gt;&lt;h4&gt;向量化Vectorization&lt;/h4&gt;&lt;p&gt;Vectorization是使用矩阵计算来代替for循环，以简化计算过程，提高效率。 如上式，Σ(...)是一个求和的过程，显然需要一个for语句循环m次，所以根本没有完全的实现vectorization。 &lt;/p&gt;&lt;p&gt;下面介绍向量化的过程： 约定训练数据的矩阵形式如下，x的每一行为一条训练样本，而每一列为不同的特称取值： &lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-ce662d1d53f309092795720afdefdca1.png" data-rawwidth="881" data-rawheight="323"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-7d70c69c5a409fa9c2357dce8910fcc8.png" data-rawwidth="884" data-rawheight="390"&gt;&lt;p&gt;Logistic回归的推导过程，采用的是极大似然法和梯度下降法取得各个参数的迭代过程。以后很多公式的推导也是类似这个过程，机器学习的过程大部分的算法都归结到概率论，如果概率论不是很熟，可以继续温习一下。所以很多人都在总觉，机器学习的问题，归宗到底就是概率论的问题。而采用极大似然的算法，其中隐藏着一个道理：求出来的参数会是最符合我们观察到的结果，实验数据决定了我们的参数。 &lt;/p&gt;&lt;h3&gt;TensorFlow下的Logistic回归&lt;/h3&gt;&lt;p&gt;现在有大量的机器学习的框架，个人开发者，大公司等都有。比较出名的还是FaceBook和谷歌的开源框架。 &lt;/p&gt;&lt;p&gt;TensorFlow是谷歌2015年开源的学习框架，结合了大量的机器学习的算法，官方的文档也比较清楚，开篇的初学者入门讲的就是关于Logistic回归的问题，这里简单的介绍一下，主要是想说明TensorFlow还是属于比较强大的工具，可以进行工具的学习。 &lt;/p&gt;&lt;p&gt;这篇文档的主要介绍如何使用TensorFlow识别MNIST，关于MNIST在之前神经网络的介绍有介绍过。MNIST里存放着一些手写的数据：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-b1f5ef1030bc74cf5ed3396f0f2c64a9.png" data-rawwidth="486" data-rawheight="295"&gt;每个数字都可以用二进制向量数组来表示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-f08519ed8e0c407856860f8e26d19251.png" data-rawwidth="573" data-rawheight="287"&gt;这些数据为神经网络的输入：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-cb216179f2b856f69d299982cb5bb153.png" data-rawwidth="559" data-rawheight="240"&gt;&lt;p&gt;利用Logistic回归的训练求解上面的参数。 代码在&lt;a href="https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py" data-editable="true" data-title="githubusercontent.com 的页面"&gt;https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py&lt;/a&gt;下可以自己参看。 &lt;/p&gt;&lt;h3&gt;总结&lt;/h3&gt;&lt;p&gt;这个章节里介绍了Logistic回归和推导的这个过程，Logistic回归是机器学习里最经常用到的算法，也是最基础的算法，通过推导Logistic回归就能够清楚机器学习的基础知识，后面有些算法的思想也和Logistic回归算法类似。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-ababd5743239a6482b089db6e9472764.png" data-rawwidth="640" data-rawheight="500"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22692266&amp;pixel&amp;useReferer"/&gt;</description><author>Datartisan</author><pubDate>Thu, 29 Sep 2016 12:21:00 GMT</pubDate></item></channel></rss>