<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>ç¨‹åºå‘˜å±±å±…ç¬”è®° - çŸ¥ä¹ä¸“æ </title><link>https://zhuanlan.zhihu.com/coding-future</link><description>ä¸»è¦å’Œç¼–ç¨‹ã€ç®—æ³•æœ‰å…³ï¼Œå¸Œæœ›æœªæ¥èƒ½æœ‰é—²æƒ…é€¸è‡´åŠ ä¸Šé«˜çº§æ‰¯æ·¡çš„å†…å®¹ã€‚</description><lastBuildDate>Thu, 06 Oct 2016 14:17:28 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>100è¡ŒPythonä¸ºPrismaåŒ–çš„å›¾ç‰‡æ¢å¤åŸå§‹è‰²å½©</title><link>https://zhuanlan.zhihu.com/p/21836208</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/4791027d7e93c79278d616f3dae224c7_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;é¦–å…ˆæ„Ÿè°¢è¯„è®ºåŒºä¸­å¤šä½å›¾åƒå¤§ç‰›æŒ‡å‡ºçš„é—®é¢˜ï¼Œä»£ç å¹¶æœªæŠŠä¸€å¼ å›¾ç‰‡åŠ ä¸ŠPrismaçš„æ•ˆæœï¼Œè€Œæ˜¯ä¸€ä¸ªç±»ä¼¼äºPrismaçš„åå¤„ç†ã€‚æ–‡ç« å’Œæ ‡é¢˜éƒ½å·²ä½œä¿®æ”¹ï¼Œå¦‚å‘ç°å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿æŒ‡æ•™ï¼Œè°¢è°¢ã€‚&lt;/p&gt;&lt;p&gt;----------------- æ­£æ–‡åˆ†å‰²çº¿ -----------------&lt;/p&gt;&lt;p&gt;å›¾åƒè¢«&lt;a href="https://deepart.io/" data-editable="true" data-title="Deepart"&gt;Deepart&lt;/a&gt;ç­‰ç±»ä¼¼Prismaçš„å·¥å…·å¤„ç†åï¼Œå¯ä»¥åŠ ä¸Šå„ç§é£æ ¼ã€‚ä½†å¯èƒ½å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå³åŸå§‹å›¾ç‰‡çš„é¢œè‰²ä¹Ÿä¸¢å¤±äº†ï¼Œä¾‹å¦‚ï¼š&lt;/p&gt;&lt;p&gt;çº½çº¦å¤œæ™¯å›¾ç‰‡ --&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/2d0076cdfc1d57db127997bfde942fbd.jpg" data-rawwidth="600" data-rawheight="338"&gt;&lt;p&gt;æ¯•åŠ ç´¢é£æ ¼å›¾ç‰‡ --&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ba14b947b225d9e5c59520a814376944.jpg" data-rawwidth="1179" data-rawheight="1536"&gt;åˆæˆå›¾ç‰‡ --&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/0d4e420fbb6aa742a7873ac3b0a6913c.jpg" data-rawwidth="600" data-rawheight="338"&gt;&lt;p&gt;åˆæˆåçš„å›¾ç‰‡ä¸­ï¼ŒåŸå§‹å›¾ç‰‡çš„é¢œè‰²ä¸¢å¤±æ‰äº†ï¼Œä¸‹é¢æœ‰ä¸€ä¸ªç®€å•çš„åŠæ³•ï¼Œå¯ä»¥å°†åˆæˆå›¾ç‰‡çš„æ ·å¼ä¸åŸå§‹å›¾ç‰‡çš„è‰²å½©åˆæˆï¼Œè®©å›¾ç‰‡æ—¢å…·æœ‰æ¯•åŠ ç´¢çš„é£æ ¼ï¼Œåˆä¿ç•™åŸå›¾çš„é¢œè‰²ã€‚&lt;/p&gt;&lt;p&gt;--- &lt;a href="https://github.com/pavelgonchar/color-independent-style-transfer" data-editable="true" data-title="æºä»£ç ä¼ é€é—¨" class=""&gt;Githubä¼ é€é—¨&lt;/a&gt; ---&lt;/p&gt;&lt;p&gt;æ•ˆæœå¦‚ä¸‹ï¼š&lt;/p&gt;&lt;p&gt;é‡æ–°å¤„ç†åçš„æ¯•åŠ ç´¢é£æ ¼çº½çº¦å¤œæ™¯å›¾ç‰‡ --&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/4791027d7e93c79278d616f3dae224c7.jpg" data-rawwidth="600" data-rawheight="338"&gt;&lt;/p&gt;&lt;p&gt;--- ä¸‹é¢æ˜¯ä»£ç å’Œæ³¨é‡Š ---&lt;/p&gt;&lt;p&gt;é¦–å…ˆåŠ è½½ä»¥ä¸‹åº“ï¼š&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;import skimage.io
import tensorflow as tf
from tensorflow.python.framework import ops, dtypes
import numpy as np
from matplotlib import pyplot as plt
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;å®šä¹‰ä¸€äº›å¸¸é‡ï¼š&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;flags = tf.app.flags
FLAGS = flags.FLAGS

""" minsk.jpgæ˜¯åŸå§‹å›¾ç‰‡æ–‡ä»¶(338 * 600) """
flags.DEFINE_string('original', 'New_York_night.jpg', 'Original Image')
""" tmp_950_color.jpgæ˜¯åŒ…å«æŸç§é£æ ¼çš„å›¾ç‰‡æ–‡ä»¶(338 * 600) """
flags.DEFINE_string('styled', 'New_York_night_picasso.jpg', 'Styled Image')

""" Tensorå ä½ç¬¦ï¼Œåé¢ç”¨Feedæ¥è®¡ç®— """
original = tf.placeholder("float", [1, 338, 600, 3])
styled = tf.placeholder("float", [1, 338, 600, 3])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;å®šä¹‰äº’è½¬RGBå’ŒYUVæ ¼å¼çš„æ–¹æ³•ï¼š&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def rgb2yuv(rgb):
    """
    Convert RGB image into YUV https://en.wikipedia.org/wiki/YUV
    """
    rgb2yuv_filter = tf.constant(
        [[[[0.299, -0.169, 0.499],
           [0.587, -0.331, -0.418],
            [0.114, 0.499, -0.0813]]]])
    rgb2yuv_bias = tf.constant([0., 0.5, 0.5])

    temp = tf.nn.conv2d(rgb, rgb2yuv_filter, [1, 1, 1, 1], 'SAME')
    temp = tf.nn.bias_add(temp, rgb2yuv_bias)

    return temp


def yuv2rgb(yuv):
    """
    Convert YUV image into RGB https://en.wikipedia.org/wiki/YUV
    """
    yuv = tf.mul(yuv, 255)
    yuv2rgb_filter = tf.constant(
        [[[[1., 1., 1.],
           [0., -0.34413999, 1.77199996],
            [1.40199995, -0.71414, 0.]]]])
    yuv2rgb_bias = tf.constant([-179.45599365, 135.45983887, -226.81599426])
    temp = tf.nn.conv2d(yuv, yuv2rgb_filter, [1, 1, 1, 1], 'SAME')
    temp = tf.nn.bias_add(temp, yuv2rgb_bias)
    temp = tf.maximum(temp, tf.zeros(temp.get_shape(), dtype=tf.float32))
    temp = tf.minimum(temp, tf.mul(
        tf.ones(temp.get_shape(), dtype=tf.float32), 255))
    temp = tf.div(temp, 255)
    return temp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;å®šä¹‰æ‹¼æ¥å›¾åƒçš„æ–¹æ³•ï¼Œå¯ä»¥æŠŠä¸¤å¼ å›¾ç‰‡æ°´å¹³è¿æ¥èµ·æ¥ï¼š&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def concat_images(imga, imgb):
    """
    Combines two color image ndarrays side-by-side.
    """
    ha, wa = imga.shape[:2]
    hb, wb = imgb.shape[:2]
    max_height = np.max([ha, hb])
    total_width = wa + wb
    new_img = np.zeros(shape=(max_height, total_width, 3), dtype=np.float32)
    new_img[:ha, :wa] = imga
    new_img[:hb, wa:wa + wb] = imgb
    return new_img&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;å‡†å¤‡å°±ç»ªï¼Œå¼€å§‹ä¸»è¦æµç¨‹ã€‚&lt;/p&gt;&lt;p&gt;åœ¨TensorFlowçš„Sessionå¼€å§‹å‰ï¼Œé¦–å…ˆå®šä¹‰ä¸€äº›å¿…è¦çš„Operationå’ŒTensor&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;""" æŠŠå«æœ‰é£æ ¼çš„å›¾åƒstyledè½¬æ¢æˆyuvæ ¼å¼çš„ç°åº¦å›¾styled_grayscale_yuv """
styled_grayscale = tf.image.rgb_to_grayscale(styled)
styled_grayscale_rgb = tf.image.grayscale_to_rgb(styled_grayscale)
styled_grayscale_yuv = rgb2yuv(styled_grayscale_rgb)

""" æŠŠéœ€è¦æ·»åŠ é£æ ¼çš„åŸå§‹å›¾åƒè½¬æ¢æˆyuvæ ¼å¼original_yuv """
original_yuv = rgb2yuv(original)

""" 
ç»„åˆå›¾åƒï¼š
1. styled_grayscale_yuvçš„Yåˆ†é‡
2. original_yuvçš„Uåˆ†é‡
3. original_yuvçš„Våˆ†é‡
"""
combined_yuv = tf.concat(3, [tf.split(3, 3, styled_grayscale_yuv)[0], tf.split(3, 3, original_yuv)[1], tf.split(3, 3, original_yuv)[2]])

""" è½¬æ¢æˆRGBæ ¼å¼ """
combined_rbg = yuv2rgb(combined_yuv)

""" åˆå§‹åŒ– """
init = tf.initialize_all_variables()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;ç„¶åæ­£å¼å¼€å§‹Sessionï¼Œå®ŒæˆOperationsï¼š&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;with tf.Session() as sess:
    sess.run(tf.initialize_all_variables())

    """ è¯»å–éœ€è¦æ·»åŠ é£æ ¼çš„åŸå§‹å›¾ç‰‡ """
    original_image = skimage.io.imread(FLAGS.original) / 255.0
    original_image = original_image.reshape((1, 338, 600, 3))

    """ è¯»å–å«æœ‰é£æ ¼çš„å›¾ç‰‡ """
    styled_image = skimage.io.imread(FLAGS.styled) / 255.0
    styled_image = styled_image.reshape((1, 338, 600, 3))

    """ ä¸ºåŸå§‹å›¾ç‰‡æ·»åŠ ä¸Šé£æ ¼ """
    combined_rbg_ = sess.run(combined_rbg, feed_dict={original: original_image, styled: styled_image})

    """ æ‹¼æ¥å‡ å¹…å›¾ç‰‡å¹¶ä¿å­˜ï¼Œåšä¸ªå¯¹æ¯” """
    summary_image = concat_images(original_image.reshape((338, 600, 3)), styled_image.reshape((338, 600, 3)))
    summary_image = concat_images(summary_image, combined_rbg_[0])
    plt.imsave("results.jpg", summary_image)
&lt;/code&gt;&lt;/pre&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21836208&amp;pixel&amp;useReferer"/&gt;</description><author>ç‹è‹¥æ„š</author><pubDate>Tue, 02 Aug 2016 18:13:56 GMT</pubDate></item><item><title>éç›‘ç£å­¦ä¹ ç®—æ³•--Kå‡å€¼èšç±»</title><link>https://zhuanlan.zhihu.com/p/21558539</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/729eb965e0ada616d3cd58adcc4c7013_r.png"&gt;&lt;/p&gt;æœ¬æ–‡ä»‹ç»æœ€å¸¸è§çš„éç›‘ç£å­¦ä¹ ç®—æ³•ã€ŒKå‡å€¼(K-means)ã€ï¼Œæ€è·¯å’Œä»¿çœŸä¸»è¦å‚è€ƒNgçš„è¯¾ç¨‹ã€ŒMachine Learningã€ä»¥åŠè¯¾ç¨‹ä½œä¸šçš„Matlabçš„å®ç°ã€‚&lt;p&gt;&lt;a href="https://github.com/wrymax/machine-learning-assignments/tree/master/week8/machine-learning-ex7/ex7" data-editable="true" data-title="çŒ›æˆ³ä¸‹è½½æœ¬æ–‡Matlabå®ç°" class=""&gt;çŒ›æˆ³ä¸‹è½½æœ¬æ–‡Matlabå®ç°&lt;/a&gt;ï¼Œå…¶ä¸­ex7.måŒ…å«äº†k-meansçš„ä»£ç ï¼Œex7_pca.måŒ…å«äº†PCAçš„ä»£ç ã€‚&lt;/p&gt;&lt;p&gt;ä»¥ä¸‹æ˜¯æ­£æ–‡ã€‚&lt;/p&gt;&lt;p&gt;&lt;b&gt;K-meansçš„æ„ä¹‰å’Œä½¿ç”¨åœºæ™¯ï¼š&lt;/b&gt;&lt;/p&gt;&lt;p&gt;åœ¨æ— ä»»ä½•å…ˆéªŒåˆ†ç±»çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œè‡ªåŠ¨å‘ç°æ•°æ®é›†çš„åˆ†ç±»ã€‚ä¾‹å¦‚ï¼š&lt;/p&gt;&lt;ol&gt;&lt;li&gt;åœ¨å¤§é‡æ–‡æœ¬ä¸­å‘ç°éšå«çš„è¯é¢˜ï¼›&lt;/li&gt;&lt;li&gt;å‘ç°å›¾åƒä¸­åŒ…å«çš„é¢œè‰²ç§ç±»ï¼›&lt;/li&gt;&lt;li&gt;ä»é”€å”®æ•°æ®ä¸­å‘ç°ä¸åŒç‰¹å¾é¡¾å®¢çš„åˆ†ç±»ã€‚&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;ğŸ‘‡ä¸‹é¢ç®€è¿°ç®—æ³•æ­¥éª¤ã€ä»¿çœŸä»¥åŠç®—æ³•ä¸­çš„é—®é¢˜ ğŸ‘‡&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.1 ç®—æ³•æ­¥éª¤&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;å‡è®¾å¸Œæœ›å°†è®­ç»ƒæ•°æ®é›†&lt;equation&gt;x^{(i)} (i = 1, 2, 3,..., m)&lt;/equation&gt;åˆ†ä¸ºKç±»ï¼›&lt;/li&gt;&lt;li&gt;åœ¨&lt;equation&gt;x^{(i)} (i = 1, 2, 3,..., m)&lt;/equation&gt;ä¸­ï¼Œéšæœºé€‰æ‹©Kä¸ªä½œä¸ºåˆå§‹åˆ†ç±»çš„å›¾å¿ƒ(centroids) &lt;equation&gt;\mu _{1}, \mu _{2}, \mu _{3},..., \mu _{K}&lt;/equation&gt;ï¼›&lt;/li&gt;&lt;li&gt;éå†&lt;equation&gt;x^{(i)}&lt;/equation&gt;ï¼Œè®¡ç®—å‡ºå’Œæ¯ä¸ª&lt;equation&gt;x&lt;/equation&gt;è·ç¦»æœ€è¿‘çš„å›¾å¿ƒ&lt;equation&gt;\mu ^{(i)}&lt;/equation&gt;ï¼Œè®°å½•å½“å‰&lt;equation&gt;x&lt;/equation&gt;å±äºç¬¬&lt;equation&gt;i&lt;/equation&gt;ç±»ï¼›&lt;/li&gt;&lt;li&gt;éå†Kç§åˆ†ç±»ï¼Œåˆ†åˆ«è®¡ç®—ä¸Šä¸€æ­¥ä¸­ï¼Œåˆ’å½’å…¶ä¸­çš„æ‰€æœ‰&lt;equation&gt;x&lt;/equation&gt;ç‚¹çš„ä¸­å¿ƒç‚¹ï¼Œå°†è¯¥ç‚¹è®¾ç½®ä¸ºæœ¬åˆ†ç±»ä¸­å¿ƒçš„å›¾å¿ƒï¼›&lt;/li&gt;&lt;li&gt;è¿­ä»£ä¸Šä¸¤æ­¥ï¼Œç›´åˆ°å›¾å¿ƒä½ç½®æ”¶æ•›ã€‚&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;1.2 Matlabä»¿çœŸï¼šK-meansè¿‡ç¨‹&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/87d35141e80060aea0eecb2f094454a6.jpg" data-rawwidth="570" data-rawheight="493"&gt;&lt;p&gt;                                         (æˆ‘åšäº†ä¸ªgifåŠ¨å›¾ï¼Œå¯èƒ½éœ€è¦æˆ³ä¸€ä¸‹å®ƒæ‰ä¼šåŠ¨èµ·æ¥)&lt;/p&gt;&lt;p&gt;&lt;b&gt;1.3 éšæœºåˆå§‹åŒ– Random Initialisation &lt;/b&gt;&lt;/p&gt;&lt;p&gt;ç”±äºåˆå§‹çš„å›¾å¿ƒæ˜¯éšæœºé€‰æ‹©çš„ï¼ŒK-meanså¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜è€Œå¯¼è‡´æœ€ç»ˆçš„å›¾å¿ƒæ— æ³•æ”¶æ•›åˆ°åˆé€‚çš„ä½ç½®ã€‚å¯ä»¥ä½¿ç”¨éšæœºåˆå§‹åŒ–æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š&lt;/p&gt;&lt;ul&gt;&lt;li&gt;å¤šæ¬¡è¿è¡ŒK-meansç®—æ³•ï¼Œè®¡ç®—&lt;equation&gt;c^{(1)},..., c^{(m)}, \mu _{1},..., \mu _{k}&lt;/equation&gt;ï¼›&lt;/li&gt;&lt;li&gt;è®¡ç®—Cost Function &lt;equation&gt;J = (c^{(1)},..., c^{(m)}, \mu _{1},..., \mu _{k})&lt;/equation&gt;ï¼Œå‡½æ•°ä»£è¡¨äº†èšç±»çš„å¤±çœŸç¨‹åº¦ï¼›&lt;/li&gt;&lt;li&gt;é€‰æ‹©Jæœ€å°çš„é‚£ä¸€ç»„åˆå§‹åŒ–ä»¥åŠæœ€ç»ˆçš„è®¡ç®—ç»“æœã€‚&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;1.4 é€‰æ‹©èšç±»æ•°é‡&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;ä½¿ç”¨ã€Œè‚˜æ–¹æ³•ã€Elbow Method&lt;/li&gt;&lt;ul&gt;&lt;li&gt;é€æ¸å¢åŠ Kï¼Œå¹¶åˆ†åˆ«è®¡ç®—Cost Function Jï¼Œå¯»æ‰¾Jè¾ƒå°çš„Kï¼Œå¦‚å›¾ï¼š&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/392ddcffe91d6e90c0316e0e5e5cb47d.jpg" data-rawwidth="620" data-rawheight="321"&gt;&lt;li&gt;ç†è®ºä¸Šï¼Œå½“åˆ†ç±»æ•°é‡Kå¢å¤§æ—¶ï¼ŒJå°†é€æ¸å˜å°ï¼›&lt;/li&gt;&lt;li&gt;ä½†ä¹Ÿå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜çš„é—®é¢˜ï¼Œå¯¼è‡´kå¢å¤§æ—¶ï¼ŒJåè€Œå¢å¤§ã€‚æ­¤æ—¶éœ€è¦é‡æ–°éšæœºåˆå§‹åŒ–åå†æ¬¡è®¡ç®—ã€‚&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Ngæ¨èçš„åŠæ³•ï¼šå‘è‡ªå·±æé—®ã€Œæˆ‘ä¸ºä»€ä¹ˆè¦ä½¿ç”¨K-meansã€ï¼Ÿå……åˆ†ç†è§£èšç±»çš„éœ€æ±‚ä»¥åŠèšç±»åèƒ½å‘ä¸‹æ¸¸è´¡çŒ®ä»€ä¹ˆä¸œè¥¿ï¼Œå¾€å¾€èƒ½ä»ä¸­å‘ç°çœŸæ­£åˆé€‚çš„èšç±»æ•°é‡ã€‚ä¾‹å¦‚ä¸‹å›¾ï¼š&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/deee28966bfec9af3579e48b3118f7c4.jpg" data-rawwidth="620" data-rawheight="329"&gt;&lt;li&gt;æ¨ªè½´ä¸ºè¡£æœåº—é¡¾å®¢çš„èº«é«˜ï¼Œçºµè½´ä¸ºé¡¾å®¢çš„ä½“é‡ã€‚å½“ä½ ç†è§£äº†é¡¾å®¢å¯èƒ½åˆ†ä¸ºã€ŒSã€Mã€Lã€ä¸‰ç±»ï¼Œæˆ–è€…ã€ŒXSã€Sã€Mã€Lã€XLã€äº”ç±»æ—¶ï¼Œæœ€ç»ˆé€‰æ‹©çš„åˆ†ç±»æ•°é‡å¯èƒ½æ¯”è¾ƒmake senseï¼Œè€Œä¸æ˜¯å®Œå…¨ä¾èµ–Cost Functionç®—å‡ºä¸€ä¸ª10ç±»æˆ–è€…4ç±»ï¼Œæœ€ç»ˆå¹¶æ²¡æœ‰å¤ªå¤§å®é™…æ„ä¹‰ã€‚&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;1.5 å®è·µï¼šä½¿ç”¨K-meanså‹ç¼©å›¾åƒ&lt;/b&gt;&lt;/p&gt;&lt;p&gt;æ€è·¯ï¼š&lt;/p&gt;&lt;ul&gt;&lt;li&gt;å¯¹ä¸€ä¸ªRGBå›¾åƒæ‰§è¡ŒK-meansç®—æ³•ï¼Œå¯»æ‰¾èƒ½æè¿°å›¾åƒçš„16ç§ä¸»è¦é¢œè‰²åˆ†ç±»ï¼›&lt;/li&gt;&lt;li&gt;å°†æ¯ä¸ªåƒç´ ç‚¹èšç±»åˆ°è¿™16ç§é¢œè‰²åˆ†ç±»ä¸­ï¼Œå¹¶åˆ†åˆ«æ›¿æ¢ä¸ºå¯¹åº”åˆ†ç±»çš„é¢œè‰²ï¼›&lt;/li&gt;&lt;li&gt;å¯¹æ¯ä¸ªåƒç´ ä½¿ç”¨é¢œè‰²çš„ç´¢å¼•æ¥ä»£æ›¿3ç»´çš„RGBäº®åº¦å€¼ï¼Œç”±æ­¤å¯ä»¥å°†å›¾åƒçš„å¤§å°å‹ç¼©åˆ°&lt;equation&gt;\frac{1}{6}&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;ä»¿çœŸå›¾ï¼š&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/037cf01978ec1bedb8ae69ebf5505221.jpg" data-rawwidth="620" data-rawheight="342"&gt;ä»£ç åœ¨æ–‡ç« å¼€å§‹å¤„çš„Githubé“¾æ¥ä¸­ï¼Œæ‰§è¡Œex7å³å¯è§‚æµ‹åˆ°ç»“æœã€‚&lt;/p&gt;&lt;p&gt;å¦‚éœ€è½¬è½½ï¼Œè¯·é™„ä¸ŠåŸé“¾æ¥ï¼Œè°¢è°¢ã€‚&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21558539&amp;pixel&amp;useReferer"/&gt;</description><author>ç‹è‹¥æ„š</author><pubDate>Mon, 11 Jul 2016 17:31:53 GMT</pubDate></item><item><title>SVMæ”¯æŒå‘é‡æœº</title><link>https://zhuanlan.zhihu.com/p/21481541</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/52c8d933d79140e2fb6710994a5d581d_r.jpg"&gt;&lt;/p&gt;æœ¬æ–‡ä»æ¦‚å¿µå’Œä¸€äº›ç®—æ³•ç»†èŠ‚ä¸Šä»‹ç»Support Vector Machineæ”¯æŒå‘é‡æœºçš„åŸç†å’ŒåŠŸèƒ½ã€‚é€šè¿‡å¯¹æ¯”Logistic Regressionï¼ŒæœŸæœ›å¯¹SVMæœ‰ä¸€ä¸ªå¿«é€Ÿçš„å…¥é—¨ä»‹ç»ã€‚å®ç°ä»£ç å¯å‚è€ƒï¼š&lt;a href="https://github.com/wrymax/machine-learning-assignments/tree/master/week7/machine-learning-ex6/ex6" data-editable="true" data-title="SVMç¤ºä¾‹"&gt;SVMç¤ºä¾‹&lt;/a&gt;ï¼Œåœ¨Matlab / Octaveä¸­åŠ è½½ï¼Œæ‰§è¡Œex6å’Œex6_spamå³å¯ã€‚&lt;b&gt;1) ç›®çš„å’Œåœºæ™¯&lt;/b&gt;SVMæ˜¯ä¸€ç§ç»å…¸çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Œä¸»è¦è§£å†³æ•°æ®åˆ†ç±»é—®é¢˜ã€‚åŸç†æ˜¯åŸºäºè®­ç»ƒå¤§é‡å·²æœ‰çš„æ•°æ®å’Œå·²çŸ¥çš„åˆ†ç±»æƒ…å†µï¼Œè®¡ç®—å‡ºä¸€ä¸ªé¢„æµ‹æ¨¡å‹ã€‚ç„¶åå¯ä»¥åœ¨æ¨¡å‹ä¸Šè·‘æ–°çš„æ•°æ®ï¼Œä»è€Œåˆ¤æ–­æ–°æ•°æ®åº”å½“å½’äºå“ªä¸€ç±»ã€‚SVMé€‚ç”¨äºçº¿æ€§å’Œéçº¿æ€§çš„åœºæ™¯ã€‚SVMçš„å…¸å‹ä½¿ç”¨åœºæ™¯å¦‚ï¼š
&lt;ol&gt;&lt;li&gt;æˆ¿ä»·ä¼°ç®—&lt;ol&gt;&lt;li&gt;æ ¹æ®è¿‡å»åå¹´æ¥æˆ¿ä»·å’Œæˆ¿å±‹é¢ç§¯ã€å§å®¤æ•°é‡ã€å½“åœ°æ¶ˆè´¹æ°´å¹³ç­‰ç­‰å„ç§å› ç´ æ•°æ®ï¼Œå°†æˆ¿å±‹åˆ†ä¸ºã€Œè±ªå®…ã€ã€ã€Œä¸­ç­‰ã€ã€ã€Œç»æµå‹ä½æˆ¿ã€ã€ã€Œè´«æ°‘çªŸã€ç­‰å‡ ç±»ï¼›&lt;/li&gt;&lt;li&gt;ä½¿ç”¨SVMè®­ç»ƒè¿™äº›æ•°æ®å¾—å‡ºä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥ç”¨æ¥é¢„æµ‹åœ¨æ–°çš„æ¡ä»¶ä¸‹ï¼ŒæŸä¸ªä½æˆ¿å¯ä»¥è¢«åˆ’å½’åˆ°å“ªç§åˆ†ç±»ï¼Œä»·å€¼åŒºé—´å¤šå°‘ã€‚&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;åƒåœ¾é‚®ä»¶åˆ†ç±»å™¨ï¼š&lt;ol&gt;&lt;li&gt;è·å–å¯ç–‘çš„spam emailå…³é”®è¯åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼šBuyã€nowç­‰ï¼ˆå®é™…Spam Corpuså¯ä»¥å‚è€ƒä½¿ç”¨&lt;a href="https://spamassassin.apache.org/publiccorpus/" data-editable="true" data-title="Apache Spam Assassin"&gt;Apache Spam Assassin&lt;/a&gt;ï¼‰ï¼›&lt;/li&gt;&lt;li&gt;æ”¶é›†å¤§é‡çš„spamå’Œéspamé‚®ä»¶æ•°æ®ï¼Œå°†å…¶ä¸­åŒ…å«çš„å¯ç–‘spamå…³é”®è¯æ‰¾å‡ºå¹¶æ ‡è®°åœ¨ç‰¹å¾å‘é‡ä¸­ï¼Œç”¨SVMè®­ç»ƒè¿™äº›æ•°æ®ï¼Œå¾—å‡ºä¸€ä¸ªæ¨¡å‹ï¼Œç”¨æ¥åˆ¤æ–­ä¸€å°æ–°çš„é‚®ä»¶æ˜¯å¦ä¸ºä¸€ä¸ªåƒåœ¾é‚®ä»¶ã€‚&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;b&gt;2) æ ¸å¿ƒæ€è·¯&lt;/b&gt;2.1 SVMçš„æ€è·¯å¯ä»¥ç”±Logistic Regressionæ¼”å˜è€Œæ¥ï¼Œä¾ç„¶æ˜¯é€šè¿‡Gradient Descentè®¡ç®—CostFunctionçš„å…¨å±€æå°å€¼ï¼Œå¾—åˆ°thetaçŸ©é˜µï¼Œä½œä¸ºåˆ†ç±»æ¨¡å‹ã€‚
å¦‚å›¾ï¼š
&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/31d0113a2e3d05cc56d4044a8d37e828.jpg" data-rawwidth="620" data-rawheight="337"&gt;2.2 SVMå’ŒLogistic Regressionçš„ä¸åŒä¹‹å¤„ï¼š
&lt;ol&gt;&lt;li&gt;æŠŠLRä¸­çš„Sigmoidå‡½æ•° &lt;equation&gt;log(h_{\theta }(x) ) = log(g(z))&lt;/equation&gt; è¢«ä¿®æ”¹ä¸ºä¸Šå›¾ä¸­çš„å‡½æ•° &lt;equation&gt;cost_{0} (z)&lt;/equation&gt; å’Œ &lt;equation&gt;cost_{1} (z)&lt;/equation&gt; å…¶ä¸­&lt;equation&gt;z = \theta^{T}X &lt;/equation&gt;&lt;ol&gt;&lt;li&gt;å½“åˆ†ç±»ç»“æœ y = 1 æ—¶ï¼ŒæœŸæœ›z &amp;gt;= 1ï¼Œæ­¤æ—¶&lt;equation&gt;cost_{1} (z)&lt;/equation&gt; = 0ï¼Œæ€»ä½“cost functionè¾¾åˆ°æœ€å°&lt;/li&gt;&lt;li&gt;å½“åˆ†ç±»ç»“æœ y = 0 æ—¶ï¼ŒæœŸæœ›z &amp;lt;= -1ï¼Œæ­¤æ—¶&lt;equation&gt;cost_{0} (z)&lt;/equation&gt; = 0ï¼Œæ€»ä½“cost functionè¾¾åˆ°æœ€å°&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;å…³äºRegularisationå‚æ•°Cï¼š&lt;ol&gt;&lt;li&gt;Logistic Regressionåœ¨cost functionçš„ååŠéƒ¨åˆ†ä¸Šä¹˜ä»¥ä¸€ä¸ªlambdaï¼ˆä¸€ä¸ªè¾ƒå°çš„å€¼ï¼Œä¾‹å¦‚0.03ï¼‰&lt;/li&gt;&lt;li&gt;SVMåœ¨cost functionçš„å‰åŠéƒ¨åˆ†ä¹˜ä»¥ä¸€ä¸ªè¾ƒå¤§çš„Cï¼ˆå¯ä»¥ç†è§£ä¸º1 / lambdaï¼Œä¾‹å¦‚1, 100, 1000ï¼‰&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;2.3 Kernel æ ¸å‡½æ•°
&lt;ol&gt;&lt;li&gt;SVMå¼•å…¥Kernelçš„æ¦‚å¿µï¼Œå°†&lt;equation&gt;z = \theta^{T}X &lt;/equation&gt;ä¸­çš„Xå˜ä¸ºä¸€ä¸ªf = g(x)ï¼Œå½¢å¼å˜ä¸ºï¼š&lt;equation&gt;z = \theta^{T} f&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;æœ€å¸¸è§çš„Kernel&lt;ol&gt;&lt;li&gt;Linear Kernel çº¿æ€§æ ¸&lt;ol&gt;&lt;li&gt;å®é™…ä¸Šå°±æ˜¯ä¸ä½¿ç”¨Kernelï¼Œç›´æ¥ä½¿ç”¨&lt;equation&gt;\theta^{T} X&lt;/equation&gt;ä½œä¸ºcost functionçš„è‡ªå˜é‡&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Gaussian Kernel é«˜æ–¯æ ¸&lt;ol&gt;&lt;li&gt;å¯¹Xå¥—ç”¨é«˜æ–¯å‡½æ•°ï¼Œå¦‚ä¸‹å›¾&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/5cdc926f1d45540203539c5b8a905eb9.png" data-rawwidth="3636" data-rawheight="1996"&gt;&lt;li&gt;å›¾ä¾‹å‚æ•°è¯´æ˜&lt;ol&gt;&lt;li&gt;ã€Œlã€æ˜¯ã€Œlandmarkã€çš„ç¼©å†™ï¼Œåœ¨å®é™…ç¼–ç¨‹ä¸­ï¼Œç›´æ¥æŠŠtraining setæ”¾ç½®åˆ°landmarkä½ç½®ä¸Šå³å¯&lt;/li&gt;&lt;li&gt;é«˜æ–¯æ ¸å‡½æ•° f å®é™…ä¸Šè®¡ç®—äº†xå’ŒæŸä¸ªlandmark lä¹‹é—´çš„è¿‘ä¼¼åº¦ï¼Œè¿™ä¸ªè¿‘ä¼¼åº¦ç”¨æ¬§æ°è·ç¦»æ¥æè¿°ï¼ˆè®¡ç®—å‘é‡å†…ç§¯ï¼‰&lt;/li&gt;&lt;li&gt;&lt;equation&gt;\sigma &lt;/equation&gt;ç”¨äºæ§åˆ¶é«˜æ–¯æ ¸çš„é™¡å³­ç¨‹åº¦ï¼Œ&lt;equation&gt;\sigma &lt;/equation&gt;è¶Šå¤§ï¼Œå‡½æ•°è¶Šå¹³æ»‘&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;å…¶ä»–æ ¸å‡½æ•°&lt;ol&gt;&lt;li&gt;Polynomial Kernel&lt;/li&gt;&lt;li&gt;String Kernel&lt;/li&gt;&lt;li&gt;chi-square kernel&lt;/li&gt;&lt;li&gt;histogram kernel&lt;/li&gt;&lt;li&gt;intersection kernel&lt;/li&gt;&lt;li&gt;æ ¹æ®Ngçš„è¯´æ³•ï¼Œè¿™äº›çœ‹èµ·æ¥å¾ˆå±Œï¼Œä½†ä»–è‡ªå·±åŸºæœ¬æ²¡æ€ä¹ˆç”¨è¿‡â€¦ å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œéƒ½æ˜¯ç”¨Gaussian Kernel&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;2.4 Logistic Regressionã€SVMå’Œç¥ç»ç½‘ç»œçš„ä½¿ç”¨åœºæ™¯å¯¹æ¯”
å‡è®¾ n ä¸ºç‰¹å¾çš„æ•°é‡ï¼Œmä¸ºè®­ç»ƒé›†çš„æ•°é‡
&lt;ol&gt;&lt;li&gt;å½“næ¯”må¤§å¾—å¤šæ—¶ï¼Œå¦‚n = 10000ï¼Œm = 1000ï¼Œå»ºè®®ä½¿ç”¨Logistic Regressionï¼Œæˆ–è€…SVM + çº¿æ€§æ ¸&lt;/li&gt;&lt;li&gt;å¦‚æœnå¾ˆå°ï¼Œmä¸å¤§ä¸å°ï¼Œä¾‹å¦‚n = 1 ~1000, m = 10 ~ 10000ï¼Œå»ºè®®ä½¿ç”¨SVM + é«˜æ–¯æ ¸&lt;/li&gt;&lt;li&gt;å¦‚æœnå¾ˆå°ï¼Œmå¾ˆå¤§ï¼Œä¾‹å¦‚n = 1 ~1000, m = 50000+ï¼Œå»ºè®®æ·»åŠ æ›´å¤šçš„ç‰¹å¾ï¼Œç„¶åä½¿ç”¨Logistic Regressionï¼Œæˆ–è€…SVM + çº¿æ€§æ ¸&lt;/li&gt;&lt;li&gt;å„ç§æƒ…å†µä¸‹ï¼Œç¥ç»ç½‘ç»œéƒ½å·¥ä½œçš„ä¸é”™ï¼Œä½†æ˜¯æ¯”SVMè®­ç»ƒçš„é€Ÿåº¦æ…¢ä¸€äº›&lt;/li&gt;&lt;/ol&gt;&lt;b&gt;3) å®è·µä¸­çš„ä¸€äº›æ³¨æ„äº‹é¡¹
&lt;/b&gt;&lt;ol&gt;&lt;li&gt;éœ€è¦é€šè¿‡å¤šæ¬¡äº¤å‰æ£€éªŒæ¥ç¡®å®šåˆé€‚çš„regularisationå‚æ•°Cå’Œé«˜æ–¯æ ¸å‚æ•°&lt;equation&gt;\sigma &lt;/equation&gt;ï¼Œæ–¹æ³•ï¼š&lt;ol&gt;&lt;li&gt;è®¾å®šä¸€ç»„å¤‡é€‰çš„Cå’Œ&lt;equation&gt;\sigma &lt;/equation&gt;ï¼Œç”¨ä»–ä»¬ä¸¤ä¸¤ç»„åˆå¾ªç¯è®­ç»ƒSVM model&lt;/li&gt;&lt;li&gt;ç”¨è®­ç»ƒå¥½çš„modelè·‘Cross Validationæ•°æ®é›†ï¼Œè®¡ç®—errors&lt;/li&gt;&lt;li&gt;æ‰¾å‡ºerrorsæœ€ä½çš„ä¸€ç»„Cå’Œsigma&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;å‡ ä¸ªå¥½ç”¨çš„SVMå·¥å…·åŒ…&lt;ol&gt;&lt;li&gt;liblinear&lt;/li&gt;&lt;li&gt;libsvm&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21481541&amp;pixel&amp;useReferer"/&gt;</description><author>ç‹è‹¥æ„š</author><pubDate>Mon, 04 Jul 2016 12:39:20 GMT</pubDate></item><item><title>åº”ç”¨æœºå™¨å­¦ä¹ ç®—æ³•çš„ä¸€äº›å…·ä½“å»ºè®®</title><link>https://zhuanlan.zhihu.com/p/21449423</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/01d6654b5392b00f4e777791d7e80c48_r.jpg"&gt;&lt;/p&gt;æœ¬æ–‡æ¥è‡ªäºå­¦ä¹ Ngçš„Machine Learningè¯¾ç¨‹ç¬”è®°ã€‚å­¦ä¹ æ—¶ç”¨è‹±æ–‡è®°å½•ï¼Œå†™æœ¬æ–‡æ—¶åŠ ä¸Šäº†éƒ¨åˆ†ä¸­æ–‡è§£é‡Šï¼Œå°½å¯èƒ½çš„å£è¯­åŒ–äº†ã€‚å¦å¤–éƒ¨åˆ†åè¯å¯èƒ½ç¿»è¯‘çš„ä¸å¤ªå¯¹ï¼Œå¦‚æœæ‚¨å‘ç°äº†è¯·ä¸åæŒ‡æ­£ï¼Œè°¢è°¢ã€‚å…·ä½“å®ç°ä»£ç è¯·å‚è€ƒï¼š&lt;a href="https://github.com/wrymax/machine-learning-assignments/tree/master/week6/machine-learning-ex5/ex5" class=""&gt;https://github.com/wrymax/machine-learning-assignments/tree/master/week6/machine-learning-ex5/ex5&lt;/a&gt;ä½¿ç”¨Matlab / OctaveåŠ è½½ï¼Œè¿è¡Œex5å³å¯ã€‚æœ¬æ–‡ä¸»è¦æŒ‡å‡ºäº†ä¸€äº›æœºå™¨å­¦ä¹ å®è·µä¸­çš„æŠ€å·§ï¼Œcodingå‰å¿…å¤‡ã€‚Let's get started.&lt;b&gt;å…³é”®åè¯ï¼š&lt;/b&gt;&lt;ol&gt;&lt;li&gt;è®­ç»ƒæ•°æ®é›† Training Set&lt;/li&gt;&lt;li&gt;è®­ç»ƒæ•°æ®é›†çš„ä»£ä»·å‡½æ•° Jtrain(theta)&lt;/li&gt;&lt;li&gt;äº¤å‰éªŒè¯æ•°æ®é›† Cross Validation&lt;/li&gt;&lt;li&gt;äº¤å‰éªŒè¯æ•°æ®é›†çš„ä»£ä»·å‡½æ•° Jcv(theta)&lt;/li&gt;&lt;li&gt;æµ‹è¯•æ•°æ®é›† Test&lt;/li&gt;&lt;li&gt;æµ‹è¯•æ•°æ®é›†çš„ä»£ä»·å‡½æ•° Jtest(theta)&lt;/li&gt;&lt;li&gt;é¢„æµ‹è¯¯å·®ã€ä»£ä»· errorï¼ˆä»£ä»·å‡½æ•°çš„è®¡ç®—ç»“æœï¼‰&lt;/li&gt;&lt;li&gt;åå·® Bias&lt;/li&gt;&lt;li&gt;ç‰¹å¾å¤šæ ·æ€§ã€æ–¹å·® Variance&lt;/li&gt;&lt;li&gt;æ¬ æ‹Ÿåˆ Under-Fitting&lt;/li&gt;&lt;li&gt;è¿‡æ‹Ÿåˆ Over-Fitting&lt;/li&gt;&lt;li&gt;æ­£åˆ™åŒ– Regularisation &lt;/li&gt;&lt;li&gt;æŸ¥å‡†ç‡ Precision&lt;/li&gt;&lt;li&gt;å¬å›ç‡ Recall&lt;/li&gt;&lt;li&gt;Få€¼ F Score&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;&lt;b&gt;æ ¸å¿ƒæ¦‚å¿µ&lt;/b&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Machine Learning Diagnostic æœºå™¨å­¦ä¹ è¯Šæ–­&lt;/b&gt;&lt;ol&gt;&lt;li&gt;To get better avenue, you may:&lt;ol&gt;&lt;li&gt;Collect larger training examples æ”¶é›†æ›´å¤šçš„è®­ç»ƒæ•°æ® =&amp;gt; è§£å†³overfitting&lt;/li&gt;&lt;li&gt;Get additional features è·å¾—å’Œä½¿ç”¨é¢å¤–çš„ç‰¹å¾ =&amp;gt; è§£å†³high bias&lt;/li&gt;&lt;li&gt;Add polynomial features æ·»åŠ å¤šé¡¹å¼ç‰¹å¾ =&amp;gt; è§£å†³high bias&lt;/li&gt;&lt;li&gt;Reduce features å‡å°‘ç‰¹å¾ =&amp;gt; è§£å†³overfitting&lt;/li&gt;&lt;li&gt;Increase lambda å¢å¤§lambda =&amp;gt; è§£å†³overfitting&lt;/li&gt;&lt;li&gt;Decrease lambda =&amp;gt; è§£å†³underfitting&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;However, it may takes you 6 month to train data, and nothing to gain.&lt;/li&gt;&lt;li&gt;Machine Learning Diagnostic is: æœºå™¨å­¦ä¹ è¯Šæ–­æ€è·¯å¦‚ä¸‹ï¼š&lt;ol&gt;&lt;li&gt;A test that you can run to gain insight what is / isnâ€™t working with a learning algorithm, and gain guidance as to how best to improve its performance. è¿™æ˜¯ä¸€ç§æµ‹è¯•æ–¹æ³•ï¼Œä½ å¯ä»¥æ®æ­¤å°è¯•å¹¶æ”¹è¿›ç®—æ³•çš„æ€§èƒ½&lt;/li&gt;&lt;li&gt;It takes time to implement. è¿™å¾—èŠ±ç‚¹æ—¶é—´ã€‚ã€‚&lt;/li&gt;&lt;li&gt;It sometimes rule out certain courses of action (changes to your learning algorithm) as being unlikely to improve its performance significantly. æœ‰å¯èƒ½è¿™å¹¶æ²¡æœ‰ä»€ä¹ˆåµç”¨&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Training/Testing Procedure è®­ç»ƒ/æµ‹è¯•è¿‡ç¨‹&lt;ol&gt;&lt;li&gt;Split dataset into 7:3, 70% of which is going to be the training set, 30% of which is going to be the testing set. åˆ‡åˆ†æ•°æ®é›†ï¼š70%ä½œä¸ºè®­ç»ƒæ•°æ®ï¼Œ30%ä½œä¸ºæµ‹è¯•æ•°æ®&lt;/li&gt;&lt;li&gt;Learn parameter theta from training data. ä½¿ç”¨è®­ç»ƒæ•°æ®ä¹ å¾—ä½ éœ€è¦çš„theta&lt;/li&gt;&lt;li&gt;Compute test set error: ä½¿ç”¨åˆšåˆšè®­ç»ƒå‡ºæ¥çš„Cost Functionæ¥è·‘ä¸€ä¸‹æµ‹è¯•æ•°æ®é›†ï¼Œå¾—åˆ°æµ‹è¯•æ•°æ®é›†çš„è¯¯å·®&lt;ol&gt;&lt;li&gt;Jtest(theta) = CostFunction(training_data) =&amp;gt; the square deviation equation &lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Model Selection and Train/Validation/Test Sets æ¨¡å‹é€‰æ‹©ä¸è®­ç»ƒ/æ ¡éªŒ/æµ‹è¯•æ•°æ®é›†&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Degree of Polynomial å¤šé¡¹å¼çº§æ•°ï¼ˆä¸€ä¸ªæˆ‘ä»¬éœ€è¦å…³æ³¨çš„æŒ‡æ ‡ï¼‰ï¼Œè¿™ä¸ªä¸œè¥¿å°±æ˜¯ä¸‹å›¾ä¸­çš„ã€Œdã€&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/4a1f050207306becba5c5573957d0625.png" data-rawwidth="3796" data-rawheight="2060"&gt;&lt;li&gt;å¦‚ä¸Šå›¾ï¼Œæˆ‘ä»¬ä»d = 1åˆ°d = 10ï¼Œåˆ†åˆ«ç”¨testæ•°æ®é›†è®¡ç®—Cost Functionï¼Œç„¶åé€‰å–ä¸€ä¸ªæœ€ä¼˜åŒ–çš„dï¼ˆä¸Šå›¾ä¸­é€‰æ‹©äº†d = 5ï¼‰ï¼Œåé¢å‡ éƒ¨è®²äº†å…·ä½“æ€ä¹ˆå®æ–½è¿™ä¸ªè¿‡ç¨‹&lt;/li&gt;&lt;li&gt;Evaluating your hypothesis è¯„ä¼°ä½ çš„é¢„æµ‹å‡½æ•°&lt;ol&gt;&lt;li&gt;Training Set - 60% of data set æŠŠ60%çš„æ•°æ®è®¾ç½®ä¸ºè®­ç»ƒæ•°æ®é›†&lt;/li&gt;&lt;li&gt;Cross Validation Set (CV) - 20% of data set æŠŠ20%çš„æ•°æ®è®¾ç½®ä¸ºäº¤å‰éªŒè¯æ•°æ®é›†&lt;/li&gt;&lt;li&gt;Test Set - 20% of data set æŠŠ20%çš„æ•°æ®è®¾ç½®ä¸ºæµ‹è¯•æ•°æ®é›†&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Use Validation Set to select the model ä½¿ç”¨äº¤å‰éªŒè¯æ•°æ®é›†æ¥é€‰æ‹©æ¨¡å‹&lt;ol&gt;&lt;li&gt;Compute cost function J(theta) by CV in different degrees&lt;/li&gt;&lt;li&gt;Choose the minimal one as the target degree, as below, d = 4&lt;/li&gt;&lt;li&gt;Estimate generalisation erro for test set Jtest(theta(4))&lt;/li&gt;&lt;li&gt;ä»¥ä¸Šè¿™æ®µè‹±æ–‡å°±æ˜¯æ­¥éª¤1 - 3äº†&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Bias vs. Variance åå·® vs. ç‰¹å¾å¤šæ ·æ€§ï¼ˆoræ–¹å·®ï¼Ÿï¼‰&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Bias =&amp;gt; ç‰¹å¾å€¼ä»¥å¤–çš„ä»£ä»·å‡½æ•°åå·®&lt;ol&gt;&lt;li&gt;High Bias =&amp;gt; high lambda =&amp;gt; Under-fitting&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Variance =&amp;gt; ç‰¹å¾å¤šæ ·æ€§&lt;ol&gt;&lt;li&gt;High Variance =&amp;gt; low lambda =&amp;gt; Overfitting&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Diagnosing Bias vs. Variance è¯Šæ–­ï¼šåå·®é«˜ä½å’Œç‰¹å¾å€¼å¤§å°&lt;ol&gt;&lt;li&gt;Set coordinates of (degree_of_polynomial_d, error); è®¾ç½®ä¸€ä¸ªæ¨ªè½´ä¸ºç‰¹å¾æ•°é‡ï¼ˆå¤šé¡¹å¼ç»´åº¦ï¼‰dï¼Œçºµè½´ä¸ºé¢„æµ‹è¯¯å·®errorçš„åæ ‡ç³»&lt;/li&gt;&lt;li&gt;Train the 60% data set, draw a curve which will converge when d goes bigger; ç”¨60%çš„æ•°æ®é›†ä½œä¸ºè®­ç»ƒé›†åˆï¼Œè®¡ç®—J(theta)ï¼Œå½“å¤šé¡¹å¼ç»´åº¦dï¼ˆç‰¹å¾æ•°ï¼‰å¢å¤§æ—¶ï¼Œerrorä¼šå‡å°‘ï¼Œå‘0æ”¶æ•›&lt;/li&gt;&lt;li&gt;Run prediction with Cross-Validation-Set by trained model, there will be an overfitting point, after which Jcv(theta) will continuously arise. ä½¿ç”¨ä»è®­ç»ƒé›†ä¸­è·å¾—çš„æ¨¡å‹ï¼Œè®¡ç®—äº¤å‰éªŒè¯æ•°æ®é›†çš„é¢„æµ‹è¯¯å·®ï¼Œä¼šå‘ç°è¿‡æ‹Ÿåˆé—®é¢˜ã€‚&lt;/li&gt;&lt;li&gt;å¦‚ä¸‹å›¾ï¼š&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d1a6d5dfd58b7cdbd0410487a5596c4f.png" data-rawwidth="3744" data-rawheight="2000"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Question é—®é¢˜æ¥äº†...&lt;/b&gt;&lt;ol&gt;&lt;li&gt;How can we figure out it is suffered from &lt;b&gt;Bias&lt;/b&gt;, or from &lt;b&gt;Variance&lt;/b&gt;? æˆ‘ä»¬å¦‚ä½•çŸ¥é“æ¨¡å‹çš„é—®é¢˜å‡ºåœ¨åå·®è¿˜æ˜¯ç‰¹å¾å¤šæ ·æ€§ä¸Šï¼Ÿ&lt;ol&gt;&lt;li&gt;Bias =&amp;gt; Underfitting, as on the left part of the diagram åå·®å¾€å¾€å’Œæ¬ æ‹Ÿåˆç›¸å…³ï¼Œå¦‚å›¾å·¦åŠéƒ¨åˆ†ï¼Œç‰¹å¾æ•°è¿‡å°‘&lt;ol&gt;&lt;li&gt;Jtraining(theta) will be high è®­ç»ƒé›†çš„ä»£ä»·å‡½æ•°errorä¼šå¾ˆå¤§&lt;/li&gt;&lt;li&gt;Jcv(theta) â‰ˆ Jtraining(theta) äº¤å‰éªŒè¯çš„ä»£ä»·å‡½æ•°errorçº¦ç­‰äºè®­ç»ƒé›†ï¼Œä¹Ÿå¾ˆå¤§&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Variance =&amp;gt; Overfitting, as on the right part of the diagram ç‰¹å¾å¤šæ ·æ€§å¾€å¾€å’Œè¿‡æ‹Ÿåˆç›¸å…³ï¼Œå¦‚å›¾å³åŠéƒ¨åˆ†ï¼Œç‰¹å¾æ•°é‡è¿‡å¤š&lt;ol&gt;&lt;li&gt;Jtraining(theta) will be low è®­ç»ƒé›†çš„ä»£ä»·å‡½æ•°errorè¶Šæ¥è¶Šä½&lt;/li&gt;&lt;li&gt;Jcv(theta) &amp;gt;&amp;gt;( much higher than ) Jtraining(theta) äº¤å‰éªŒè¯æ•°æ®é›†çš„ä»£ä»·å‡½æ•°erroråœ¨ç»è¿‡ä¸€ä¸ªæå°å€¼åå¼€å§‹ä¸Šå‡ï¼Œæœ€ç»ˆè¿œå¤§äºè®­ç»ƒé›†çš„é”™è¯¯ã€‚è¿™æ˜¯å…¸å‹çš„è¿‡æ‹Ÿåˆç‰¹å¾ï¼šå¯¹æ–°æ•°æ®çš„fittingæ€§èƒ½éå¸¸å·®ã€‚&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;å¦‚ä¸‹å›¾ï¼š&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a0db6ba0b74fd2185a95d4ff0974c079.png" data-rawwidth="3788" data-rawheight="1920"&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Choosing the regularisation parameter lambda é€‰æ‹©æ­£åˆ™åŒ–å‚æ•°lambda&lt;ol&gt;&lt;li&gt;Try lambda from small to large, like ä»å°åˆ°å¤§ï¼Œå°è¯•lambda&lt;ol&gt;&lt;li&gt;0&lt;/li&gt;&lt;li&gt;0.01&lt;/li&gt;&lt;li&gt;0.02&lt;/li&gt;&lt;li&gt;0.04&lt;/li&gt;&lt;li&gt;0.08&lt;/li&gt;&lt;li&gt;...&lt;/li&gt;&lt;li&gt;10&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Pick up best fitting lambda of Cross-Validation cost function, say Theta(5) é€‰æ‹©ä¸€ä¸ªå¯¹äº¤å‰éªŒè¯æ•°æ®é›†çš„ä»£ä»·å‡½æ•°æ‹Ÿåˆæœ€å¥½çš„lambdaå€¼ï¼Œä¾‹å¦‚theta(5)&lt;/li&gt;&lt;li&gt;Compute J(theta) by the test data set è®¡ç®—æµ‹è¯•æ•°æ®é›†çš„ä»£ä»·å‡½æ•°&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2d7c1a31ce3a19e9aadc47c4ac0b73aa.png" data-rawwidth="3764" data-rawheight="2024"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Bias/Variance as a function of the regularisation parameter lambda ä»¥lambdaä¸ºå‚æ•°çš„åå·®/æ–¹å·®å‡½æ•°&lt;/b&gt;&lt;ol&gt;&lt;li&gt;When lambda is 0 å½“lambdaä¸º0æ—¶&lt;ol&gt;&lt;li&gt;you can fit the training set relatively well, since there is no regularisation. è®­ç»ƒé›†ä¼šæ‹Ÿåˆçš„ç›¸å¯¹ä¸é”™ï¼Œå› ä¸ºæ²¡æœ‰åšä»»ä½•çš„æ­£åˆ™åŒ–&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;When lambda is small å½“lambdaå¾ˆå°æ—¶&lt;ol&gt;&lt;li&gt;You get a small value of Jtrain è®­ç»ƒé›†çš„é¢„æµ‹è¯¯å·®ä¹Ÿå¾ˆå°&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;When lambda goes large å½“lambdaå˜å¤§æ—¶&lt;ol&gt;&lt;li&gt;The bias becomes larger, so Jtrain goes much larger åå·®è¶Šæ¥è¶Šå¤§ï¼Œè®­ç»ƒé›†çš„é¢„æµ‹è¯¯å·®ä¼šæ˜¾è‘—å¢å¤§&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/b473417172cd29c59f9a5655c6889a16.png" data-rawwidth="3820" data-rawheight="2016"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Learning Curves å­¦ä¹ æ›²çº¿&lt;/b&gt;&lt;ol&gt;&lt;li&gt;When training set number ã€Œmã€grows&lt;ol&gt;&lt;li&gt;It is much harder to fit, so training error grows è®­ç»ƒé›†å¢å¤§ï¼Œä»£ä»·å‡½æ•°è¶Šæ¥è¶Šéš¾æ‹Ÿåˆæ‰€æœ‰çš„æ•°æ®é›†ï¼Œerrorä¼šå¢å¤§&lt;/li&gt;&lt;li&gt;As examples grows, it does better at generalising to new examples, so Cross-Validation error decreases è®­ç»ƒé›†å¢å¤§æ—¶ï¼Œä»£ä»·å‡½æ•°å½’çº³æ–°å…ƒç´ çš„æ€§èƒ½ä¼šæ›´å¥½ï¼Œå› æ­¤cross-validationçš„é”™è¯¯ç‡ä¼šä¸‹é™&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c92244a19933e73a4ed2dc2a8d93911f.jpg" data-rawwidth="620" data-rawheight="326"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;High Bias é«˜åå·®çš„æƒ…å†µ&lt;ol&gt;&lt;li&gt;High Bias means low Variance, so h(theta) would a low-dimensional function, which cannot fit all the dataset well é«˜åå·®ä»£è¡¨é¢„æµ‹å¤šé¡¹å¼çš„ç»´åº¦è¿‡ä½ï¼Œå› æ­¤å¾ˆéš¾é¢„æµ‹æ•´ä¸ªæ•°æ®é›†&lt;/li&gt;&lt;li&gt;When m is small, Jcv is high and Jtrain is low, while both of them will converge to similar value when dataset grows large enough å½“æ•°æ®é›†çš„æ€»æ•°å¾ˆå°æ—¶ï¼Œäº¤å‰éªŒè¯é›†çš„é¢„æµ‹è¯¯å·®å¾ˆå¤§ï¼Œè®­ç»ƒæ•°æ®é›†çš„é¢„æµ‹è¯¯å·®å¾ˆå°ï¼›ä½†å½“mè¶Šæ¥è¶Šå¤§ï¼Œä¸¤è€…å°†è¶Šæ¥è¶Šæ¥è¿‘&lt;/li&gt;&lt;li&gt;Both the error of Jcv and Jtrain would be fairly HIGH äº¤å‰éªŒè¯å’Œè®­ç»ƒæ•°æ®é›†çš„Jéƒ½ä¼šå¾ˆå¤§&lt;/li&gt;&lt;li&gt;Conclusion ç»“è®º&lt;ol&gt;&lt;li&gt;If a learning algorithm is suffering from high bias, getting more training data will not(by itself) help much å½“å­¦ä¹ ç®—æ³•å­˜åœ¨é«˜åå·®é—®é¢˜æ—¶ï¼Œè®­ç»ƒæ›´å¤šçš„æ•°æ®æ— æ³•è§£å†³é—®é¢˜ï¼ˆJä¼šæ”¶æ•›äºä¸€ä¸ªå¾ˆé«˜çš„é”™è¯¯å€¼ï¼Œä¸å†ä¸‹é™ï¼‰&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/a2561bd221232925c4625490b7ba316f.jpg" data-rawwidth="620" data-rawheight="330"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;High Variance é«˜ç‰¹å¾å¤šæ ·æ€§çš„æƒ…å†µ&lt;ol&gt;&lt;li&gt;High Variance means low lambda and the polynomial hypothesis function has many many features é«˜ç‰¹å¾å¤šæ ·æ€§è¯´æ˜æ­£åˆ™åŒ–å‚æ•°lambdaå¾ˆå°ï¼Œæ­¤å¤–é¢„æµ‹å¤šé¡¹å¼æœ‰éå¸¸å¤šçš„ç‰¹å¾&lt;/li&gt;&lt;li&gt;When m is small å½“è®­ç»ƒæ•°æ®é‡må¾ˆå°æ—¶&lt;ol&gt;&lt;li&gt;Jtrain is small; as m grows up, Jtrain becomes larger too, but the training set error would still be pretty low è®­ç»ƒé›†çš„Jä¹Ÿå¾ˆå°ï¼›ä½†æ˜¯å½“è®­ç»ƒæ•°æ®è¶Šæ¥è¶Šå¤šæ—¶ï¼Œç”±äºé¢„æµ‹å‡½æ•°çš„ç»´åº¦è¿‡é«˜ï¼Œæ‹Ÿåˆå¼€å§‹å˜å¾—å›°éš¾ï¼ŒJtrainé€æ¸ä¸Šå‡ï¼Œä½†æ˜¯ä»ç„¶æ˜¯ä¸€ä¸ªéå¸¸å°çš„æ•°å€¼&lt;/li&gt;&lt;li&gt;Jcv is large; as m grows up, Jtrain becomes smaller and coverage to a value similar with Jtrain äº¤å‰éªŒè¯æ•°æ®é›†çš„Jå¾ˆå¤§ï¼ˆé«˜ç‰¹å¾å¤šæ ·æ€§å¸¦æ¥çš„è¿‡æ‹Ÿåˆä¼šå¯¼è‡´é¢„æµ‹å‡½æ•°å¯¹æ ·æœ¬å¤–çš„æ•°æ®ç‚¹é¢„æµ‹åå·®å¾ˆå¤§ï¼‰ï¼›å½“må¢å¤§æ—¶ï¼ŒJcvä¼šé€æ¸ä¸‹é™&lt;/li&gt;&lt;li&gt;The indicative diagnostic that we have a high variance problem æˆ‘ä»¬é‡åˆ°é«˜ç‰¹å¾å¤šæ ·æ€§é—®é¢˜çš„ä¸€ä¸ªè±¡å¾æ€§æŒ‡æ ‡ï¼š&lt;ol&gt;&lt;li&gt;With m becomes larger, there is a large gap between the training error and cross-validation error éšç€må¢å¤§ï¼Œè®­ç»ƒæ•°æ®é›†çš„é¢„æµ‹è¯¯å·®å’Œäº¤å‰éªŒè¯æ•°æ®é›†çš„é¢„æµ‹è¯¯å·®ä¹‹é—´ä¼šå­˜åœ¨ä¸€ä¸ªå¾ˆå¤§çš„ç©ºç™½&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Conclusion ç»“è®º&lt;ol&gt;&lt;li&gt;If learning algorithm is suffering from high variance, getting moe training data is likely to help å½“å­¦ä¹ ç®—æ³•å­˜åœ¨é«˜ç‰¹å¾å¤šæ ·æ€§ä¸ºé¢˜æ˜¯ï¼Œä½¿ç”¨æ›´å¤šçš„è®­ç»ƒæ•°æ®å¯èƒ½ä¼šæœ‰å¸®åŠ©&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/ffe4cfa386a14c901823696c246575f3.jpg" data-rawwidth="620" data-rawheight="330"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Deciding what to try next (revisited) å†³å®šä¸‹ä¸€æ­¥åšä»€ä¹ˆ&lt;/b&gt;&lt;ol&gt;&lt;li&gt;When debugging a learning algorithm, you find your model makes unacceptably large errors in its prediction, what to do next? å½“è°ƒè¯•ä¸€ä¸ªå­¦ä¹ ç®—æ³•æ—¶ï¼Œä½ å‘ç°ä½ çš„é¢„æµ‹æ¨¡å‹å¾—å‡ºäº†ä¸å¯æ¥å—çš„é«˜è¯¯å·®ï¼Œä¸‹ä¸€æ­¥è¯¥æ€ä¹ˆåŠï¼Ÿ&lt;ol&gt;&lt;li&gt;Get more training examples ä½¿ç”¨æ›´å¤šçš„è®­ç»ƒæ•°æ®&lt;ol&gt;&lt;li&gt;When Jcv is much higher than Jtrain, it fixes high variance å½“Jcvæ¯”Jtrainå¤§çš„å¤šæ—¶ï¼Œå®ƒå¯ä»¥è§£å†³é«˜ç‰¹å¾å¤šæ ·æ€§çš„é—®é¢˜&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try smaller sets of features å°è¯•å‡å°‘ç‰¹å¾æ•°é‡&lt;ol&gt;&lt;li&gt;It fixes high variance problem too å‡å°‘ä¸€éƒ¨åˆ†ç”¨å¤„ä¸å¤§çš„ç‰¹å¾å¯ä»¥è§£å†³é«˜ç‰¹å¾å¤šæ ·æ€§çš„é—®é¢˜&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try getting additional features å°è¯•ä½¿ç”¨é¢å¤–çš„ç‰¹å¾&lt;ol&gt;&lt;li&gt;Maybe the model is under-fitting (high bias), try additional features can make the model fitting training set better é«˜è¯¯å·®ä¹Ÿæœ‰å¯èƒ½æ˜¯å› ä¸ºç‰¹å¾æ•°é‡å¤ªå°‘äº†ï¼Œå› æ­¤ä½¿ç”¨é¢å¤–çš„ç‰¹å¾å¯ä»¥è§£å†³é«˜åå·®çš„é—®é¢˜&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try adding polynomial features å°è¯•æ·»åŠ å¤šé¡¹å¼ç‰¹å¾ (x1 * x1, x2 * x2, x1 * x2, etc)&lt;ol&gt;&lt;li&gt;Which also solves high bias problem ä¹Ÿæ˜¯å¯ä»¥è§£å†³é«˜åå·®çš„é—®é¢˜ï¼ˆæ¬ æ‹Ÿåˆï¼‰&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try decreasing lambda å°è¯•é™ä½æ­£åˆ™åŒ–å‚æ•°lambda&lt;ol&gt;&lt;li&gt;Which also solves high bias problem ä¹Ÿæ˜¯å¯ä»¥è§£å†³é«˜åå·®çš„é—®é¢˜ï¼ˆæ¬ æ‹Ÿåˆï¼‰&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Try increasing lambda å°è¯•å¢åŠ æ­£åˆ™åŒ–å‚æ•°lambda&lt;ol&gt;&lt;li&gt;Which also solves high variance problem ä¹Ÿæ˜¯å¯ä»¥è§£å†³é«˜ç‰¹å¾æ•°é‡çš„é—®é¢˜ï¼ˆè¿‡æ‹Ÿåˆï¼‰&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Neural Networks and overfitting ç¥ç»ç½‘ç»œä¸è¿‡æ‹Ÿåˆ&lt;ol&gt;&lt;li&gt;Small neural network å°å‹ç¥ç»ç½‘ç»œ&lt;ol&gt;&lt;li&gt;fewer parameters, more prone to under-fitting ç½‘ç»œå±‚æ•°å°‘ã€ç¥ç»å…ƒæ•°é‡å°‘ï¼Œæ›´æ˜“å¯¼è‡´æ¬ æ‹Ÿåˆ&lt;/li&gt;&lt;li&gt;Computationally cheaper è®¡ç®—èµ„æºæ¶ˆè€—å°‘&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Large neural network å¤§å‹ç¥ç»ç½‘ç»œ&lt;ol&gt;&lt;li&gt;Type1. few layers, lot of units ç±»å‹1ï¼šå±‚æ•°å°‘ï¼Œæ¯å±‚çš„ç¥ç»å…ƒå¤š&lt;/li&gt;&lt;li&gt;Type2. few units, lot of layers ç±»å‹2ï¼šå±‚æ•°å¤šï¼Œæ¯å±‚çš„ç¥ç»å…ƒå°‘&lt;/li&gt;&lt;li&gt;more parameters, more prone to over-fitting å‚æ•°å¤šï¼Œæ›´æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆ&lt;/li&gt;&lt;li&gt;Computationally more expensive è®¡ç®—èµ„æºæ¶ˆè€—å¤š&lt;/li&gt;&lt;li&gt;Use regularisation to address overfitting ä½¿ç”¨æ­£åˆ™åŒ–æ¥è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜&lt;/li&gt;&lt;li&gt;Try one layer, two layers and three layers.. and compute Jcv(theta) to decide how many layers you will use å°è¯•1å±‚ã€2å±‚ã€3å±‚â€¦ å¹¶è®¡ç®—äº¤å‰éªŒè¯ä»£ä»·å‡½æ•°ï¼Œæ®æ­¤æ¥é€‰æ‹©æœ€åˆé€‚çš„ç¥ç»ç½‘ç»œå±‚æ•°&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;b&gt;Machine Learning System Design æœºå™¨å­¦ä¹ ç³»ç»Ÿè®¾è®¡&lt;/b&gt;&lt;b&gt;ï¼ˆæœ¬æ®µä¸­æ¥¼ä¸»æ‡’ç™Œå‘ä½œä¸æƒ³æ‰“å­—ã€‚ã€‚ã€‚ç›´æ¥è´´Ngçš„æˆªå›¾äº†ï¼‰&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Building a Spam Classifier æ„å»ºä¸€ä¸ªåƒåœ¾é‚®ä»¶åˆ†ç±»å™¨&lt;ol&gt;&lt;li&gt;Prioritising What to Work On &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/230490f942998f0541070e26c91f0dfa.png" data-rawwidth="1832" data-rawheight="838"&gt;&lt;/li&gt;&lt;li&gt;Recommended Approach æ¨èçš„æ–¹æ³•&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/5c6966a18a75ba8633f77780dbaf382e.png" data-rawwidth="1756" data-rawheight="722"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/81a7af0b71a42cf0752b0ef9784ac2ab.png" data-rawwidth="3636" data-rawheight="1872"&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Handling Skewed Data å¤„ç†æ­ªæ›²/åæ–œçš„æ•°æ®&lt;ol&gt;&lt;li&gt;Error Metrics for Skewed Data æ­ªæ›²æ•°æ®çš„é”™è¯¯åº¦é‡æ–¹æ³•&lt;ol&gt;&lt;li&gt;æŸ¥å‡†ç‡ Precision&lt;ol&gt;&lt;li&gt;Precision = TruePositive / (No. of predicted positive)&lt;/li&gt;&lt;li&gt;No. of predicted positive = TruePositive + FalsePositive&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;å¬å›ç‡ Recall&lt;ol&gt;&lt;li&gt;Recall = TruePositive / (No. of actual positive)&lt;/li&gt;&lt;li&gt;No. of actual positive = TruePositive + FalseNegative&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Trading off Precision and Recall æƒè¡¡æŸ¥å‡†ç‡å’Œå¬å›ç‡&lt;ol&gt;&lt;li&gt;High Precision and Low Recall: Suppose we want to predict y = 1 only if very confident &lt;ol&gt;&lt;li&gt;Predict 1 if h(x) &amp;gt;= 0.9&lt;/li&gt;&lt;li&gt;Predict 0 if h(x) &amp;lt; 0.9&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;High Recall and Low Precision: Suppose we want to predict y = 0 only if very confident&lt;ol&gt;&lt;li&gt;Predict 1 if h(x) &amp;gt;= 0.3&lt;/li&gt;&lt;li&gt;Predict 0 if h(x) &amp;lt; 0.3&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/490a48d1d7c4c3ddb1c507066c99301c.png" data-rawwidth="3788" data-rawheight="2020"&gt;&lt;li&gt;F Score Få€¼&lt;ol&gt;&lt;li&gt;For making a decision which pair of Precision and Recall is better&lt;/li&gt;&lt;li&gt;Measure P and R on the &lt;b&gt;cross validation set&lt;/b&gt;, and choose the value of threshold which maximises F score, as below&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/8c912d6f5472417b4bf5a45724cc3108.png" data-rawwidth="3756" data-rawheight="1952"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Using Large Data Sets ä½¿ç”¨å¤§æ•°æ®é›†&lt;ol&gt;&lt;li&gt;é¦–å…ˆé—®è‡ªå·±ä¸¤ä¸ªé—®é¢˜ï¼š&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Question1: Can a human expert accurately predict y from given features x?  äººç±»ç –å®¶ä½¿ç”¨ç›®å‰çš„ç‰¹å¾æŒ‡æ ‡ï¼Œèƒ½å¦å‡†ç¡®é¢„æµ‹å‡ºç»“æœï¼Ÿ&lt;ol&gt;&lt;li&gt;e.g. Can a realtor predict the housing price simply based on housing sizes? ä¾‹å¦‚ï¼šä¸€ä¸ªæˆ¿åœ°äº§ç»çºªäººæ˜¯å¦èƒ½ä»…ä»…åŸºäºæˆ¿å±‹é¢ç§¯é¢„æµ‹å‡ºæˆ¿ä»·ï¼Ÿ&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Question2: Can we get a large training set to train with many features? æˆ‘ä»¬èƒ½ä¸èƒ½æåˆ°æ›´å¤šçš„æ•°æ®æ¥è®­ç»ƒå¦‚æ­¤å¤šçš„ç‰¹å¾ï¼Ÿ&lt;/li&gt;&lt;/ol&gt;&lt;li&gt;å¦‚æœå¯ä»¥ï¼Œé‚£ä¹ˆï¼š&lt;/li&gt;&lt;ol&gt;&lt;li&gt;Use a learning algorithm with many parameters ( many features / many hidden layers )  å¤šè®¾ç½®ä¸€äº›ç‰¹å¾ï¼Œæˆ–è€…ç¥ç»ç½‘ç»œä¸­çš„éšè—å±‚&lt;/li&gt;&lt;li&gt;Use a very large training set å°½é‡å¤šçš„ä½¿ç”¨è®­ç»ƒæ•°æ®&lt;/li&gt;&lt;/ol&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21449423&amp;pixel&amp;useReferer"/&gt;</description><author>ç‹è‹¥æ„š</author><pubDate>Wed, 29 Jun 2016 14:15:41 GMT</pubDate></item><item><title>ç¥ç»ç½‘ç»œçš„å­¦ä¹  / è®­ç»ƒè¿‡ç¨‹</title><link>https://zhuanlan.zhihu.com/p/21381359</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/17db6ca9a6df9e33f302b13568e97dda_r.jpg"&gt;&lt;/p&gt;ä»¥ä¸‹ä¸ºéƒ¨åˆ†å­¦ä¹ ç¬”è®°ã€‚&lt;p&gt;å…·ä½“å®ç°ä»£ç å‚è€ƒ: &lt;a href="https://github.com/wrymax/machine-learning-assignments/tree/master/week5/machine-learning-ex4/ex4"&gt;https://github.com/wrymax/machine-learning-assignments/tree/master/week5/machine-learning-ex4/ex4&lt;/a&gt;&lt;/p&gt;&lt;ol&gt;&lt;li&gt;&lt;b&gt;Cost Function ä»£ä»·å‡½æ•°&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Important Parameters:&lt;ol&gt;&lt;li&gt;L =&amp;gt; Total number of layers in network&lt;/li&gt;&lt;li&gt;Sl =&amp;gt; Number of units ( not counting bias unit ) in layer l&lt;/li&gt;&lt;li&gt;As below, L = 4, S1 = 3, S2 = 5, S4 = SL = 4&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/f168eae6991fd9681da794b9fc3e0b12.png" data-rawwidth="626" data-rawheight="342"&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Two Classification Methods&lt;ol&gt;&lt;li&gt;Binary Classification äºŒå…ƒåˆ†ç±»&lt;ol&gt;&lt;li&gt;y = 0 or 1&lt;/li&gt;&lt;li&gt;SL = K = 1 ( One output unit )&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Multi-class Classification å¤šå…ƒåˆ†ç±»&lt;ol&gt;&lt;li&gt;y is logical vectors, which uses 1 to denote the class&lt;/li&gt;&lt;li&gt;SL = K, K &amp;gt;= 3 ( K output units )&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/62e9dc13f799ec5ec0cb8df167179063.png" data-rawwidth="1890" data-rawheight="992"&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;The Cost Function&lt;ol&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/70270b37927da2357329ae77eef65d6a.png" data-rawwidth="1854" data-rawheight="1038"&gt;&lt;li&gt;J(theta) sum up the cost function in logistic regression of ALL Layers.&lt;/li&gt;&lt;li&gt;Regularisation sum up all Theta elements between each two layers.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Back Propagation å‘åä¼ æ’­&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Compute Gradient ç”¨äºè®¡ç®—æ¢¯åº¦( CostFunctionå¯¹Thetaçš„åå¾®åˆ† )&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/eba219186b9387a30205698194cc0ffe.png" data-rawwidth="1894" data-rawheight="1014"&gt;&lt;/li&gt;&lt;li&gt;Algorithm ç®—æ³•è§£é‡Š&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/d750e6a3a3cf8706181b653b3efe3d22.png" data-rawwidth="1894" data-rawheight="1032"&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;&lt;b&gt;Back Propagation in Practice å‘åä¼ æ’­å®è·µæŠ€å·§&lt;/b&gt;&lt;ol&gt;&lt;li&gt;Learning Algorithm å­¦ä¹ ç®—æ³•&lt;ol&gt;&lt;li&gt;initialTheta&lt;/li&gt;&lt;li&gt;costFunction&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Unrolling Parameters å±•å¼€å‚æ•°&lt;ol&gt;&lt;li&gt;Change matrices into vectors &lt;/li&gt;&lt;li&gt;Change vectors into matrices&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Gradient Checking æ¢¯åº¦æ£€æŸ¥&lt;ol&gt;&lt;li&gt;Use numerical estimate method to compute derivatives&lt;/li&gt;&lt;li&gt;Pros:&lt;ol&gt;&lt;li&gt;It can check is derivatives are correct&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Cons:&lt;ol&gt;&lt;li&gt;It is super slow. &lt;/li&gt;&lt;li&gt;When you make sure back propagation gives similar values as gradient, just turn off it.&lt;/li&gt;&lt;li&gt;Be sure to disable gradient checking code before training your classifier. Or the training process would be super slow.&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Random Initialisation éšæœºåˆå§‹åŒ–&lt;ol&gt;&lt;li&gt;â€œZero Initialisation" does not work in neural network.&lt;/li&gt;&lt;li&gt;Random Initialisation: Symmetry breaking&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Put things together&lt;ol&gt;&lt;li&gt;Training a neural network&lt;ol&gt;&lt;li&gt;Pick a network architecture&lt;ol&gt;&lt;li&gt;Number of input units: Dimension of features x(i)&lt;/li&gt;&lt;li&gt;Number of output units: Number of Classes&lt;/li&gt;&lt;li&gt;Layers:&lt;ol&gt;&lt;li&gt;Number of layers&lt;/li&gt;&lt;li&gt;Units in each layer&lt;ol&gt;&lt;li&gt;Same units number in each layer&lt;/li&gt;&lt;li&gt;Usually the more units the better&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Randomly initialise weights&lt;ol&gt;&lt;li&gt;Small values near zero&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;li&gt;Implement forward propagation to get prediction for any x(i)&lt;/li&gt;&lt;li&gt;Implement code to compote cose function J(theta)&lt;/li&gt;&lt;li&gt;Implement backprop to compute partial derivatives of J(theta)&lt;ol&gt;&lt;li&gt;for i = 1:m&lt;ol&gt;&lt;li&gt;Perform forward propagation and back-propagation using example (x(i), y(i))&lt;/li&gt;&lt;li&gt;Get activations a(l) and delta(l) for l = 2,â€¦,L&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/21381359&amp;pixel&amp;useReferer"/&gt;</description><author>ç‹è‹¥æ„š</author><pubDate>Mon, 20 Jun 2016 00:26:30 GMT</pubDate></item></channel></rss>