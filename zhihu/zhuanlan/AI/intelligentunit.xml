<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>智能单元 - 知乎专栏</title><link>https://zhuanlan.zhihu.com/intelligentunit</link><description>斯坦福CS231n官方教程笔记翻译连载。

深度增强学习领域论文和项目的原创思考和Demo复现。

领域内其他感兴趣论文和项目的原创思考解读。</description><lastBuildDate>Tue, 06 Dec 2016 16:15:10 GMT</lastBuildDate><generator>Ricky</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>深度学习论文阅读路线图 Deep Learning Papers Reading Roadmap</title><link>https://zhuanlan.zhihu.com/p/23080129</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6f589df38509d14f839737645322a011_r.jpg"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;相信很多想入门深度学习的朋友都会遇到这个问题，就是应该看哪些论文。包括我自己，也是花费了大量的时间在寻找文章上。另一方面，对于一些已经入门的朋友，常常也需要了解一些和自己研究方向不同的方向的文章。&lt;/p&gt;&lt;p&gt;因此，这里做了一个深度学习论文阅读路线图，也就是paper list，希望能够帮助大家对深度学习的全貌和具体的方向有一个深入的理解。&lt;/p&gt;&lt;h2&gt;2 路线图的构建原则&lt;/h2&gt;&lt;p&gt;有以下四个原则：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;从整体到局部。即从Survey的文章，影响大局的文章到具体子问题子领域的文章。&lt;/li&gt;&lt;li&gt;从过去到最前沿。即每个topic的文章是按照时间顺序排列的，这样大家就可以清楚的看到这个方向的研究发展脉络。&lt;/li&gt;&lt;li&gt;从通用到应用。即有些深度学习的文章是面向深度学习通用理论，比如Resnet，可以用在任意的神经网络中，而有些文章则是具体应用，比如Image Caption。&lt;/li&gt;&lt;li&gt;面向最前沿。收集的文章会有很多是最新的，甚至就是几天前出来的，这样能保证路线图是最新的。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;每一种topic只选择最有代表性的几篇文章，比如深度增强学习（Deep Reinforcement Learning），这个领域现在有几十篇文章，但只选择几篇，要深入了解甚至做为自己的研究方向，还需要进一步的阅读该领域的文章。&lt;/p&gt;&lt;h2&gt;3 说明&lt;/h2&gt;&lt;p&gt;这个论文阅读路线图选择的文章除了文章本身的影响力和重要性之外，也依赖于本人对文章的理解。因此会有一定的主观性，即我觉得这篇文章好，值得读，所以推荐。这方面需要大家的理解。也欢迎大家提出批评意见以改进。&lt;/p&gt;&lt;p&gt;这个路线图还在完善，会不断更新。&lt;/p&gt;&lt;p&gt;&lt;b&gt;欢迎大家pull requests！（基本要求：一个topic不超过10篇，并且包含当前最前沿和最有影响力的文章，也欢迎增加新的topic）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;路线图在Github上，地址是：&lt;/p&gt;&lt;p&gt;&lt;a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap" data-editable="true" data-title="GitHub - songrotek/Deep-Learning-Papers-Reading-Roadmap: Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!" class=""&gt;GitHub - songrotek/Deep-Learning-Papers-Reading-Roadmap: Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech!&lt;/a&gt;&lt;/p&gt;&lt;p&gt;以下是截图：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-eef22d5319df25855f056b2683df8ff3.png" data-rawwidth="2012" data-rawheight="98"&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-cccfd935d9e3f50f963046c3892b61ab.png" data-rawwidth="1880" data-rawheight="1366"&gt;希望对大家有所帮助！&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/23080129&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Thu, 20 Oct 2016 12:12:51 GMT</pubDate></item><item><title>深层学习为何要“Deep”（上）</title><link>https://zhuanlan.zhihu.com/p/22888385</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-b1c917b1f2616bc51c7d833fdcc0c05d_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;2016年11月22日更新：&lt;a href="https://link.zhihu.com/?target=https%3A//yjango.gitbooks.io/superorganism/content/shen_ceng_wang_luo.html" class="" data-editable="true" data-title="深层神经网络为什么要"&gt;深层神经网络为什么要&lt;/a&gt;deep（下）（修订）&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;结合&lt;a href="https://yjango.gitbooks.io/superorganism/content/xue_xi.html" class="" data-editable="true" data-title="生物学习"&gt;生物学习&lt;/a&gt;与&lt;a href="https://yjango.gitbooks.io/superorganism/content/tong_ji_xue_xi.html" class="" data-title="机器学习" data-editable="true"&gt;机器学习&lt;/a&gt;一起来看&lt;/b&gt;&lt;/p&gt;&lt;p&gt;深层学习开启了人工智能的新时代。不论任何行业都害怕错过这一时代浪潮，因而大批资金和人才争相涌入。但深层学习却以“黑箱”而闻名，不仅调参难，训练难，“新型”网络结构的论文又如雨后春笋般地涌现，使得对所有结构的掌握变成了不现实。我们缺少一个对深层学习合理的认识。&lt;/p&gt;&lt;p&gt;本文就是通过对深层神经网络惊人表现&lt;b&gt;背后原因&lt;/b&gt;的思考，揭示&lt;b&gt;设计一个神经网络的本质&lt;/b&gt;，从而获得一个对“&lt;b&gt;如何设计&lt;/b&gt;网络”的全局指导。由于问题本身过于庞大，我们先把问题拆分成几部分加以思考。&lt;/p&gt;&lt;p&gt;&lt;b&gt;1、神经网络为什么可以用于识别 （已回答）2、神经网络变深后我们获得了什么 &lt;/b&gt;（已回答）3、“过深”的网络的效果又变差的原因 4、“深浅”会影响神经网络表现的背后原因 5、RNN、CNN以及各种不同网络结构的共性是什么 6、设计神经网络的本质是什么 &lt;/p&gt;&lt;p&gt;文章分为&lt;b&gt;上下&lt;/b&gt;两部分。 &lt;b&gt;上篇&lt;/b&gt;涉及的内容是1,2两个问题，是为了理解“深层”神经网络的&lt;b&gt;预备知识&lt;/b&gt;。描述的是&lt;strong&gt;为何能识别&lt;/strong&gt;和&lt;strong&gt;如何训练&lt;/strong&gt;两部分。看完后能明白的是：1、为什么神经网络&lt;strong&gt;能够&lt;/strong&gt;识别，2、训练网络&lt;strong&gt;基本流程&lt;/strong&gt;，以及深层神经网络大家族中其他技术&lt;strong&gt;想要解决的问题&lt;/strong&gt;（并不需要知道具体的解决步骤）。 &lt;/p&gt;&lt;p&gt;文章的理解需要线性代数基础知识，数学零基础的请看&lt;/p&gt;&lt;p&gt;&lt;a href="https://zhuanlan.zhihu.com/p/23361299?refer=YJango" class="" data-editable="true" data-title="串讲 线性代数、概率、熵 - 超有机体 - 知乎专栏"&gt;串讲 线性代数、概率、熵 - 超有机体 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;p&gt;对神经网络有了大致了解后，《深层学习为何要“Deep”（下）》会进一步围绕“深层”二字再次讨论深层学习为何要“Deep”，会讨论CNN、RNN、Transfer learning、distillation training等技术的共性，并解释&lt;strong&gt;设计网络结构的本质&lt;/strong&gt;是什么。&lt;/p&gt;目录&lt;ul&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E4%B8%80%E5%9F%BA%E6%9C%AC%E5%8F%98%E6%8D%A2%E5%B1%82" data-editable="true" data-title="一基本变换层"&gt;一基本变换层&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E4%BA%8C%E7%90%86%E8%A7%A3%E8%A7%86%E8%A7%92" data-editable="true" data-title="二理解视角"&gt;二理解视角&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E6%95%B0%E5%AD%A6%E8%A7%86%E8%A7%92%E7%BA%BF%E6%80%A7%E5%8F%AF%E5%88%86" data-editable="true" data-title="数学视角线性可分"&gt;数学视角线性可分&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E7%89%A9%E7%90%86%E8%A7%86%E8%A7%92%E7%89%A9%E8%B4%A8%E7%BB%84%E6%88%90" data-editable="true" data-title="物理视角物质组成"&gt;物理视角物质组成&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E4%B8%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AE%AD%E7%BB%83" data-editable="true" data-title="三神经网络的训练"&gt;三神经网络的训练&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E5%A6%82%E4%BD%95%E8%AE%AD%E7%BB%83" data-editable="true" data-title="如何训练"&gt;如何训练&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E9%97%AE%E9%A2%98" data-editable="true" data-title="梯度下降的问题"&gt;梯度下降的问题&lt;/a&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#1%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E5%80%BC" data-editable="true" data-title="1局部极小值"&gt;1局部极小值&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#2%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97" data-editable="true" data-title="2梯度的计算"&gt;2梯度的计算&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E5%9F%BA%E6%9C%AC%E6%B5%81%E7%A8%8B%E5%9B%BE" data-editable="true" data-title="基本流程图"&gt;基本流程图&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="http://blog.csdn.net/u010751535/article/details/52739803#%E5%9B%9B%E6%B7%B1%E5%B1%82%E7%9A%84%E6%80%9D%E8%80%83%E7%9C%9F%E7%9A%84%E5%8F%AA%E6%9C%89%E8%BF%99%E4%BA%9B%E5%8E%9F%E5%9B%A0%E5%90%97" data-editable="true" data-title="四深层的思考真的只有这些原因吗"&gt;四深层的思考真的只有这些原因吗&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;一、基本变换：层&lt;/h2&gt;&lt;p&gt;神经网络是由一层一层构建的，那么每&lt;strong&gt;层&lt;/strong&gt;究竟在做什么？&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;数学式子&lt;/strong&gt;：&lt;equation&gt;\vec{y}= a(W\cdot\vec{x} + {b})&lt;/equation&gt;，其中&lt;equation&gt;\vec{x}&lt;/equation&gt;是输入向量，&lt;equation&gt;\vec{y}&lt;/equation&gt;是输出向量，&lt;equation&gt;\vec{b}&lt;/equation&gt;是偏移向量，&lt;equation&gt;W&lt;/equation&gt;是权重矩阵，&lt;equation&gt;a()&lt;/equation&gt;是激活函数。每一层仅仅是把输入&lt;equation&gt;\vec x&lt;/equation&gt;经过如此简单的操作得到&lt;equation&gt;\vec y&lt;/equation&gt;。&lt;/li&gt;&lt;li&gt;&lt;strong&gt;数学理解&lt;/strong&gt;：通过如下5种对输入空间（输入向量的集合）的操作，完成 &lt;strong&gt;输入空间 ——&amp;gt; 输出空间&lt;/strong&gt; 的变换 (矩阵的行空间到列空间)。 注：用“空间”二字的原因是被分类的并不是单个事物，而是&lt;strong&gt;一类&lt;/strong&gt;事物。空间是指这类事物所有个体的集合。&lt;ul&gt;&lt;li&gt;&lt;strong&gt;1.&lt;/strong&gt; 升维/降维&lt;/li&gt;&lt;li&gt;&lt;strong&gt;2.&lt;/strong&gt; 放大/缩小&lt;/li&gt;&lt;li&gt;&lt;strong&gt;3.&lt;/strong&gt; 旋转&lt;/li&gt;&lt;li&gt;&lt;strong&gt;4.&lt;/strong&gt; 平移&lt;/li&gt;&lt;li&gt;&lt;strong&gt;5.&lt;/strong&gt; “弯曲” 这5种操作中，1,2,3的操作由&lt;equation&gt;W\cdot\vec{x}&lt;/equation&gt;完成，4的操作是由&lt;equation&gt;+\vec{b}&lt;/equation&gt;完成，5的操作则是由&lt;equation&gt;a()&lt;/equation&gt;来实现。 (此处有动态图&lt;a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/1layer.gif" data-editable="true" data-title="5种空间操作" class=""&gt;5种空间操作&lt;/a&gt;，帮助理解)&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt;每层神经网络的数学理解：&lt;strong&gt;用线性变换跟随着非线性变化，将输入空间投向另一个空间&lt;/strong&gt;。&lt;/p&gt;&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;物理理解&lt;/strong&gt;：对 &lt;equation&gt;W\cdot\vec{x}&lt;/equation&gt; 的理解就是&lt;strong&gt;通过组合形成新物质&lt;/strong&gt;。&lt;equation&gt;
a()&lt;/equation&gt;又符合了我们所处的世界都是非线性的特点。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;情景：&lt;/strong&gt;&lt;equation&gt;\vec{x}&lt;/equation&gt;是二维向量，维度是碳原子和氧原子的数量&lt;equation&gt; [C;O]&lt;/equation&gt;，数值且定为&lt;equation&gt;[1;1]&lt;/equation&gt;，若确定&lt;equation&gt;\vec{y}&lt;/equation&gt;是三维向量，就会形成如下网络的形状 (神经网络的每个节点表示一个维度)。通过改变权重的值，可以获得若干个不同物质。右侧的节点数决定了想要获得多少种不同的新物质。（矩阵的行数） &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-69d03cf2b3677ad7dc3b0d9af58841b4.jpg" data-rawwidth="144" data-rawheight="164"&gt;&lt;strong&gt;1.&lt;/strong&gt;如果权重W的数值如（1），那么网络的输出y⃗ 就会是三个新物质，[二氧化碳，臭氧，一氧化碳]。 &lt;equation&gt;\left[
 \begin{matrix}
   CO_{2}\\
   O_{3}\\
   CO
  \end{matrix}
  \right]=
 \left[
 \begin{matrix}
   1 &amp;amp; 2 \\
   0 &amp;amp; 3\\
   1 &amp;amp; 1
  \end{matrix}
  \right] \cdot \left[
 \begin{matrix}
   C \\
   O \\
  \end{matrix}
  \right]&lt;/equation&gt; （1）&lt;/li&gt;&lt;li&gt;&lt;strong&gt;2.&lt;/strong&gt;也可以减少右侧的一个节点，并改变权重W至（2），那么输出&lt;equation&gt;\vec{y}&lt;/equation&gt; 就会是两个新物质，&lt;equation&gt;[ O_{0.3};CO_{1.5}]&lt;/equation&gt;。  &lt;equation&gt;\left[
 \begin{matrix}
    O_{0.3}\\
   CO_{1.5}\\
  \end{matrix}
  \right]=
 \left[
 \begin{matrix}
   0&amp;amp; 0.3 \\
   1 &amp;amp; 1.5\\
  \end{matrix}
  \right] \cdot \left[
 \begin{matrix}
   C \\
   O \\
  \end{matrix}
  \right]&lt;/equation&gt;（2）&lt;strong&gt;3.&lt;/strong&gt;如果希望通过层网络能够从[C, O]空间转变到&lt;equation&gt;[CO_{2};O_{3};CO]&lt;/equation&gt;空间的话，那么网络的学习过程就是将W的数值变成尽可能接近(1)的过程 。如果再加一层，就是通过组合&lt;equation&gt;[CO_{2};O_{3};CO]&lt;/equation&gt;这三种基础物质，形成若干更高层的物质。 &lt;strong&gt;4.&lt;/strong&gt;重要的是这种组合思想，组合成的东西在神经网络中并不需要有物理意义。 &lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt;每层神经网络的物理理解：&lt;strong&gt;通过现有的不同物质的组合形成新物质&lt;/strong&gt;。&lt;/p&gt;&lt;/blockquote&gt;&lt;h1&gt;二、理解视角：&lt;/h1&gt;&lt;p&gt;现在我们知道了每一层的行为，但这种行为又是如何完成识别任务的呢？&lt;/p&gt;&lt;h2&gt;数学视角：“线性可分”&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;一维情景&lt;/strong&gt;：以分类为例，当要分类正数、负数、零，三类的时候，一维空间的直线可以找到两个超平面（比当前空间低一维的子空间。当前空间是直线的话，超平面就是点）分割这三类。但面对像分类奇数和偶数无法找到可以区分它们的点的时候，我们借助 x % 2（取余）的转变，把x变换到另一个空间下来比较，从而分割。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-92ff4b847ac5fa41d91d1e76a910c483.jpg" data-rawwidth="370" data-rawheight="63"&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;二维情景&lt;/strong&gt;：平面的四个象限也是线性可分。但下图的红蓝两条线就无法找到一超平面去分割。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-b1bd0f75b46ed27daf27910f2a6b6e3f.jpg" data-rawwidth="197" data-rawheight="204"&gt;神经网络的解决方法依旧是转换到另外一个空间下，用的是所说的&lt;strong&gt;5种空间变换操作&lt;/strong&gt;。比如下图就是经过放大、平移、旋转、扭曲原二维空间后，在三维空间下就可以成功找到一个超平面分割红蓝两线 (同SVM的思路一样)。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-8d7d1ef957ebbf8ba9bb9cf8ff2d87ff.jpg" data-rawwidth="197" data-rawheight="198"&gt;上面是一层神经网络可以做到的，如果把&lt;equation&gt;\vec{y}&lt;/equation&gt; 当做新的输入再次用这5种操作进行第二遍空间变换的话，网络也就变为了二层。最终输出是&lt;equation&gt;\vec{y}= a_{2}(W_{2}\cdot(a_{1}(W_{1}\cdot\vec{x} + {b}_{1})) + {b}_{2})&lt;/equation&gt;。 设想网络拥有很多层时，对原始输入空间的“扭曲力”会大幅增加，如下图，最终我们可以轻松找到一个超平面分割空间。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-d831a2d915c01c692c1a624cb5bb2c9f.jpg" data-rawwidth="204" data-rawheight="201"&gt;当然也有如下图失败的时候，关键在于“如何扭曲空间”。所谓监督学习就是给予神经网络网络大量的训练例子，让网络从训练例子中学会如何变换空间。每一层的权重W就&lt;strong&gt;控制着如何变换空间&lt;/strong&gt;，我们最终需要的也就是训练好的神经网络的所有层的权重矩阵。。这里有非常棒的&lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html" data-editable="true" data-title="可视化空间变换demo"&gt;可视化空间变换demo&lt;/a&gt;，&lt;strong&gt;一定要&lt;/strong&gt;打开尝试并感受这种扭曲过程。更多内容请看&lt;a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" data-editable="true" data-title="Neural Networks, Manifolds, and Topology"&gt;Neural Networks, Manifolds, and Topology&lt;/a&gt;。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1e2a3347073ec2d97966220513708178.jpg" data-rawwidth="205" data-rawheight="203"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;上面的内容有三张动态图，对于理解这种空间变化非常有帮助。由于知乎不支持动态图，可以在gitbook&lt;a href="https://yjango.gitbooks.io/-deep/content/wei_he_you_yong.html" class="" data-editable="true" data-title="深层学习为何要“deep”"&gt;深层学习为何要“deep”&lt;/a&gt;上感受那三张图。&lt;b&gt;一定一定要感受&lt;/b&gt;。&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;线性可分视角：神经网络的学习就是&lt;strong&gt;学习如何利用矩阵的线性变换加激活函数的非线性变换，将原始输入空间投向线性可分/稀疏的空间去分类/回归。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;增加节点数：增加维度，即增加线性转换能力。&lt;/strong&gt;&lt;strong&gt;增加层数：增加激活函数的次数，即增加非线性转换次数。&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h2&gt;物理视角：“物质组成”&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;类比&lt;/strong&gt;：回想上文由碳氧原子通过不同组合形成若干分子的例子。从分子层面继续迭代这种组合思想，可以形成DNA，细胞，组织，器官，最终可以形成一个完整的人。继续迭代还会有家庭，公司，国家等。这种现象在身边随处可见。并且原子的内部结构与太阳系又惊人的相似。不同层级之间都是以类似的几种规则再不断形成新物质。你也可能听过&lt;strong&gt;分形学&lt;/strong&gt;这三个字。可通过观看&lt;a href="http://www.tudou.com/programs/view/o41zy0SeSS0" data-editable="true" data-title="从1米到150亿光年" class=""&gt;从1米到150亿光年&lt;/a&gt;来感受自然界这种层级现象的普遍性。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-3ec7216f7ab84dac089836b166c0ae28.jpg" data-rawwidth="488" data-rawheight="340"&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;人脸识别情景&lt;/strong&gt;：我们可以模拟这种思想并应用在画面识别上。由像素组成菱角再组成五官最后到不同的人脸。每一层代表不同的不同的物质层面 (如分子层)。而每层的W&lt;strong&gt;存储着如何组合上一层的物质从而形成新物质&lt;/strong&gt;。 如果我们完全掌握一架飞机是如何从分子开始一层一层形成的，拿到一堆分子后，我们就可以判断他们是否可以以此形成方式，形成一架飞机。 附：&lt;a href="http://playground.tensorflow.org/" data-editable="true" data-title="Tensorflow playground"&gt;Tensorflow playground&lt;/a&gt;展示了数据是如何“流动”的。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-82f05552fd2ddde28a0ef20814d7acbb.png" data-rawwidth="624" data-rawheight="218"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;blockquote&gt;&lt;p&gt;物质组成视角：神经网络的学习过程就是&lt;strong&gt;学习物质组成方式的过程。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;增加节点数：增加同一层物质的种类，比如118个元素的原子层就有118个节点。&lt;/strong&gt;&lt;strong&gt;增加层数：增加更多层级，比如分子层，原子层，器官层，并通过判断更抽象的概念来识别物体。&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;h1&gt;三、神经网络的训练&lt;/h1&gt;&lt;p&gt;知道了神经网络的学习过程就是&lt;strong&gt;学习&lt;/strong&gt;控制着空间变换方式（物质组成方式）的&lt;strong&gt;权重矩阵&lt;/strong&gt;后，接下来的问题就是&lt;strong&gt;如何学习&lt;/strong&gt;每一层的权重矩阵W。&lt;/p&gt;&lt;h2&gt;如何训练：&lt;/h2&gt;&lt;p&gt;既然我们希望网络的输出尽可能的接近真正想要预测的值。那么就可以通过&lt;strong&gt;比较&lt;/strong&gt;当前网络的&lt;strong&gt;预测值&lt;/strong&gt;和我们真正想要的&lt;strong&gt;目标值&lt;/strong&gt;，再根据两者的差异情况来更新每一层的权重矩阵（比如，如果网络的预测值高了，就调整权重让它预测低一些，不断调整，直到能够预测出目标值）。因此就需要先&lt;strong&gt;定义“如何比较&lt;/strong&gt;预测值和目标值的&lt;strong&gt;差异&lt;/strong&gt;”，这便是&lt;strong&gt;损失函数或目标函数（loss function or objective function）&lt;/strong&gt;，用于衡量预测值和目标值的差异的方程。loss function的输出值（loss）越高表示差异性越大。那神经网络的训练就变成了尽可能的缩小loss的过程。 所用的方法是&lt;strong&gt;梯度下降（Gradient descent）&lt;/strong&gt;：通过使loss值向当前点对应梯度的反方向不断移动，来降低loss。一次移动多少是由&lt;strong&gt;学习速率（learning rate）&lt;/strong&gt;来控制的。&lt;/p&gt;&lt;h2&gt;梯度下降的问题：&lt;/h2&gt;&lt;p&gt;然而使用梯度下降训练神经网络拥有两个主要难题。&lt;/p&gt;&lt;h3&gt;1、局部极小值&lt;/h3&gt;&lt;p&gt;梯度下降寻找的是loss function的局部极小值，而我们想要全局最小值。如下图所示，我们希望loss值可以降低到右侧深蓝色的最低点，但loss有可能“卡”在左侧的局部极小值中。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-00fe10bf8877137bc5957cf0cd7f9219.png" data-rawwidth="420" data-rawheight="250"&gt;试图解决“卡在局部极小值”问题的方法分两大类：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;调节步伐：&lt;/strong&gt;调节学习速率，使每一次的更新“步伐”不同。常用方法有：&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;随机梯度下降（Stochastic Gradient Descent (SGD)：每次只更新一个样本所计算的梯度&lt;/p&gt;&lt;/li&gt;&lt;li&gt;小批量梯度下降（Mini-batch gradient descent）：每次更新若干样本所计算的梯度的平均值&lt;/li&gt;&lt;li&gt;动量（Momentum）：不仅仅考虑当前样本所计算的梯度；Nesterov动量（Nesterov Momentum）：Momentum的改进&lt;/li&gt;&lt;li&gt;&lt;p&gt;Adagrad、RMSProp、Adadelta、Adam：这些方法都是训练过程中依照规则降低学习速率，部分也综合动量&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;优化起点&lt;/strong&gt;：合理初始化权重（weights initialization）、预训练网络（pre-train），使网络获得一个较好的“起始点”，如最右侧的起始点就比最左侧的起始点要好。常用方法有：高斯分布初始权重（Gaussian distribution）、均匀分布初始权重（Uniform distribution）、Glorot 初始权重、He初始权、稀疏矩阵初始权重（sparse matrix）&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h3&gt;2、梯度的计算&lt;/h3&gt;&lt;p&gt;机器学习所处理的数据都是高维数据，该&lt;strong&gt;如何快速计算梯度&lt;/strong&gt;、而不是以年来计算。 其次如何更新&lt;strong&gt;隐藏层&lt;/strong&gt;的权重？ 解决方法是：计算图：&lt;strong&gt;反向传播算法&lt;/strong&gt;这里的解释留给非常棒的&lt;a href="http://colah.github.io/posts/2015-08-Backprop/" data-editable="true" data-title="Computational Graphs: Backpropagation" class=""&gt;Computational Graphs: Backpropagation&lt;/a&gt;需要知道的是，&lt;strong&gt;反向传播算法是求梯度的一种方法&lt;/strong&gt;。如同快速傅里叶变换（FFT）的贡献。 而计算图的概念又使梯度的计算更加合理方便。&lt;/p&gt;&lt;h3&gt;基本流程图：&lt;/h3&gt;&lt;p&gt;下面就结合图简单浏览一下训练和识别过程，并描述各个部分的作用。要&lt;b&gt;结合图解阅读以下内容。但手机显示的图过小，最好用电脑打开&lt;/b&gt;。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-af6c22793412bfc37e1428369a0e36e0.jpg" data-rawwidth="743" data-rawheight="345"&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;收集训练集（train data）：&lt;/strong&gt;也就是同时有input以及对应label的数据。每个数据叫做训练样本（sample）。label也叫target，也是机器学习中最贵的部分。上图表示的是我的数据库。假设input本别是x的维度是39，label的维度是48。&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;设计网络结构（architecture）：&lt;/strong&gt;确定层数、每一隐藏层的节点数和激活函数，以及输出层的激活函数和损失函数。上图用的是两层隐藏层（最后一层是输出层）。隐藏层所用激活函数a( )是ReLu，输出层的激活函数是线性linear（也可看成是没有激活函数）。隐藏层都是1000节点。损失函数L( )是用于比较距离MSE：mean((output - target)^2)。MSE越小表示预测效果越好。训练过程就是不断减小MSE的过程。到此所有数据的维度都已确定：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;训练数据：&lt;equation&gt;input \in R^{39} ;label \in R^{48}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;权重矩阵：&lt;equation&gt;W_{h1}\in R^{1000x39};W_{h2}\in R^{1000x1000} ;W_{o}\in R^{48x1000}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;偏移向量：&lt;equation&gt;b_{h1}\in R^{1000};b_{h2}\in R^{1000} ;b_{o}\in R^{48}&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;网络输出：&lt;equation&gt;output \in R^{48}&lt;/equation&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;数据预处理（preprocessing）：&lt;/strong&gt;将所有样本的input和label处理成能够使用神经网络的数据，label的值域符合激活函数的值域。并简单优化数据以便让训练易于收敛。比如中心化（mean subtraction）、归一化（normlization）、主成分分析（PCA）、白化（whitening）。假设上图的input和output全都经过了中心化和归一化。&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;strong&gt;权重初始化（weights initialization）&lt;/strong&gt;：&lt;equation&gt;W_{h1},W_{h2},W_{0}&lt;/equation&gt;在训练前不能为空，要初始化才能够计算loss从而来降低。&lt;equation&gt;W_{h1},W_{h2},W_{0}&lt;/equation&gt;初始化决定了loss在loss function中从哪个点开始作为起点训练网络。上图用均匀分布初始权重（Uniform distribution）。&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;训练网络（training）&lt;/strong&gt;：训练过程就是用训练数据的input经过网络计算出output，再和label计算出loss，再计算出gradients来更新weights的过程。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;正向传递：，算当前网络的预测值&lt;equation&gt;output =linear (W_{o} \cdot Relu(W_{h2}\cdot Relu(W_{h1}\cdot input+b_{h1})+b_{h2}) +b_{o})&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;计算loss：&lt;equation&gt;loss = mean((output - target)^2)&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;计算梯度：从loss开始反向传播计算每个参数（parameters）对应的梯度（gradients）。这里用Stochastic Gradient Descent (SGD) 来计算梯度，即每次更新所计算的梯度都是从一个样本计算出来的。传统的方法Gradient Descent是正向传递所有样本来计算梯度。SGD的方法来计算梯度的话，loss function的形状如下图所示会有变化，这样在更新中就有可能“跳出”局部最小值。 &lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-0c0e7f5ffa98c2c1eb87763dd5d1d9a3.png" data-rawwidth="469" data-rawheight="227"&gt;&lt;/li&gt;&lt;li&gt;更新权重：这里用最简单的方法来更新，即所有参数都 &lt;equation&gt;W = W - learningrate * gradient&lt;/equation&gt;&lt;/li&gt;&lt;li&gt;预测新值：训练过所有样本后，打乱样本顺序再次训练若干次。训练完毕后，当再来新的数据input，就可以利用训练的网络来预测了。这时的output就是效果很好的预测值了。下图是一张&lt;b&gt;实际值&lt;/b&gt;和&lt;b&gt;预测值&lt;/b&gt;的三组对比图。输出数据是48维，这里只取1个维度来画图。蓝色的是实际值，绿色的是实际值。最上方的是训练数据的对比图，而下方的两行是神经网络模型&lt;b&gt;从未见过&lt;/b&gt;的数据预测对比图。（不过这里用的是RNN，主要是为了让大家感受一下效果）&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-a675e692f7f7755d91bcdba5e988e910.jpg" data-rawwidth="2000" data-rawheight="1600"&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;注：此部分内容&lt;strong&gt;不是&lt;/strong&gt;这篇文章的&lt;strong&gt;重点&lt;/strong&gt;，但为了理解&lt;strong&gt;深层&lt;/strong&gt;神经网络，需要明白最基本的训练过程。 若能理解训练过程是通过梯度下降尽可能缩小loss的过程即可。 若有理解障碍，可以用python实践一下&lt;a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" data-editable="true" data-title="从零开始训练一个神经网络" class=""&gt;从零开始训练一个神经网络&lt;/a&gt;，体会整个训练过程。若有时间则可以再体会一下计算图自动求梯度的方便&lt;a href="https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/index.html#mnist-for-ml-beginners" data-editable="true" data-title="利用TensorFlow" class=""&gt;利用TensorFlow&lt;/a&gt;。&lt;/p&gt;&lt;h2&gt;结合&lt;a href="https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class="" data-editable="true" data-title="Tensorflow playground"&gt;Tensorflow playground&lt;/a&gt;理解&lt;b&gt;5种空间操作&lt;/b&gt;和&lt;b&gt;物质组成视角&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;打开网页后，总体来说，蓝色代表正值，黄色代表负值。拿&lt;b&gt;分类&lt;/b&gt;任务来分析。&lt;/p&gt;&lt;ul&gt;&lt;li&gt;数据：在二维平面内，若干点被标记成了两种颜色。黄色，蓝色，表示想要区分的两类。你可以把平面内的任意点标记成任意颜色。网页给你提供了4种规律。神经网络会根据你给的数据训练，再分类相同规律的点。&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6d17d1fb77d3ae1838f5253193456317.png" data-rawwidth="173" data-rawheight="169"&gt;&lt;ul&gt;&lt;li&gt;输入：在二维平面内，你想给网络多少关于“点”的信息。从颜色就可以看出来，&lt;equation&gt;x_{1}&lt;/equation&gt;左边是负，右边是正，&lt;equation&gt;x_{1}&lt;/equation&gt;表示此点的横坐标值。同理，&lt;equation&gt;x_{2}&lt;/equation&gt;表示此点的纵坐标值。&lt;equation&gt;x_{1}^{2}&lt;/equation&gt;是关于横坐标值的“抛物线”信息。你也可以给更多关于这个点的信息。给的越多，越容易被分开。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-111e37e8479aa57bedbfb2dbcd8e5b63.png" data-rawwidth="92" data-rawheight="228"&gt;&lt;/li&gt;&lt;li&gt;连接线：表示权重，蓝色表示用神经元的原始输出，黄色表示用负输出。深浅表示权重的绝对值大小。鼠标放在线上可以看到具体值。也可以更改。在（1）中，当把&lt;equation&gt;x_{2}&lt;/equation&gt;输出的一个权重改为-1时，&lt;equation&gt;x_{2}&lt;/equation&gt;的形状直接倒置了。不过还需要考虑激活函数。（1）中用的是linear。在（2）中，当换成sigmoid时，你会发现没有黄色区域了。因为sigmoid的值域是(0,1)&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-2820b0562c8fcd7a49d57c4deb1e4f3c.png" data-rawwidth="315" data-rawheight="136"&gt;（1）&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-83fd9f01e2ea38c7f6b8aeaa308cf040.png" data-rawwidth="294" data-rawheight="123"&gt;（2）&lt;/li&gt;&lt;li&gt;输出：黄色背景颜色都被归为黄点类，蓝色背景颜色都被归为蓝点类。深浅表示可能性的强弱。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-dff3f6e72881ebd222414eabb9504671.png" data-rawwidth="1116" data-rawheight="363"&gt;上图中所有在黄色背景颜色的点都会被分类为“黄点“，同理，蓝色区域被分成蓝点。在上面的分类分布图中你可以看到每一层通过上一层信息的组合所形成的。权重（那些连接线）控制了“如何组合”。神经网络的学习也就是从数据中学习那些权重。Tensorflow playground所表现出来的现象就是“在我文章里所写的“物质组成思想”，这也是为什么我把&lt;a href="https://link.zhihu.com/?target=http%3A//playground.tensorflow.org/" class="" data-editable="true" data-title="Tensorflow playground"&gt;Tensorflow playground&lt;/a&gt;放在了那一部分。&lt;/li&gt;&lt;/ul&gt;不过你要是把Tensorflow的个名字拆开来看的话，是tensor（张量）的flow（流动）。Tensorflow playground的作者想要阐述的侧重点是“&lt;b&gt;张量如何流动&lt;/b&gt;”的。&lt;b&gt;5种空间变换的理解&lt;/b&gt;：Tensorflow playground下没有体现5种空间变换的理解。需要打开这个网站尝试：&lt;a href="http://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html" data-editable="true" data-title="ConvNetJS demo: Classify toy 2D data" class=""&gt;ConvNetJS demo: Classify toy 2D data&lt;/a&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-55811ac3d91f56f19543714b1b5abe49.png" data-rawwidth="841" data-rawheight="425"&gt;左侧是原始输入空间下的分类图，右侧是转换后的高维空间下的扭曲图。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-a81a10592b96a1d2b067e1d4ae3951e7.png" data-rawwidth="848" data-rawheight="417"&gt;最终的扭曲效果是所有绿点都被扭曲到了一侧，而所有红点都被扭曲到了另一侧。这样就可以线性分割（用超平面（这里是一个平面）在中间分开两类）&lt;h1&gt;四、“深层”的思考：真的只有这些原因吗？&lt;/h1&gt;&lt;p&gt;文章的最后稍微提一下深层神经网络。深层神经网络就是拥有更多层数的神经网络。&lt;/p&gt;&lt;p&gt;按照上文在理解视角中所述的观点，可以想出下面两条理由关于为什么更深的网络会更加容易识别，增加容纳变异体（variation）（红苹果、绿苹果）的能力、鲁棒性（robust）。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;数学视角&lt;/strong&gt;：变异体（variation）很多的分类的任务需要高度非线性的分割曲线。不断的利用那5种空间变换操作将原始输入空间像“捏橡皮泥一样”在高维空间下捏成更为线性可分/稀疏的形状。 &lt;strong&gt;物理视角&lt;/strong&gt;：通过对“&lt;strong&gt;抽象概念&lt;/strong&gt;”的判断来识别物体，而非细节。比如对“飞机”的判断，即便人类自己也无法用语言或者若干条规则来解释自己如何判断一个飞机。因为人脑中真正判断的不是是否“有机翼”、“能飞行”等细节现象，而是一个抽象概念。层数越深，这种概念就越抽象，所能&lt;strong&gt;涵盖的变异体&lt;/strong&gt;就越多，就可以容纳战斗机，客机等很多种不同种类的飞机。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;然而深层神经网络的惊人表现真的只有这些原因吗？&lt;/strong&gt;&lt;strong&gt;为什么神经网络过深后，预测的表现又变差？ 而且这时变差的原因是由于“过深”吗？&lt;/strong&gt;&lt;strong&gt;接下来要写的《深层学习为何要“Deep”（下）》是关于“深层”二字的进一步思考，找出所有网络结构的共性，并解释设计神经网络的本质是什么。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;附带：知乎不支持动态图这一点让文章对问题的解释大打折扣。我在gitbook上也有编辑，&lt;a href="http://link.zhihu.com/?target=https%3A//yjango.gitbooks.io/-deep/content/" class="" data-editable="true" data-title="深层学习为何要“deep”"&gt;深层学习为何要“deep”&lt;/a&gt;，对于文章当中的几个动态图，一定一定要去感受一下。喜欢csdn的也可以在这里&lt;a href="http://link.zhihu.com/?target=http%3A//blog.csdn.net/u010751535" class="" data-editable="true" data-title="JYango's"&gt;JYango's&lt;/a&gt;阅读。&lt;/p&gt;&lt;p&gt;另一篇《&lt;a href="https://zhuanlan.zhihu.com/p/23024247?refer=YJango" class="" data-editable="true" data-title="智能的最小单元"&gt;智能的最小单元&lt;/a&gt;》可以有助于理解生物智能和人工智能之间的关系，更能体会到深层神经网络为何强大。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22888385&amp;pixel&amp;useReferer"/&gt;</description><author>YJango</author><pubDate>Wed, 12 Oct 2016 02:33:48 GMT</pubDate></item><item><title>逻辑与神经之间的桥</title><link>https://zhuanlan.zhihu.com/p/22457562</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/fc658b76b2f0ed0c863953573cd5f462_r.jpg"&gt;&lt;/p&gt;这是我迄今为止最有成果的论文，欢迎讨论！&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-2860c7305704f8a9f47e3dec8c3f7cb0.jpg" data-rawwidth="963" data-rawheight="1509"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-1ab416079097690e31fa00c95c1b97ea.jpg" data-rawwidth="955" data-rawheight="1483"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-3eeb1d2483a14d86f38a4e750d623d08.jpg" data-rawwidth="955" data-rawheight="1495"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-68eb7652697f1969b0b4cf13f001d5e8.jpg" data-rawwidth="953" data-rawheight="1491"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-977f44c4f962b2f4b5bf630153ea2a3d.jpg" data-rawwidth="956" data-rawheight="1485"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-c2a24fb3f29db902b505acc436480404.jpg" data-rawwidth="954" data-rawheight="1489"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-1ba6dc1c99e604b25733b52d620ef12a.jpg" data-rawwidth="956" data-rawheight="1491"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-109ad8458276933f1195ccde55a638c4.jpg" data-rawwidth="951" data-rawheight="1488"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-f6451264e7924fd973676972a7f6e49b.jpg" data-rawwidth="962" data-rawheight="349"&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22457562&amp;pixel&amp;useReferer"/&gt;</description><author>甄景贤</author><pubDate>Thu, 15 Sep 2016 12:55:35 GMT</pubDate></item><item><title>最前沿 之 谷歌的协作机械臂</title><link>https://zhuanlan.zhihu.com/p/22758556</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6f589df38509d14f839737645322a011_r.jpg"&gt;&lt;/p&gt;&lt;p&gt;本文由我和&lt;a href="https://www.zhihu.com/people/li-yi-ying-73" data-editable="true" data-title="李艺颖" class=""&gt;李艺颖&lt;/a&gt;共同撰写。&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;前几天也就是2016年10月3号，Google Research Blog上发表了最新的Blog，介绍他们在机器人上的工作：&lt;a href="https://research.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html" data-editable="true" data-title="googleblog.com 的页面" class=""&gt;https://research.googleblog.com/2016/10/how-robots-can-acquire-new-skills-from.html&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-6bbe8af2dbc2e7e7036c00aac03a5917.png" data-rawwidth="1898" data-rawheight="1228"&gt;现在Google们都喜欢发文章+写博客的套路，似乎不太需要我们做面向大众的解读了。因此，本文决定不介绍他们基本的工作了，我们来研究一下他们的具体工作，来点干货。&lt;/p&gt;&lt;p&gt;这次Google联合了Google Brain和DeepMind一起搞，一次发了四篇文章（够狠，链接转自官方博客）：&lt;/p&gt;&lt;p&gt;【1】&lt;a href="https://arxiv.org/abs/1610.00633" data-editable="true" data-title="Deep Reinforcement Learning for Robotic Manipulation" class=""&gt;Deep Reinforcement Learning for Robotic Manipulation&lt;/a&gt;. &lt;i&gt;Shixiang Gu, Ethan Holly, Timothy Lillicrap, Sergey Levine.&lt;/i&gt; [&lt;a href="https://sites.google.com/site/deeproboticmanipulation/" data-editable="true" data-title="video" class=""&gt;video&lt;/a&gt;]【2】&lt;a href="https://arxiv.org/abs/1610.00696" data-editable="true" data-title="Deep Visual Foresight for Planning Robot Motion" class=""&gt;Deep Visual Foresight for Planning Robot Motion&lt;/a&gt;. &lt;i&gt;Chelsea Finn, Sergey Levine.&lt;/i&gt; [&lt;a href="https://www.youtube.com/watch?v=CKRWJEVSXMI" data-editable="true" data-title="video"&gt;video&lt;/a&gt;] [&lt;a href="https://sites.google.com/site/brainrobotdata/home" data-editable="true" data-title="data"&gt;data&lt;/a&gt;]【3】&lt;a href="https://arxiv.org/abs/1610.00673" data-editable="true" data-title="Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search" class=""&gt;Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search&lt;/a&gt;. &lt;i&gt;Ali Yahya, Adrian Li, Mrinal Kalakrishnan, Yevgen Chebotar, Sergey Levine.&lt;/i&gt;  [&lt;a href="https://youtu.be/ZBFwe1gF0FU" data-editable="true" data-title="video"&gt;video&lt;/a&gt;]【4】&lt;a href="https://arxiv.org/abs/1610.00529" data-editable="true" data-title="Path Integral Guided Policy Search" class=""&gt;Path Integral Guided Policy Search&lt;/a&gt;. &lt;i&gt;Yevgen Chebotar, Mrinal Kalakrishnan, Ali Yahya, Adrian Li, Stefan Schaal, Sergey Levine. &lt;/i&gt;[&lt;a href="https://www.youtube.com/watch?v=ncp1kY5JV90" data-editable="true" data-title="video"&gt;video&lt;/a&gt;]&lt;/p&gt;&lt;p&gt;所以，今天我们来以快速的分析一下这四篇文章。&lt;/p&gt;&lt;h2&gt;2 Deep Reinforcement Learning for Robotic Manipulation &lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-4a8ecedc83c7b64c450d760978316c0b.png" data-rawwidth="1234" data-rawheight="660"&gt;&lt;p&gt;这篇文章的题目弄得很大，用深度增强学习来解决机器人的操纵问题。具体一点就是让机器人从零开始学会开门。&lt;/p&gt;&lt;p&gt;文章中表示实现让机械臂自己学会开门就是他们的贡献，他们是第一个做出这个demonstration的。当然，这么说也是ok的。&lt;/p&gt;&lt;p&gt;对于理论上的创新，他们表示他们拓展了NAF算法，变成异步NAF。我表示异步的思想早就有了，而且他们的异步NAF的实现方式实在是太太太简单了。就是&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;用多个线程收集不同机器人的数据，然后用一个线程去训练，并且训练线程在服务器上，训练后不断把最新的神经网络参数传递给每一个机器人用于新的采样&lt;/b&gt;。&lt;/blockquote&gt;&lt;p&gt;可能让机器人自己开门这个任务在我看来是本来就能实现的，所以其实并没有太多震撼的地方。而且，在这篇文章中，并&lt;b&gt;不使用视觉输入！&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;不使用视觉输入，声称能让机器人学会开门有多大意义呢?&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;从文章中可以看到，对于开门这个任务，门把手的位置的给定的：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;In addition, we append a target position to the
state, which depends on the task: for the door
opening, this is the handle position when the door is closed and the quaternion measurement of the sensor attached to
the door frame.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;大家看到了吧，不但门把手的位置，连门的姿态也就是四元数quaternion也是有传感器来测量的。那么，这样号称做了一个很屌的Demonstration真的有意思吗？比较&lt;b&gt;质疑这篇文章的贡献&lt;/b&gt;。这里使用的神经网络也只是两个200的全连接神经网络。某种程度上讲，其实这个任务的状态输入是低维而不是高维的。我觉得如果这篇文章能够完全使用图像输入来实现端到端自学习的话，那么就很厉害！虽然该团队之前有&lt;a href="https://arxiv.org/abs/1603.02199" data-editable="true" data-title="一篇文章"&gt;一篇文章&lt;/a&gt;就是用视觉输入，但是用了十几台机器人，并且训练几个月来收集数据。现在这个任务只要几小时，不过没有视觉，意义不够大。&lt;/p&gt;&lt;p&gt;关于NAF算法，可以参考本专栏的：&lt;a href="https://zhuanlan.zhihu.com/p/21609472?refer=intelligentunit" data-editable="true" data-title="DQN从入门到放弃7 连续控制DQN算法-NAF - 智能单元 - 知乎专栏" class=""&gt;DQN从入门到放弃7  连续控制DQN算法-NAF - 智能单元 - 知乎专栏&lt;/a&gt;&lt;/p&gt;&lt;p&gt;关于DDPG算法，可以参考本人的CSDN blog：&lt;a href="http://blog.csdn.net/songrotek/article/details/50917337" data-editable="true" data-title="Paper Reading 3:Continuous control with Deep Reinforcement Learning" class=""&gt;Paper Reading 3:Continuous control with Deep Reinforcement Learning&lt;/a&gt;&lt;/p&gt;&lt;p&gt;关于DDPG的源码复现，可以参考本人的github：&lt;a href="https://github.com/songrotek/DDPG" data-editable="true" data-title="GitHub - songrotek/DDPG: Reimplementation of DDPG(Continuous Control with Deep Reinforcement Learning) based on OpenAI Gym + Tensorflow" class=""&gt;GitHub - songrotek/DDPG: Reimplementation of DDPG(Continuous Control with Deep Reinforcement Learning) based on OpenAI Gym + Tensorflow&lt;/a&gt;&lt;/p&gt;&lt;h2&gt;3 Deep Visual Foresight for Planning Robot Motion&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-644e1a2ea1c79ff7492365e10044eb5a.png" data-rawwidth="1052" data-rawheight="744"&gt;这篇文章和深度增强学习没有直接关系，完全另外一个思路。为什么研究这个，我们先来说说&lt;b&gt;机器人学习之难难在哪？&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;机器人学习之难难在环境不完全可见，难在没有Model！&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;在控制领域，有一种任务相对比较好做，比如火箭发射！（当然也是很难的大工程），但是火箭发射上个世纪就解决了，人类甚至可以发射探测器到很远的地方，火星探测器也上了好几次了。Elon Musk前不久才提出他的火星登陆计划：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-df90f1f33f6539d183c3619d9a4f733e.png" data-rawwidth="2428" data-rawheight="1078"&gt;&lt;blockquote&gt;&lt;b&gt;为什么人类早早就能把探测器发送到那么远的地方，却连“简单”的让机器人开个门都那么难？&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;因为对于航天工程，我们可以精确的计算探测器，火箭的运动模型（运动方程），我们可以精确的计算出火箭在怎样的推力下会达到的轨迹，因此我们可以精确的控制。我们有足够的人力，足够的资源来进行数学计算，我们也就能够实现很好的控制。但是&lt;b&gt;对于机器人开门这种事，我们没法算&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;为什么？&lt;/p&gt;&lt;ol&gt;&lt;li&gt;每个门都可能不一样，门把手也不一样&lt;/li&gt;&lt;li&gt;机器人的位置不固定，门的位置也不固定。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;我们可以针对某个特定的门精确计算来实现控制，但是我们不可能遍历所有的门，更何况对于一个新的门怎么办？大家看到了，这里没有所谓的Model模型，我们无法建立模型，特别是如果我们只使用摄像头，类似人的第一视角，那么我们得到的信息更是有限。每时每刻的控制都将不一样。这就是机器人学习难的原因。&lt;/p&gt;&lt;p&gt;所以，深度增强学习的很多方式都是所谓的Model-Free的方法，也就是不需要模型，通过trial-and-error来学会整个过程。&lt;/p&gt;&lt;p&gt;可是，人类并不仅仅是通过trial-and-error来学习的。我们人类其实在大脑里能够构建一些基本的模型的，也就是比如门把手的位置，很多东西的位置，在我们大脑中是有概念的，我们也能够预测他们的位移。特别是足球的守门员，就需要掌握一项技能，那就是预判球的位置。&lt;/p&gt;&lt;p&gt;所以，问题就这么来了：我们能不能来预测一下物体的位置，从而帮助机器人抓取物品？&lt;/p&gt;&lt;p&gt;这篇文章也就做了这个事，弄了一个物体预测模型来预测物体的位置。本质上是未来研究model-based的方法。&lt;/p&gt;&lt;p&gt;个人看法：model-based和model-free方法结合起来用能使机器人学习发挥出更大威力。&lt;/p&gt;&lt;p&gt;关于预测模型，其实这篇文章也不新鲜了，之前就有文章研究预测atari游戏的画面的，也有文章预测汽车的运行轨迹的。只是这篇文章比较具体，面向机器人控制的具体问题，把预测模型和机器人控制MPC直接结合在一起，从而形成了一个确实的demo。&lt;/p&gt;&lt;p&gt;所以，重复一下，&lt;b&gt;这篇文章的关键不是弄成一个预测模型，而是真正把模型给用在机器人控制上。在文章中，作者也是说的很明确，没有夸大的成分：&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;The primary contribution of our paper is to demonstrate
that deep predictive models of video can be used by real
physical robotic systems to manipulate previously unseen
objects.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;我们稍微来说一下这个深度预测模型：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a61c2f5dd46954bdd64d884fa7c1753a.png" data-rawwidth="2378" data-rawheight="842"&gt;结构上搞的比较复杂。先说输入输出。输入有三个，一个是当前帧，当前的状态state和动作action，然后输出下一帧图像（预测）。&lt;/p&gt;&lt;p&gt;中间的结构大致可以分成三部分：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;CNN+LSTM部分，用于提取图像特征信息，不过这里的输出很特别，并没有使用反卷积直接生成图像，而是输出一个图像的mask蒙版。这个蒙版可以认为是计算出里面的物体的移动流也就是pixel flow。&lt;/li&gt;&lt;li&gt;状态和动作输入部分，将状态和动作从卷积层的中间插入。&lt;/li&gt;&lt;li&gt;图像生成部分。首先是利用中间的卷积层抽取多个卷积核然后与原始图像做卷积，得到变化后的图像transformed images，然后和mask一起生成pixel flow map F，然后F与原始图像一起生成下一帧的图像。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;所以这个神经网络本质上是要学习出物体移动的像素流pixel flow，类似光流，从而计算出下一帧的图像。作者虽然限定了神经网络的结构，但是在学习训练时只使用视频，动作，状态数据做监督学习，也就是端到端的学习，中间的mask蒙版和像素流并没有单独的监督学习。&lt;b&gt;然而文章中并没有对mask和flow的具体表现形式做分析，是否就是产生出对物体运动的捕捉我比较怀疑。&lt;/b&gt;只能说通过训练，神经网络确实学习了捕捉物体的运动并能够根据输入加以预测。&lt;/p&gt;&lt;p&gt;接下来是这篇文章的区别其他文章的工作，直接使用这个预测模型与经典的MPC控制结合，来实现机械臂的控制。基本是思路就是利用预测模型预测不同动作未来的移动情况，从而选择最优的移动方式。这种方式取得成功说明利用预测模型进行机器人控制的可行性。&lt;b&gt;下一步进一步拓展深度预测模型将成为可能，&lt;/b&gt;这也是这篇文章最大的意义。&lt;/p&gt;&lt;h2&gt;4 Path Integral Guided Policy Search&lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-81ffccb38f7e3c8ca417bdb6949f3adc.png" data-rawwidth="1164" data-rawheight="488"&gt;&lt;p&gt;这篇文章以及下一篇文章是对Sergey Levine提出的GPS算法的拓展和改进。关于GPS（Guided Policy Search）这个算法，我一直觉得不是一个好的算法，至少未来这个算法我认为没有必要（为什么之后说），但是这个算法展示性比深度增强学习算法强，能够直接应用到真实的机器人上。先说一下深度增强学习应用到机器人上最大的困难，就是&lt;b&gt;采样！&lt;/b&gt;我们可以在仿真环境中千百万次的训练机器人，但是我们没办法在真实环境中这么做。时间不允许，机器人也不允许。以此同时，完全的高维输入，高维输出做机器人控制目前仍是困难很大的。&lt;/p&gt;&lt;p&gt;&lt;b&gt;那么GPS是什么呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;它的想法非常简单，就是把问题先分解成不同的初始条件，比如开门，有的是这个角度，有的是那个角度。然后，针对不同的条件单独训练一个局部策略Local Policy。那么这个训练方式他这里&lt;b&gt;不管了！！！也就是你想用什么传统的控制算法都可以！&lt;/b&gt;然后有了这些可以用的策略之后，就利用策略采集样本，只是把输入变了，比如变成视觉输入，然后利用样本训练一个网络来代替这些局部策略，也就是模仿学习Imitation Learning，通过这种方式实现视觉伺服。我表示&lt;b&gt;GPS很没意思。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;为什么没意思？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;就说机器人开门这件事吧。我们要实现视觉控制，也就是让机器人看着打开门，和人一样。我们希望即使门放在不同的角度位置机器人也可以开。按照深度增强学习做法，那必须是End-to-End啊，输入视觉信息，输出控制，然后训练。从Deep Reinforcement Learning for Robot Manipulation这篇我们大概可以猜测出，这样做失败了，所以那篇文章并没有使用视觉输入。但我现在就要视觉输入这么办？&lt;/p&gt;&lt;p&gt;&lt;b&gt;Imitation Learning！模仿学习&lt;/b&gt;&lt;/p&gt;&lt;p&gt;神经网络啥都能学习，因此只要我们能够收集到好的输入输出样本，我们就可以训练。模仿学习就是这么干。我们可以利用人类获取样本。比如人拿着机械臂做几百次开门动作，然后记录这些动作作为样本进行训练。但是只是用人比较麻烦，不用人就用机器人控制的算法，比如LQR，我们就有model怎么啦。我们先利用机械臂的model信息和门的精确信息来优化出一条最优轨迹，然后这不就是样本了吗？为了实现神经网络的通用性，我们面对不同的门的角度位置弄多条对应的最优轨迹，然后收集所有样本进行训练。在拓展一下，就是反过来利用训练的神经网络生成样本，然后反过来让控制算法进行优化。&lt;/p&gt;&lt;p&gt;&lt;b&gt;所以，虽然GPS能够实现视觉伺服控制，但是其中间过程一点也不单纯。使用了太多额外的信息来做训练。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在我看来，训练时也只使用视觉信息，不使用额外信息才是有用的算法。GPS最致命的地方也在这里，这个开门可以精确建模，可以有额外数据，但是很多其他任务可没有那么多额外数据可以弄。&lt;b&gt;GPS本质上不具备自学习能力，而只是传统方法的神经网络化。&lt;/b&gt;监督学习无法使机器人超越传统方法。&lt;/p&gt;&lt;p&gt;那么这篇文章又做了什么事呢？将LQR用一个model free的RL算法替代。PI2这个算法之前不是很了解，看了文章感觉就是一种进化算法。基本思想就是采用多种路径，然后让路径概率向着损失较小的方向靠。和CEM（交叉熵方法）也差不多。就是对于Cost，采样概率还有参数更新方式不一样。&lt;/p&gt;&lt;p&gt;有了PI2算法，GPS就可以做到model free了。但是训练过程还是一样。PI2算法的使用过程中并不使用视觉信息。只是因此采用的方法不一样（比起LQR），能够通过训练来解决开门这种间断连续控制问题（要先让机械臂移动到门把手那里，然后抓住，旋转，打开）。在我看来，直接用深度增强学习算法比如DDPG甚至REINFORCE来训练local policy不就完了，最后再综合所有样本监督学习一个，效果肯定好。&lt;/p&gt;&lt;p&gt;总的来说，GPS看似有用，实则鸡肋，改进它意义不大，还不如研究如何实现少样本学习。&lt;/p&gt;&lt;h2&gt;5 Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-324517c941a6e509a94a369ad86dc97e.png" data-rawwidth="1654" data-rawheight="534"&gt;这篇文章其实和Deep Reinforcement Learning for Robot Manipulation很类似，只是针对的算法不一样，说白了就将Guided Policy Search拓展成并行异步的版本，从而可以实现多机器人协同训练。&lt;/p&gt;&lt;p&gt;----------------------&lt;/p&gt;&lt;p&gt;下面的分析来自&lt;a href="https://www.zhihu.com/people/li-yi-ying-73" data-editable="true" data-title="李艺颖" class=""&gt;李艺颖&lt;/a&gt;：&lt;/p&gt;&lt;p&gt;从架构方面，这十分契合云机器人的主旨，机器人可以将它们各自的经验通过网络传递给其它机器人。斯坦福人工智能百年报告之《人工智能与2030年的生活》中也指出以家用机器人为例，多机器人协同能够使机器人“共享更多家庭内收集的数据集，反过来能提供给云端进行机器学习，进一步改进已经部署的机器人。”本文就是鉴于考虑到真实世界中环境具有多样性和复杂性，所以想到让机器人将自身经历传递给彼此，使它们在环境中相互配合来学习技能，同时机器人也基于自身任务的特定样本改进局部的策略。实验采用4个机器人，任务是基于视觉学习开门，4个机器人对应的门的姿态和外观也都有所不同，采用的算法是GPS，在它们反复尝试和共享经历中不断提高任务执行水平。多机器人提高了样本的多样性，提高了学习的泛化能力和可使用能力。&lt;/p&gt;-----------------------&lt;p&gt;算法的做法依然是很简单。一句话就可以概括。就是几个机器人分别有一个local policy来优化，每个机器人面对的场景都不一样。然后训练，将样本上传服务器，在服务器上监督学习一个神经网络，然后用这个神经网络Global Policy来辅助采样优化local policy。&lt;/p&gt;&lt;p&gt;只能说多机器人协助必然能够提升学习训练速度，但是这种idea非常简单，算法的改进也是非常简单。当然，我们也不得不承认，Google的整个实验难度很大，要训练好很难，甚至这个训练用的机械臂都是Google自己造的（话说Google高层不让卖）。&lt;/p&gt;&lt;h2&gt;6 小结&lt;/h2&gt;&lt;p&gt;Google似乎想惊艳一下大家，一次发四篇文章说他们的机器人进展。但是很可惜，从具体文章的内容和贡献来看，并没有太多惊艳的思想和效果。多机器人协作是必然，核心还在于算法的改进。当然，我们也必须承认，Google能够实现让机器人完全使用视觉来开门是一个不错的Demo，只是这个Demo是在当前算法框架下必然可以实现的，不过也只有土豪的Google能这么做。&lt;/p&gt;&lt;p&gt;最后，大家也看到了，&lt;b&gt;让机器人学会开门竟然是21世纪的今天人类最前沿科技都还没很好解决的问题&lt;/b&gt;，可见人类的文明程度是有多低。但，这也就是我们研究机器人实现机器人革命的机会！&lt;/p&gt;&lt;h2&gt;声明：本文为原创文章，未经允许不得转载。另外本文的图片都来自于本文介绍的四篇paper和网络。&lt;/h2&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22758556&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Fri, 07 Oct 2016 16:39:58 GMT</pubDate></item><item><title>最前沿：围棋之后，AI玩FPS游戏也能秀人类一脸了！</title><link>https://zhuanlan.zhihu.com/p/22604627</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-9b132a69e08ced90ec418e66699b2c3a_r.png"&gt;&lt;/p&gt;&lt;b&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创，禁止未授权转载&lt;/b&gt;。&lt;blockquote&gt;前言：9月23日，基于经典第一人人称射击游戏毁灭战士DOOM的AI挑战赛“&lt;a href="http://vizdoom.cs.put.edu.pl/competition-cig-2016" data-editable="true" data-title="Visual Doom AI Competition @ CIG 2016"&gt;Visual Doom AI Competition @ CIG 2016&lt;/a&gt;”尘埃落定，&lt;b&gt;Facebook团队和Intel团队的AI分别拿下两个赛制最佳&lt;/b&gt;，同时也涌现出若干优秀的学生参赛AI。本文根据最新的公开信息，对&lt;b&gt;赛事本身&lt;/b&gt;和&lt;b&gt;卡耐基梅隆大学参赛团队的AI&lt;/b&gt;做出简要介绍。&lt;/blockquote&gt;&lt;h2&gt;内容列表&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;背景介绍&lt;/li&gt;&lt;ul&gt;&lt;li&gt;放轻松，先看游戏视频&lt;/li&gt;&lt;li&gt;游戏毁灭战士DOOM简介&lt;/li&gt;&lt;li&gt;竞赛平台ViZDOOM简介&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;赛事介绍&lt;/li&gt;&lt;ul&gt;&lt;li&gt;赛制与规则&lt;/li&gt;&lt;li&gt;参赛队伍&lt;/li&gt;&lt;li&gt;赛事结果&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;卡耐基梅陇大学参赛AI介绍&lt;/li&gt;&lt;ul&gt;&lt;li&gt;基本模型与算法&lt;/li&gt;&lt;li&gt;遇到困难与解决问题的创新思路&lt;/li&gt;&lt;li&gt;算法评价&lt;/li&gt;&lt;li&gt;超越人类玩家的实验结果&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;感言&lt;/li&gt;&lt;li&gt;作者反馈&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;背景介绍&lt;/h2&gt;&lt;p&gt;&lt;b&gt;第一人称射击游戏&lt;/b&gt;，&lt;b&gt;杀红眼的死亡竞赛&lt;/b&gt;和&lt;b&gt;人工智能&lt;/b&gt;，每一个概念充满了话题性，当融合了这三个要素的基于游戏毁灭战士DOOM的AI挑战赛“&lt;a href="http://vizdoom.cs.put.edu.pl/competition-cig-2016" data-editable="true" data-title="Visual Doom AI Competition @ CIG 2016"&gt;Visual Doom AI Competition @ CIG 2016&lt;/a&gt;”尘埃落定，就是搞个大新闻的时候了：）&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;根据评论更新&lt;/b&gt;：这些人工智能算法与玩家们常见的游戏内置bot和外挂有本质区别，内置bot和外挂都是通过获取游戏内部数据来获得不对称优势，而这些算法，他们在测试时和人类一样，&lt;b&gt;只通过游戏屏幕的图像信息来玩游戏&lt;/b&gt;。&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;然而亮点中更有惊喜&lt;/b&gt;，本次赛事的两个亮点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Facebook和Intel均派出团队参数；&lt;/li&gt;&lt;li&gt;先前公布出“&lt;b&gt;已经超越人类玩家水平AI&lt;/b&gt;”的卡耐基梅隆团队&lt;b&gt;并！未！夺！冠！&lt;/b&gt;细思极恐哈哈&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;惊喜的是&lt;/b&gt;，在“有限制的死亡竞赛”赛制中夺冠的Facebook团队的成员，就是知乎上的两位&lt;a href="https://www.zhihu.com/people/9819f6938be0d3bb133ad0151eefd188" data-hash="9819f6938be0d3bb133ad0151eefd188" class="member_mention" data-editable="true" data-title="@田渊栋" data-hovercard="p$b$9819f6938be0d3bb133ad0151eefd188"&gt;@田渊栋&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/5c2b06e8ddc61687e42a1a64fb72bbaf" data-hash="5c2b06e8ddc61687e42a1a64fb72bbaf" class="member_mention" data-editable="true" data-title="@吴育昕" data-hovercard="p$b$5c2b06e8ddc61687e42a1a64fb72bbaf"&gt;@吴育昕&lt;/a&gt; 。其中田研究员已经在知乎问题&lt;a href="https://www.zhihu.com/question/50916351?from=profile_question_card" class="" data-editable="true" data-title="如何评价基于游戏毁灭战士（Doom）的AI死亡竞赛大赛结果？ - 人工智能"&gt;如何评价基于游戏毁灭战士（Doom）的AI死亡竞赛大赛结果？ - 人工智能&lt;/a&gt;，给出了参加比赛的一些经过，只是由于文章尚在撰写，暂时不公布技术细节。总之个人看到结果时，非常高兴，祝贺他们！&lt;/p&gt;&lt;h2&gt;看视频提问题&lt;/h2&gt;&lt;p&gt;欢乐起来，这里先卖关子，大家来看看下面三个游戏视频，猜猜看哪个视频是人类玩家在玩，哪个视频是AI玩家在玩？猜对了也没有奖励：）&lt;/p&gt;&lt;ul&gt;&lt;li&gt;视频A&lt;/li&gt;&lt;/ul&gt;&lt;video id="69686" data-swfurl="" poster="" data-sourceurl="http://v.youku.com/v_show/id_XMTczNjU3MTc2NA==.html#paction" data-name='猜一猜是人类玩家还是AI玩家1—在线播放—优酷网，视频高清在线观看" /&amp;gt;&amp;lt;meta name="irTitle" content="猜一猜是人类玩家还是AI玩家1" /&amp;gt;&amp;lt;meta  name="irAlbumName" content="猜一猜是人类玩'&gt;&lt;/video&gt;&lt;ul&gt;&lt;li&gt;视频B&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;video id="69687" data-swfurl="" poster="" data-sourceurl="http://v.youku.com/v_show/id_XMTczNjU4NDA1Ng==.html" data-name='猜一猜2—在线播放—优酷网，视频高清在线观看" /&amp;gt;&amp;lt;meta name="irTitle" content="猜一猜2" /&amp;gt;&amp;lt;meta  name="irAlbumName" content="猜一猜2" /&amp;gt;&amp;lt;meta name="keywords"'&gt;&lt;/video&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;视频C&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;video id="69688" data-swfurl="" poster="" data-sourceurl="http://v.youku.com/v_show/id_XMTczNjU4NjY1Ng==.html" data-name='猜一猜3—在线播放—优酷网，视频高清在线观看" /&amp;gt;&amp;lt;meta name="irTitle" content="猜一猜3" /&amp;gt;&amp;lt;meta  name="irAlbumName" content="猜一猜3" /&amp;gt;&amp;lt;meta name="keywords"'&gt;&lt;/video&gt;&lt;/p&gt;&lt;p&gt;正确答案在后文中，请大家带着疑问继续阅读吧！&lt;/p&gt;&lt;h2&gt;毁灭战士DOOM&lt;/h2&gt;&lt;p&gt;DOOM于1993年由id software发行，现在已经发行到了第四代。初代的画面是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-9b132a69e08ced90ec418e66699b2c3a.png" data-rawwidth="1166" data-rawheight="656"&gt;DOOM4的游戏画面是这样的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-3131f28e046482d3283f763d188abe85.jpg" data-rawwidth="1805" data-rawheight="1023"&gt;作为硬核的古典派第一人称射击游戏，&lt;b&gt;就是突出一个“莽”一个“爽”&lt;/b&gt;。毁灭战士系列是一个伟大的系列，我个人认为&lt;b&gt;其初代&lt;/b&gt;和&lt;b&gt;重返德军总部&lt;/b&gt;，&lt;b&gt;毁灭公爵&lt;/b&gt;等都可以算是第一人称射击游戏上古时代的开山鼻祖。这一次AI们一起相互伤害的的游戏，是DOOM初代，也就是第一幅图中的游戏画面。&lt;/p&gt;&lt;p&gt;需要向领域外读者们指出的是：AI们在竞赛的时候，获取的信息比人类玩家更少，&lt;b&gt;只有游戏画面数据信息&lt;/b&gt;，&lt;b&gt;AI们的行动都仅仅基于图像输入后算法输出的决策控制&lt;/b&gt;，连声音也没有！这和游戏内置AI不同，游戏内置AI是能够通过游戏引擎获取所有的游戏内部信息的。&lt;/p&gt;&lt;h2&gt;ViZDOOM平台简介&lt;/h2&gt;&lt;p&gt;ViZDOOM的官网&lt;a href="http://vizdoom.cs.put.edu.pl" data-editable="true" data-title="在此"&gt;在此&lt;/a&gt;。ViZDOOM是什么，请允许我引用其官网的简介如下：&lt;/p&gt;&lt;blockquote&gt;ViZDoom is a Doom-based AI research platform for reinforcement learning from raw visual information. It allows developing AI bots that play Doom using only the screen buffer. ViZDoom is primarily intended for research in machine visual learning, and deep reinforcement learning, in particular.ViZDOOM是一个基于DOOM的AI研究平台，主要针对面向原始视觉信息输入的增强学习。它可以&lt;b&gt;让研究者开发出只利用游戏屏幕数据玩DOOM的AI机器人&lt;/b&gt;。ViZDOOM主要面向的是机器视觉学习，&lt;b&gt;更确切地说，就是深度增强学习&lt;/b&gt;。&lt;/blockquote&gt;关于ViZDOOM的详细情况，其作者们于2016年5月在arXiv上发布了论文：《&lt;a href="http://arxiv.org/abs/1605.02097" data-editable="true" data-title="ViZDoom: A Doom-based AI Research Platformfor Visual Reinforcement Learning" class=""&gt;ViZDoom: A Doom-based AI Research Platformfor Visual Reinforcement Learning&lt;/a&gt;》，可以通过阅读论文获取。&lt;p&gt;需要说明的是，&lt;b&gt;基于ViZDOOM，研究者是可以直接访问DOOM的游戏引擎的，可以拿到游戏内部的信息&lt;/b&gt;。甚至可以利用这些内部信息来训练自己的AI，但是，在测试阶段，是不能得到内部信息的，只能让AI根据游戏画面来自主决策和行动。&lt;/p&gt;&lt;p&gt;用大白话来说，就是训练AI你可以开上帝模式，我不管你。但是真刀真枪干的时候，你AI老老实实地只能用自己的眼睛看游戏画面玩游戏，作弊是休想的。&lt;/p&gt;&lt;h2&gt;赛事介绍&lt;/h2&gt;&lt;p&gt;赛事的全程是&lt;b&gt;Visual Doom AI Competition @ CIG 2016&lt;/b&gt;，赛事地址在&lt;a href="http://vizdoom.cs.put.edu.pl/competition-cig-2016" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。比赛的核心目标就是要回答一个问题：&lt;/p&gt;&lt;blockquote&gt;&lt;b&gt;到底AI能不能只根据原始视觉信息高效地玩毁灭战士？&lt;/b&gt;&lt;/blockquote&gt;&lt;p&gt;现在答案已经很明显了，这里啰嗦一下&lt;b&gt;对自己的保守观念的鄙视和被打脸的过程&lt;/b&gt;。当时赛事出来，&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-editable="true" data-title="@Flood Sung" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt;（&lt;b&gt;他现在玩《守望先锋》，正在前往多拉多.....&lt;/b&gt;）就给我说了这个事情，我当时就说：&lt;/p&gt;&lt;blockquote&gt;哎哟，你看deepmind也才把Atari那些游戏玩得差不多，那种偏策略的反馈比较延迟的游戏效果还不太好，这就大跃进地要搞FPS了？肯定效果不好！&lt;/blockquote&gt;&lt;p&gt;前几天，卡耐基梅隆大学参赛团队The Terminators（&lt;b&gt;终结者&lt;/b&gt;）的AI机器人Arnold（&lt;b&gt;阿诺德&lt;/b&gt;）（哎哟，兄弟们，你们&lt;b&gt;这么明目张胆地取名字&lt;/b&gt;，州长&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-09ef482c8355d212fef4e6661a719bc8.jpg" data-rawwidth="1920" data-rawheight="1080"&gt;会来找你们哟）的演示视频和论文（后文会解读）出来了，我一看，&lt;b&gt;直接承认自己被打脸&lt;/b&gt;，机器人玩得非常流畅，论文里面的结果显示&lt;b&gt;AI已经把20个卡耐基梅隆大学的学生玩家（平均水平）按在地上摩擦&lt;/b&gt;。当时我觉得，这个阿诺德是要发啊，&lt;b&gt;肯定能夺冠&lt;/b&gt;！&lt;/p&gt;&lt;p&gt;结果比赛结果出来，&lt;b&gt;阿诺德&lt;/b&gt;在第一个赛制里面败给了Facebook的F1机器人，在难度更大的第二个赛制里面败给了Intel的IntelAct。&lt;b&gt;我的脸又火辣辣的了&lt;/b&gt;......不过好在阿诺德两个赛制都参加了，而且都拿了第二，所以也算是很强了，所以我感觉稍微好些......&lt;/p&gt;&lt;p&gt;既然说到了赛制，那么接下来就介绍下：&lt;/p&gt;&lt;h2&gt;赛制与规则&lt;/h2&gt;&lt;p&gt;&lt;b&gt;赛制&lt;/b&gt;分两种：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;已知地图上的受限制死亡竞赛：&lt;b&gt;武器只有火箭炮，机器人可以捡血包和弹药；&lt;/b&gt;&lt;/li&gt;&lt;li&gt;未知地图上的的不受限制死亡竞赛：&lt;b&gt;机器人初始只有手枪，可以捡各种武器弹药和血包。&lt;/b&gt;提供了两张地图用于训练，3张未知地图用于测试。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;规则分为允许与禁止：&lt;/b&gt;相对比较细节，领域外同学随意看看即可&lt;b&gt;。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;允许：&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;加载自己的设置文件；&lt;/li&gt;&lt;li&gt;使用任意分辨率；&lt;/li&gt;&lt;li&gt;使用任意可获取的按钮；&lt;/li&gt;&lt;li&gt;使用任意可获取的游戏变量；&lt;/li&gt;&lt;li&gt;使用任意可获取的屏幕数据格式（深度信息不行）；&lt;/li&gt;&lt;li&gt;改变渲染设置；&lt;/li&gt;&lt;li&gt;设置机器人的名字与颜色；&lt;/li&gt;&lt;li&gt;使用doom2.wad或freedoom2.wad文件随你。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;b&gt;禁止&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在机器人根目录外修改文件系统；&lt;/li&gt;&lt;li&gt;网络通信；&lt;/li&gt;&lt;li&gt;使用send_game_command命令；&lt;/li&gt;&lt;li&gt;使用new_episode。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;奖金！！&lt;/h2&gt;&lt;p&gt;对了，大家可能会问有没有奖金呢？有奖金的哟！&lt;/p&gt;&lt;ul&gt;&lt;li&gt;在赛制1中的前三名能分别获取1000，300，200欧元；&lt;/li&gt;&lt;li&gt;在赛制2中的前三名能分别获得2000，1000，500欧元。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这个金额和前阵子的DOTA2奖金比起来确实是九牛一毛，但是对于科技的进步来说，意义则大不一样。&lt;/p&gt;&lt;h2&gt;参赛队伍&lt;/h2&gt;&lt;p&gt;我在&lt;a href="https://www.zhihu.com/question/50916351/answer/123272182?from=profile_answer_card" data-editable="true" data-title="如何评价基于游戏毁灭战士（Doom）的AI死亡竞赛大赛结果？ - 杜客的回答" class=""&gt;如何评价基于游戏毁灭战士（Doom）的AI死亡竞赛大赛结果？ - 杜客的回答&lt;/a&gt;中已经做了一些介绍，总得说来，除了&lt;b&gt;Facebook和Intel这两个明星团队&lt;/b&gt;，其他3各值得关注的团队是分别是&lt;b&gt;卡耐基梅隆大学的Arnold，埃塞克斯大学的Clyde和东芬兰大学的tuho。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;目前，公开了论文的只有卡耐基梅隆大学团队，Facebook的田研究员表示正在撰写文章，其他团队的情况尚未看到相关信息，欢迎知友补充。&lt;/p&gt;&lt;h2&gt;比赛结果&lt;/h2&gt;&lt;p&gt;这里让我偷个懒，直接从官网截图如下：&lt;/p&gt;&lt;p&gt;&lt;b&gt;赛制1：&lt;/b&gt;F1就是Facebook团队，第二名是阿诺德。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-24b9939cd484f3d374a5c6efcddcffe4.png" data-rawwidth="2110" data-rawheight="862"&gt;&lt;b&gt;赛制2&lt;/b&gt;：第一名是Intel团队，第二名是阿诺德。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-1a5664433a5d69143b24a6a3385bd602.png" data-rawwidth="2116" data-rawheight="696"&gt;&lt;h2&gt;卡内基梅隆大学参赛AI介绍&lt;/h2&gt;&lt;p&gt;从上面的比赛结果中可以看出，卡耐基梅隆大学The Terminators团队的机器人Arnold综合实力不错，在两个赛制中都得到了亚军的好成绩，同时，他们也是目前唯一发布了论文《&lt;a href="http://arxiv.org/abs/1609.05521" data-editable="true" data-title="Playing FPS Games with Deep Reinforcement Learning"&gt;Playing FPS Games with Deep Reinforcement Learning&lt;/a&gt;》的团队，所以下面主要根据他们的论文做一个简要解读。&lt;/p&gt;&lt;h2&gt;基本模型与算法&lt;/h2&gt;&lt;p&gt;论文使用的模型的出发点还是DQN和DRQN模型，鉴于领域内的知友对于这两个模型都比较熟悉，而领域外的知友对数学公式也并不感兴趣，所以这里我还是采取了偷懒的办法：&lt;/p&gt;&lt;p&gt;&lt;b&gt;关于DQN模型&lt;/b&gt;，请阅读我们专栏&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-editable="true" data-title="@Flood Sung" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt;的教程&lt;a href="https://zhuanlan.zhihu.com/p/21421729?refer=intelligentunit" class="" data-editable="true" data-title="DQN从入门到放弃5 深度解读DQN算法 "&gt;DQN从入门到放弃5 深度解读DQN算法&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;关于DRQN模型&lt;/b&gt;，简单解释如下：DQN的模型是假设在每一步，机器人都能得到环境的全部观察st。但是在只能观察到部分环境的情况下，机器人只能得到部分观察&lt;equation&gt;o_t&lt;/equation&gt;，不足以来推断整个系统的状态。像DOOM这样的游戏就是这样。&lt;/p&gt;&lt;p&gt;为了解决这种情况，2015年，Hausknecht和Stone发布论文《&lt;a href="http://arxiv.org/abs/1507.06527" data-title="Deep recurrent q-learning for partially observable mdps" class="" data-editable="true"&gt;Deep recurrent q-learning for partially observable mdps&lt;/a&gt;》，引入了DRQN，它不去估计&lt;equation&gt;Q(s_t,a_t)&lt;/equation&gt;, 而是估计&lt;equation&gt;Q(o_t,h_{t-1},a_t)&lt;/equation&gt;。其中，&lt;equation&gt;h_t&lt;/equation&gt;是一个额外的输入，该输入是由前一状态的网络返回的，表达了机器人的隐藏状态。像LSTM那样的循环神经网络可以在普通的DQN模型之上来实现，在这种情况下，&lt;equation&gt;h_t=LSTM(h_{t-1},o_t)&lt;/equation&gt;，而我们则估计&lt;equation&gt;Q(h_t,o_t)&lt;/equation&gt;。&lt;/p&gt;&lt;h2&gt;困难与解决思路&lt;/h2&gt;&lt;p&gt;作者们在论文中坦率地指出，一开始他们是&lt;b&gt;用的标准的DRQN模型，结果效果很不好&lt;/b&gt;。算法只能在很简单的场景中有良好表现，到了死亡竞赛场景中，表现就很差了。&lt;/p&gt;&lt;p&gt;这是怎么回事呢？作者们通过与Sandeep Subramanian和Kanthashree Mysore Sathyendra讨论，得出以下结论：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;We reason that the agents were not able to accurately detect
enemies.&lt;/p&gt;我们推出，算法表现不好的愿意是&lt;b&gt;机器人不能很准确地探测到敌人&lt;/b&gt;。&lt;/blockquote&gt;&lt;p&gt;有了原因，作者们就给出了新的解决思路，主要是以下几点：&lt;/p&gt;&lt;li&gt;&lt;b&gt;概念上&lt;/b&gt;：将游戏过程看做两个阶段，导航阶段和行动阶段。&lt;b&gt;导航阶段就是机器人探索地图，发现物品并捡起物品。行动阶段就是攻击敌人&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;框架上&lt;/b&gt;：对应不同阶段，使用两个独立的模型来进行训练。导航阶段使用的是原始的DQN，行动阶段是论文创新修改的融入了游戏特征信息的DRQN模型来训练。&lt;/li&gt;&lt;li&gt;&lt;b&gt;增加游戏特征信息的DRQN模型&lt;/b&gt;：这是论文的&lt;b&gt;核心创新点&lt;/b&gt;之一，说到底，就是将游戏高级信息（比如视野中是否出现敌人）融入到DRQN进行训练，值得仔细看看，图示如下：&lt;/li&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-bc9e23834bdd07662f5f2fe30a11b12a.png" data-rawwidth="1030" data-rawheight="366"&gt;在我的回答中，有知友说没有看到游戏状态信息的输入，想知道是怎么在training的时候加入的，我的回答是：有的。注意512维的layer4和后面k维的两个全连接层，CNN输入输入给他们，其中k就是想要探测的k个游戏特征信息，在训练的时候，网络的代价函数是把DRQN的代价函数和交叉熵损失合起来。虽然有很多游戏信息可以获取，但是论文&lt;b&gt;只用了当前画面中是否出现敌人的指示器&lt;/b&gt;。加入这个特征极大地提升了性能，对比图示如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-6baec594c6ec08c59a1aa546f2caa107.png" data-rawwidth="714" data-rawheight="486"&gt;&lt;p&gt;经过这么一改进，作者们非常开心，还做了一些其他的结构来融入游戏信息，但是效果都不太好。&lt;b&gt;说明分享卷积层对模型性能有决定性影响&lt;/b&gt;。&lt;b&gt;联合训练DRQN模型和游戏特征探测使得卷积核能够获取游戏的相关联信息&lt;/b&gt;。在他们的实验中，只花了几个小时就达到了最佳敌人探测水平，准确率0.9。在此之后，LSTM就能得到包含敌人及其位置的特征，使得训练进一步加速。&lt;/p&gt;&lt;p&gt;&lt;b&gt;将游戏分成两个阶段的思路也很重要&lt;/b&gt;：死亡竞赛可以分成两个阶段，探索梯度收集物品发现敌人，攻击敌人。称之为导航阶段和行动阶段。训练了两个网络，每个网络针对的是不同的阶段。当前的DQN模型不能将两个针对不同任务优化的网络合并在一起。但是，当前游戏的阶段可以通过预测敌人是否能被看见（行动阶段）或不被看见（探索阶段）来决定，这可以从游戏特征信息中推断。将任务分成两个阶段，每个阶段用不同网络训练的优势：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;框架模块化，不同阶段用不同网络训练测试；&lt;/li&gt;&lt;li&gt;两个网络可以并行训练，使得训练更快；&lt;/li&gt;&lt;li&gt;导航阶段只需要3个动作（前进，左右移），极大降低了Q函数需要学习的状态-动作对，使得训练更快；&lt;/li&gt;&lt;/ol&gt;&lt;b&gt;行动网络使用的是DRQN加游戏信息特征，探索网络使用的是DQN&lt;/b&gt;。在评价计算的时候，行动网络每一步都调用，如果视野中没有敌人，或者弹药用尽的时候，导航网络调用来决定行动。在&lt;b&gt;回馈（reward）设计&lt;/b&gt;上，作者们采取了&lt;b&gt;回馈共享（&lt;/b&gt;&lt;b&gt;reward shaping）&lt;/b&gt;的思路，即：修改回报函数，包含一些小的中间回报来加速学习过程。在击杀敌人给正向回报，自杀给负回报的基础上，向&lt;b&gt;行动网络&lt;/b&gt;引入了以下中间回报：&lt;ul&gt;&lt;li&gt;捡到东西加分；&lt;/li&gt;&lt;li&gt;掉血减分；&lt;/li&gt;&lt;li&gt;射击减少弹药减分；&lt;/li&gt;&lt;/ul&gt;&lt;b&gt;导航网络&lt;/b&gt;：&lt;ul&gt;&lt;li&gt;捡到东西加分；&lt;/li&gt;&lt;li&gt;走到岩浆减分；&lt;/li&gt;&lt;li&gt;走的距离越长加分越多，有助于走完整个地图。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;算法评价&lt;/h2&gt;&lt;p&gt;本质上讲卡内基梅隆大学的AI&lt;b&gt;为了简化AI的训练难度，通过人类的知识将游戏的环节设计成导航阶段和行动阶段，然后设计了三个网络&lt;/b&gt;（一个简单点的DQN网络用于导航阶段，一个特征识别网络用于选择不同的阶段，一个DRQN网络用于行动阶段），然后作者巧妙的将特征识别（有没有敌人）的网络和DRQN网络结合在一起训练。特征识别网络是一个典型的监督学习网络，而DRQN则是增强学习的范畴，两者在这里竟然同时合在一起训练，确实是有意思的事情。&lt;/p&gt;&lt;p&gt;最后这篇文章只使用DQN和DQN的变种DRQN，&lt;b&gt;并没有使用目前最强的深度增强学习算法A3C&lt;/b&gt;。A3C在Atari上的性能是Nature版本DQN的4倍，效果惊人。所以，这可能是Arnold只拿第二的原因吧。如果他们基于A3C来训练，相信效果会更好，但是整个模型基本都得改一下。我们显然也想提出这样的疑问：“能不能只使用一个网络来玩Doom达到这样的高水平？”在Arnold的基础上监督学习一个端到端网络算是一个办法，但是这样并不能更好的提升游戏AI的水平。最后的最后，Arnold在训练过程中利用了游戏内部的特征来训练，而据田渊栋的回答，他们的方法不怎么使用游戏内部的特征信息，很期待他们的思路。&lt;/p&gt;&lt;h2&gt;实验结果&lt;/h2&gt;&lt;p&gt;在论文结果中，显示AI水平已经超过了人类玩家，这些玩家是卡耐基梅隆大学的学生：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-8c3b0951fcd58a17af807c347f05eac7.png" data-rawwidth="884" data-rawheight="320"&gt;上面这个表格，显示的是论文对比AI算法和人类玩家游戏水平的结果，具体如下：&lt;ul&gt;&lt;li&gt;&lt;b&gt;K/D比例&lt;/b&gt;：击杀/死亡比例；&lt;/li&gt;&lt;li&gt;&lt;b&gt;单个玩家场景&lt;/b&gt;：机器人和玩家分别和10个游戏原内置AI对战3分钟；&lt;/li&gt;&lt;li&gt;&lt;b&gt;多玩家场景&lt;/b&gt;：人类玩家和机器人对抗5分钟。&lt;/li&gt;&lt;li&gt;&lt;b&gt;自杀数&lt;/b&gt;：火箭炮等武器过近的射击点会造成自我伤害。&lt;b&gt;注意&lt;/b&gt;：人类玩家自杀失误高于AI。&lt;/li&gt;&lt;/ul&gt;&lt;b&gt;人类得分是取所有人类玩家的平均值&lt;/b&gt;。在&lt;b&gt;两个场景中都有20名人类玩家参加&lt;/b&gt;。可以看到AI相对于学生玩家的水平。当然，你可以说职业玩家水平可以更高，但是我们只需要回忆一下围棋，回忆一下AlphaGO......AlphaGo也是基于深度增强学习哟。&lt;h2&gt;感言&lt;/h2&gt;&lt;p&gt;在我的回答的评论中，知友&lt;a href="https://www.zhihu.com/people/1b01e645e0b40fc30ae54d156e7558aa" data-hash="1b01e645e0b40fc30ae54d156e7558aa" class="member_mention" data-editable="true" data-title="@碧海居士" data-hovercard="p$b$1b01e645e0b40fc30ae54d156e7558aa"&gt;@碧海居士&lt;/a&gt;应该不是领域内人士，但是他的评论值得一看：&lt;/p&gt;&lt;blockquote&gt;如果说围棋是纯拼算法和计算量的话，游戏的实效性就决定了这东西离进入实用近了一步。毕竟战场是即时制而不是回合制的……想象一下这AI用在自动攻击的无人载具上是个多么恐怖的事情……&lt;/blockquote&gt;&lt;p&gt;我们虽然时刻都希望人工智能能够给人类带来共同利益（for the common good），然而现实总是现实，每一次人类科技的进步，总是被首先运用于军事，这点我们必须承认。我们将AI欢乐地在射击游戏上跑，环境仿真越真实，算法效果越好，那么其潜在军事价值就越大，这&lt;b&gt;实在不是一个让人感到欢乐的话题&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;意识到这一点，我自己有了&lt;b&gt;两个想法&lt;/b&gt;：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;对于个人研究者来说，如果基于理想主义，可以不在类似射击的平台上进行算法应用研究，而在一些比较民用方向的平台上研究，比如&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-editable="true" data-title="@Flood Sung" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt;在&lt;a href="https://zhuanlan.zhihu.com/p/22523121?refer=intelligentunit" data-editable="true" data-title="最前沿：深度增强学习再发力，家用机器人已近在眼前 - 智能单元 - 知乎专栏" class=""&gt;最前沿：深度增强学习再发力，家用机器人已近在眼前 - 智能单元 - 知乎专栏&lt;/a&gt;中介绍的斯坦福的室内机器人仿真。&lt;/li&gt;&lt;li&gt;为了国家安全，还是要有一批研究者要继续在军事方面的研究。毕竟这个世界不是康德的世界，&lt;a href="http://baike.baidu.com/link?url=6_8uJ7auA3XHl6BUClDwz5rJcFehxO1Pe0LmNB8FfIpJJJiABj9o6N23p7XReX5M"&gt;霍布斯的世界&lt;/a&gt;离我们并不遥远，只有相当的实力，才能确保和平。这个是我基于人生经历的个人观点，&lt;b&gt;不想争论&lt;/b&gt;，所以有不同意见的知友，我尊重，但也保留自己的观点，&lt;b&gt;不想花时间讨论&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;作者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;等田研究员的文章出来后，&lt;a href="https://www.zhihu.com/people/23deec836a24f295500a6d740011359c" data-hash="23deec836a24f295500a6d740011359c" class="member_mention" data-hovercard="p$b$23deec836a24f295500a6d740011359c"&gt;@Flood Sung&lt;/a&gt;应该会做解析；&lt;/li&gt;&lt;li&gt;欢迎大家留言讨论，除了最后我特别指出的那个观点；&lt;/li&gt;&lt;li&gt;3个视频都是机器人在玩，你猜对了吗？哈哈！&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22604627&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Sat, 24 Sep 2016 18:59:40 GMT</pubDate></item><item><title>看图说话的AI小朋友——图像标注趣谈（下）</title><link>https://zhuanlan.zhihu.com/p/22520434</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7c2ef2d9f4d9244473a201b0b19318ec_r.jpeg"&gt;&lt;/p&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创，禁止未授权转载。&lt;blockquote&gt;前言：近来&lt;b&gt;图像标注（Image Caption）&lt;/b&gt;问题的研究热度渐高。本文希望在把问题和研究介绍清楚的同时行文通俗有趣，让非专业读者也能一窥其妙。&lt;/blockquote&gt;&lt;h2&gt;内容列表：&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;图像标注问题简介&lt;/li&gt;&lt;ul&gt;&lt;li&gt;图像标注是什么&lt;/li&gt;&lt;li&gt;当前水平&lt;/li&gt;&lt;li&gt;价值和意义&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;图像标注数据集&lt;/li&gt;&lt;ul&gt;&lt;li&gt;MSCOCO&lt;/li&gt;&lt;li&gt;Flickr8K和Flickr30K&lt;/li&gt;&lt;li&gt;PASCAL 1K&lt;/li&gt;&lt;li&gt;创建一个守望先锋数据集？&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;图像标注评价标准&lt;/li&gt;&lt;ul&gt;&lt;li&gt;人类判断与自动评价标准&lt;/li&gt;&lt;li&gt;Perplexity&lt;/li&gt;&lt;li&gt;BLEU&lt;/li&gt;&lt;li&gt;ROUGE&lt;/li&gt;&lt;li&gt;METEOR&lt;/li&gt;&lt;li&gt;CIDEr &lt;/li&gt;&lt;/ul&gt;&lt;li&gt;图像标注模型发展 &lt;i&gt;&lt;b&gt;注：下篇起始处&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;百度的m-RNN&lt;/li&gt;&lt;li&gt;谷歌的NIC&lt;/li&gt;&lt;li&gt;目前最高水平模型&lt;/li&gt;&lt;li&gt;模型的比较思考&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;代码实践&lt;/li&gt;&lt;ul&gt;&lt;li&gt;CS231n的LSTM_Captioning&lt;/li&gt;&lt;li&gt;基于Numpy的NerualTalk&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;图像标注问题展望&lt;/li&gt;&lt;ul&gt;&lt;li&gt;模型的更新&lt;/li&gt;&lt;li&gt;自动评价标注的更新&lt;/li&gt;&lt;li&gt;数据集的更新&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;上篇回顾&lt;/h2&gt;&lt;p&gt;在上篇中，我们介绍了研究图像标注问题常用的&lt;b&gt;数据集&lt;/b&gt;和评价算法常用的&lt;b&gt;自动评价标准&lt;/b&gt;，并脑洞了一个&lt;b&gt;开源创建基于守望先锋游戏画面的中文图像标注数据集&lt;/b&gt;的想法。文章发布后，有不少感兴趣的知友表示愿意参与，并提出了意见和建议。对此我将单独分出一篇文章介绍相关情况，请感兴趣的知友注意。在下篇中，我们将对比较有代表性的图像标注方法进行介绍，展示一些代码实践。&lt;/p&gt;&lt;h2&gt;图像标注模型的发展&lt;/h2&gt;&lt;p&gt;说是发展，其实时间也并不长，将CNN和RNN结合的模型用于解决图像标注问题的研究最早也就从2014开始提出，在2015年开始对模型各部分组成上进行更多尝试与优化，到2016年CVPR上成为一个热门的专题。&lt;/p&gt;&lt;p&gt;在这个发展中，将RNN和CNN结合的核心思路没变，变化的是使用了更好更复杂的CNN模型，效果更好的LSTM，图像特征输入到RNN中的方式，以及更复合的特征输入等。正由于其发展时间跨度较短，通过阅读该领域的一些重要文章，可以相对轻松地理出大牛们攻城拔寨的思路脉络，这对我们自己从事研究的思路也会有所启发。&lt;/p&gt;&lt;h2&gt;m-RNN模型&lt;/h2&gt;&lt;p&gt;2014年10月，百度研究院的Junhua Mao和Wei Xu等人在arXiv上发布论文《&lt;a href="http://arxiv.org/abs/1410.1090" data-editable="true" data-title="Explain Images with Multimodal Recurrent Neural Networks"&gt;Explain Images with Multimodal Recurrent Neural Networks&lt;/a&gt;》，提出了&lt;b&gt;multimodal Recurrent Neural Network（即m-RNN）&lt;/b&gt;模型，创造性地将深度卷积神经网络CNN和深度循环神经网络RNN结合起来，用于解决图像标注和图像和语句检索等问题。通过14年的相关新闻可知，Wei Xu应该是百度研究院的徐伟，Junhua Mao是徐伟团队中的毛俊华，此外还有杨亿，王江等。&lt;/p&gt;&lt;p&gt;这篇论文是首先抓住这个想法并实现的文章，作者们在文中也当仁不让地说：&lt;/p&gt;&lt;blockquote&gt;To the best of our knowledge, this is the first work that incorporates the Recurrent
Neural Network in a deep multimodal architecture.&lt;/blockquote&gt;&lt;p&gt;在后续的几篇优秀论文中，m-RNN都被作为一个基准方法用于比较和超越。因此，&lt;b&gt;首先介绍百度研究院的m-RNN模型，在于其创造性工作。知友切莫遇百度即黑&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;论文中，在对原始RNN结构进行简要说明后，提出了m-RNN模型如下：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-32d251cf7486c257c030e3e5eaad93c3.png" data-rawwidth="1566" data-rawheight="352"&gt;其结构特点可以归纳如下：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;模型的&lt;b&gt;输入&lt;/b&gt;是图像和与图像对应的标注语句（比如在上图中，这个语句就可能是a man at a giant tree in the jungle）。其&lt;b&gt;输出&lt;/b&gt;是对于下一个单词的可能性的分布；&lt;/li&gt;&lt;li&gt;模型在&lt;b&gt;每个时间帧&lt;/b&gt;都有6层：分别是输入层、2个单词嵌入层，循环层，多模型层和最后的Softmax层；&lt;/li&gt;&lt;li&gt;输入单词本来是以独热码（one-hot）方式编码，但是经过两个单词嵌入层后，最终变换为稠密单词表达。在该文中，单词表达层是随机初始化并在训练过程中自己学习的。第二个嵌入层输出的激活数据，作为输入直接进入到多模型层（蓝色线条）；&lt;/li&gt;&lt;li&gt;循环层的维度是256维，在其中进行的是对&lt;b&gt;t&lt;/b&gt;时刻的单词表达向量&lt;equation&gt;w(t)&lt;/equation&gt;和&lt;b&gt;t-1&lt;/b&gt;时刻的循环层激活数据&lt;equation&gt;r(t-1)&lt;/equation&gt;的变换和计算，具体计算公式是：&lt;equation&gt;r(t)=f_2(U_r\cdot r(t-1)+w(t))&lt;/equation&gt;。其中，函数&lt;equation&gt;f_2(.)&lt;/equation&gt;是&lt;b&gt;ReLU&lt;/b&gt;，这个非线性激活函数在&lt;a href="https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit" data-editable="true" data-title="本专栏的CS231n笔记系列"&gt;本专栏的CS231n笔记系列&lt;/a&gt;中已经详细介绍过，这里略过。而&lt;equation&gt;U_r&lt;/equation&gt;是为了将&lt;equation&gt;r(t-1)&lt;/equation&gt;映射到和&lt;equation&gt;w(t)&lt;/equation&gt;同样的向量空间中所做的变换；&lt;/li&gt;&lt;li&gt;512维的&lt;b&gt;多模型层&lt;/b&gt;连接着模型的&lt;b&gt;语言部分&lt;/b&gt;和&lt;b&gt;图像部分&lt;/b&gt;。&lt;b&gt;图像部分&lt;/b&gt;就是上图中绿色虚线包围的部分，其本质是利用深度卷积神经网络来提取图像的特征。在该文中，使用的是大名鼎鼎的AlexNet的&lt;b&gt;第七层的激活数据&lt;/b&gt;作为特征数据输入到多模型层，如此就得到了图像特征向量&lt;equation&gt;I&lt;/equation&gt;。而&lt;b&gt;语言部分&lt;/b&gt;就是包含了单词嵌入层和循环层；&lt;/li&gt;&lt;li&gt;多模型层中所做的计算是：&lt;equation&gt;m(t)=g_2(V_w\cdot w(t)+V_r\cdot r(t)+I)&lt;/equation&gt;。其中，&lt;b&gt;m&lt;/b&gt;表示的是多模型层的特征向量，&lt;b&gt;I&lt;/b&gt;表示的是图像部分输入的特征向量，&lt;b&gt;w(t)&lt;/b&gt;和&lt;b&gt;r(t)&lt;/b&gt;的解释同上。至于&lt;equation&gt;V_w&lt;/equation&gt;和&lt;equation&gt;V_r&lt;/equation&gt;，依旧是一个矩阵变换。在这个公式中，&lt;b&gt;需要特！别！注！意！的&lt;/b&gt;是：&lt;b&gt;在每个t时刻，图像特征&lt;equation&gt;I&lt;/equation&gt;都作为输入进入了计算&lt;/b&gt;。这里向大家提问：&lt;b&gt;这样做好不好呢&lt;/b&gt;？先思考一下。后面会给出答案。最后，&lt;equation&gt;g_2(.)&lt;/equation&gt;函数是一个带参数的tanh函数：&lt;equation&gt;g_2(x)=1.7159\cdot tanh(\frac{2}{3}x)&lt;/equation&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;该网络在训练的时候，设计的&lt;b&gt;代价函数是基于语句的困惑度（Perplexity）&lt;/b&gt;的。关于困惑度，我们在&lt;a href="https://zhuanlan.zhihu.com/p/22408033?refer=intelligentunit" data-title="上篇中已经介绍" class="" data-editable="true"&gt;上篇中已经介绍&lt;/a&gt;，这里就不重复了。论文设计的代价函数为：&lt;/p&gt;&lt;equation&gt;C=\frac{1}{N}\sum^N_{i=1}L\cdot log_2PPL(w^{(i)}_{1:L}|I^{(i)})+||\theta||^2_2&lt;/equation&gt;&lt;p&gt;其中N是训练集中单词的数量，&lt;equation&gt;\theta&lt;/equation&gt;是模型的参数。所以&lt;equation&gt;||\theta||^2_2&lt;/equation&gt;实际上是一个正则化部分。而L是单词序列的长度。&lt;b&gt;训练的目标&lt;/b&gt;就是最小化代价函数值。可以看见，上述代价函数是可导的，由此就可以用反向传播来求梯度，而后用随机梯度下降方法来学习参数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型的语句生成&lt;/b&gt;：模型从一个特殊的开始符号“##START##”或者任意个参考单词（这里的意思是，作者们可以输入参考语句中的前K个单词作为开始）开始，然后模型开始计算下一个单词的概率分布&lt;equation&gt;P(w|w_{1:n-1}|I)&lt;/equation&gt;。然后取概率最大的一个单词作为选取的单词，同时再把这个单词作为输入，预测下一个单词，循环往复，直到生成结束符号##END##。&lt;/p&gt;&lt;p&gt;&lt;b&gt;实验数据集和标注&lt;/b&gt;：该论文发表较早，使用的数据集有我们在上篇中介绍的&lt;b&gt;Flickr8K和30K&lt;/b&gt;，也有我们没有介绍的&lt;b&gt;IAPR TC-12&lt;/b&gt;。使用的自动评价标准也较少，有&lt;b&gt;Perplexity，BLUE1-3，没有BLUE4，其余评价都没有。&lt;/b&gt;与该方法对比的，也是一些相对传统的方法。因此在这里，就对其实验结果略过了，感兴趣的知友可以自行阅读论文。&lt;/p&gt;&lt;p&gt;&lt;b&gt;综上&lt;/b&gt;：该论文的主要贡献就是提出了将RNN和CNN结合起来的模型。模型中有一些设计在后续中被证明不是良好的设计，后续的论文在这个模型的基础上逐渐优化。&lt;/p&gt;&lt;h2&gt;NIC模型&lt;/h2&gt;&lt;p&gt;2014年11月，谷歌的Vinyals等人发布了论文《&lt;a href="http://arxiv.org/abs/1411.4555" data-title="Show and Tell: A Neural Image Caption Generator" class="" data-editable="true"&gt;Show and Tell: A Neural Image Caption Generator&lt;/a&gt;》，推出了&lt;b&gt;NIC（Neural Image Caption）模型&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;相较于百度的m-RNN模型，NIC模型的主要不同点在于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;抛弃RNN，使用了&lt;b&gt;LSTM&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;CNN部分使用了一个比AlexNet&lt;b&gt;更好的卷积神经网络&lt;/b&gt;；&lt;/li&gt;&lt;li&gt;CNN提取的&lt;b&gt;图像特征数据只在开始输入一次&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;而从论文角度来看，该论文使用的图像标注数据集较为丰富，有Pascal VOC 2008，Flickr8K和30K，MSCOCO，SBU。其采用的自动评价标准也较为齐全，有BLEU-1，BLEU-4，METEOR和CIDEr。同时，就像我在上篇中提到的那样，论文还用人工方法客观地对NIC模型生成的标注语句进行了分级评价，展示了得分和实际效果之间的距离。下面我们主要对NIC模型本身进行一些讲解。&lt;/p&gt;&lt;p&gt;&lt;b&gt;NIC模型结构&lt;/b&gt;如下图所示：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-a5051f70a403c8284dae4fede8131bba.png" data-rawwidth="1056" data-rawheight="586"&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;图像特征部分&lt;/b&gt;是换汤不换药：我们可以看见，图像经过卷积神经网络，最终还是变成了特征数据（就是特征向量）出来了。唯一的不同就是这次试用的CNN不一样了，取得第几层的激活数据不一样了，归根结底，出来的还是特征向量；&lt;/li&gt;&lt;li&gt;&lt;b&gt;但是！&lt;/b&gt;图像特征只在刚开始的时候输入了LSTM，后续没有输入，这点和m-RNN模型是不同的！&lt;/li&gt;&lt;li&gt;&lt;b&gt;单词输入部分&lt;/b&gt;还是老思路：和m-RNN模型一样，每个单词采取了独热（one-hot）编码，用来表示单词的是一个维度是词汇表数量的向量。向量和矩阵&lt;equation&gt;W_e&lt;/equation&gt;相乘后，作为输入进入到LSTM中。&lt;/li&gt;&lt;li&gt;&lt;b&gt;使用LSTM来替换了RNN&lt;/b&gt;。LSTM是什么东西呢，简单地来说，可以把它看成是效果更好RNN吧。为什么效果更好呢？因为它的公式更复杂哈哈😝（并不是）。如果知友对LSTM的细节感兴趣，想要理解LSTM。&lt;b&gt;建议观看CS231n的视频课程第10课：Recurrent Neural Networks, Image Captioning, LSTM&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;以上模型所示的流程，可以用下列公式来概括：&lt;/p&gt;&lt;equation&gt;x_{-1}=CNN(I)&lt;/equation&gt;&lt;equation&gt;x_t=W_eS_t,\quad t\in\{0...N-1\}&lt;/equation&gt;&lt;equation&gt;p_{t+1}=LSTM(x_t),\quad t\in\{0...N-1\}&lt;/equation&gt;&lt;p&gt;那么，为什么在NIC模型中，只在第一次输入图像特征数据，而不是每次都输入了呢？论文中说：&lt;/p&gt;&lt;blockquote&gt;We empirically verified
that feeding the image at each time step as an extra input
yields inferior results, as the network can explicitly exploit
noise in the image and overfits more easily.我们从实践经验上证实如果在每一个时间点都输入图像数据，将会导致较差的结果。网络可能会放大图像数据中的噪音，并且更容易过拟合。&lt;/blockquote&gt;&lt;p&gt;后续的论文中，基本上都是采取在初始时输入一次图像特征数据，不再使用m-RNN每次都输入的方法了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型的训练&lt;/b&gt;：NIC模型的损失函数和m-RNN模型却有不同，但基本思路还是一样的：一个可求导的损失函数，利用反向传播来求梯度，然后利用随机梯度下降来学习到最优的参数。其损失函数为：&lt;/p&gt;&lt;equation&gt;L(I,S)=-\sum^N_{t=1}logp_t(s_t)&lt;/equation&gt;&lt;p&gt;&lt;b&gt;实验结果&lt;/b&gt;：经过了以上这些改进后，NIC模型比起m-RNN模型还是有了较大进步：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-a51e0c551ffce8b739942a3fb6a822a9.png" data-rawwidth="1100" data-rawheight="526"&gt;上图是不同算法在不同数据集上的BLEU-1得分的比较。可以看到NIC比起m-RNN还是有较大的进步的。当然，该论文值得深入学习的地方还有很多，但是本文主要聚焦于图像标注方面，对论文中其他任务（如图像检索等）的结果，在这里就不过多介绍了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;综上&lt;/b&gt;：NIC模型相较于m-RNN模型，其重要的改进在于：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;首先，在语言模型部分将RNN替换为了实践证明在NLP方面效果更好的&lt;b&gt;LSTM&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;其次，在图像模型部分使用了效果&lt;b&gt;更好的卷积神经网络模型&lt;/b&gt;来做图像特征数据的提取。&lt;/li&gt;&lt;li&gt;最后，改变了图像特征数据的输入方式，从m-RNN的每个时间点都输入变成了&lt;b&gt;只在初始时输入1次&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;目前最高水平模型&lt;/h2&gt;&lt;p&gt;在介绍完前面两个模型后，仍然有一些论文继续做出了更好水平的方法，但是没有选择介绍是因为他们的思路其实和NIC模型相较于m-RNN模型做出的改进思路是雷同的：更好的卷积神经网络模型，更好的语言模型，不同的图像输入方式，不同的单词嵌入方式等等。&lt;/p&gt;&lt;p&gt;那么为什么要选择这个模型呢？大家会说：当然咯，因为这是目前最好的嘛。其实并不完全是这个原因。关于如何看待论文，不久之前我看到了清华大学的&lt;a href="https://www.zhihu.com/people/45e79aed2c8b69623a57b4889414afe0" data-hash="45e79aed2c8b69623a57b4889414afe0" class="member_mention" data-editable="true" data-title="@肖寒" data-hovercard="p$b$45e79aed2c8b69623a57b4889414afe0"&gt;@肖寒&lt;/a&gt;博士在&lt;a href="https://www.zhihu.com/question/50508148/answer/121343083?from=profile_answer_card" data-editable="true" data-title="某个问题"&gt;某个问题&lt;/a&gt;下的回答，&lt;b&gt;个人认为说得非常好&lt;/b&gt;，这里&lt;b&gt;强力推荐&lt;/b&gt;：&lt;/p&gt;&lt;blockquote&gt;不过，一般注水的作者相对而言都是新手，因为比较有经验的研究者都知道：&lt;b&gt;&lt;i&gt;“论文的一切都在于贡献，不在于结果”&lt;/i&gt;&lt;/b&gt;你的结果只是一个说明你贡献的例证，多那么点少那么点，&lt;b&gt;大家看了毫无区别&lt;/b&gt;。你注水除了恶心我们这些后来实验的人，就没什么别的用处了。有那些&lt;b&gt;疯狂调参和使劲弄技巧&lt;/b&gt;的时间，真不如&lt;b&gt;拿来整理好你自己的思路，把论文的论述过程做到有理有据&lt;/b&gt;！&lt;i&gt;因为 80.2 和 80.3 正常人都没法记住其间区别，但你&lt;b&gt;循循善诱的精致论述会让所有人印象深刻&lt;/b&gt;。我希望新手不要本末倒置！&lt;/i&gt;&lt;/blockquote&gt;&lt;p&gt;所以，选择《&lt;a href="http://arxiv.org/abs/1506.01144" data-editable="true" data-title="What Value Do Explicit High Level Concepts Have in Vision to Language Problems?"&gt;What Value Do Explicit High Level Concepts Have in Vision to Language Problems?&lt;/a&gt;》这篇论文中的模型来讲，不仅仅是因为它效果好，还因为它的贡献：通过实验回答了论文题目本身提出的这个问题：&lt;b&gt;在视觉到语言问题（比如图像标注）中，明确的高等级概念到底有没有价值？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;这个问题一旦我们对于现在流行的CNN+RNN模型比较熟练了，就会自然而然地产生疑问：话说这图像特征也不知道是啥，反正我卷积神经网络几个层一过，变成了激活数据，变成了一堆浮点数构成的向量，然后就往RNN初始状态里面一丢，诶，效果还可以。但是&lt;b&gt;为啥呢？！&lt;/b&gt;为啥效果会不错呢？这明明就是一堆说不清楚的特征啊啊！图像的信息并没有用更高级的语义信息表达，就这么稀里糊涂的扔进去了。&lt;/p&gt;&lt;p&gt;该论文在摘要中就一针见血地指明了这个问题：&lt;/p&gt;&lt;blockquote&gt;Much recent progress in Vision-to-Language (V2L) prob-
lems has been achieved through a combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Net-
works (RNNs). This approach &lt;b&gt;does not explicitly represent
high-level semantic concepts, but rather seeks to progress
directly from image features to text&lt;/b&gt;. &lt;/blockquote&gt;这种直接把用CNN提取的图像特征数据扔进RNN的方法&lt;b&gt;寻求的是从图像特征直接到文本，而不是先将其用更高等级的语义概念进行表达&lt;/b&gt;。那么面对这个情况，作者们做了什么（也就是贡献）呢？&lt;blockquote&gt;In this paper we&lt;b&gt; investigate&lt;/b&gt; whether this direct approach succeeds due to, or
despite, the fact that it &lt;b&gt;avoids the explicit representation of
high-level information&lt;/b&gt;. We propose a method of &lt;b&gt;incorporating high-level concepts into the successful CNN-RNN approach&lt;/b&gt;, and show that it achieves a &lt;b&gt;significant improvement&lt;/b&gt;
on the state-of-the-art in both image captioning and visual
question answering. &lt;/blockquote&gt;&lt;p&gt;作者们说，我们就来调查一下，当前流行的这个方法它成功，到底是不是因为它就是避免了将图像信息表达为高等级的语义信息呢？于是作者们在当前的CNN+RNN模型中，增加了一个高等级的语义概念表达，结果发现这么一改，结果很好，出现了很大的提升。这就说明，&lt;b&gt;之前稀里糊涂地把图像特征直接扔进RNN并不是一个好办法，将图像特征用高等级的语义概念表达后再输入RNN会更好&lt;/b&gt;！&lt;/p&gt;&lt;p&gt;这篇论文解答了我同样存在的疑惑，并且通过改进和实验证明，我们存在疑惑的地方是可以有所作为的，改进后的方法有了较大提升。这就是我选择这篇论文的最主要原因。总之，看完摘要我就非常高兴，迫不及待地就开始跳进去想看看人家到底是怎么来做的了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;模型结构&lt;/b&gt;：如下图所示，需要注意的特点有：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-40533b56d6cf861e3d96396cd15baba0.png" data-rawwidth="1122" data-rawheight="796"&gt;&lt;ul&gt;&lt;li&gt;在语言模型部分使用的是LSTM，这一点和之前的模型&lt;b&gt;没有太大区别&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;针对3各不同的任务（图像标注、单个单词问答，语句问答）分别实际了3个语言模型部分，这里我们&lt;b&gt;只关注第一个图像标注任务&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;&lt;b&gt;改进重在视觉部分&lt;/b&gt;：请知友们往上看看之前的m-RNN和NIC模型，在他们的视觉部分，图像的处理是相对简单的：图像输入CNN，然后从CNN靠后的层中取出激活数据，输入到RNN即可。然而在这里，我们看到情况变复杂了。&lt;/li&gt;&lt;li&gt;首先预训练一个的单标签的CNN（蓝色虚线中），然后把该CNN的参数迁移到下方多标签的CNN中（红色虚线中），并对多标签的CNN做精细调整（fine-tune）。&lt;/li&gt;&lt;li&gt;图像输入到红色虚线中的CNN，输出的是一个&lt;b&gt;有高等级语义概念和对应概率的向量&lt;/b&gt;，并将这个向量作为语言部分LSTM的输入。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;也就是说，&lt;b&gt;输入LSTM的不是一个不知道到底是什么的浮点数向量了，而是我们可以理解的语义概念的概率的向量&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;论文介绍模型的时候说：我们的模型还是由图像分析和语句生成两个部分构成。&lt;b&gt;在图像分析部分，&lt;/b&gt;我们使用有监督学习来预测一个属性的集合，这些属性实际上就是图像的标注语句中常见的单词。这一步是如何做到的呢？我们把这一步&lt;b&gt;看做是一个多标签分类问题&lt;/b&gt;，训练了一个对应的深度卷积神经网络来实现。&lt;/p&gt;&lt;p&gt;图像经过模型的图像分析部分，输出的就是&lt;equation&gt;V_{att}(I)&lt;/equation&gt;，它是一个向量，其长度等于标签集合中标签的数量（也就是词汇表的数量），&lt;b&gt;每个维度上装的是某个标签对应的预测概率&lt;/b&gt;。然后这个&lt;equation&gt;V_{att}(I)&lt;/equation&gt;就要作为输入进入到LSTM，也就是语言生成部分了。&lt;/p&gt;&lt;p&gt;在针对图像标注问题的语言模型部分，该论文中简明扼要地说，我们就是按照《&lt;a href="http://arxiv.org/abs/1411.4555" data-title="Show and Tell: A Neural Image Caption Generator" class="" data-editable="true"&gt;Show and Tell: A Neural Image Caption Generator&lt;/a&gt;》论文中的方法来进行语句生成的，喏，就上面的NIC模型，所以这里也就不更多逼逼啦。&lt;/p&gt;&lt;p&gt;&lt;b&gt;属性预测部分&lt;/b&gt;：该论文，我个人感到最有价值的部分，还是在它的图像分析部分中&lt;b&gt;如何从图像到属性的实现，这是它的核心创新点，&lt;/b&gt;所以对该部分做一个比较细节的介绍。需要注意的要点有：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;属性词汇表的构建&lt;/b&gt;：语义属性是从训练集标注语句中提取出来的，可以是句子中的任何部分：物体名称（名词），动作（动词）或者性质（形容词）。使用了&lt;equation&gt;c&lt;/equation&gt;个最常用的单词来构建属性词汇表。在构建的时候，对复数和时态不区分，比如ride和riding，bag和bags被看做一个单词。这样就有效地缩小了词汇表数量，最后得到一个包含256个单词的属性词汇表；&lt;/li&gt;&lt;li&gt;&lt;b&gt;属性预测器的实现&lt;/b&gt;：有了词汇表，就希望给出一张图片，能够得到多个对应的在词汇表中的属性单词。将这个需求，&lt;b&gt;看做是一个多标签分类问题来解决&lt;/b&gt;。具体怎么做呢？如下图所示：&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-9c5eee8af0ab6d2abd8e1bc0de6d20c5.png" data-rawwidth="1212" data-rawheight="676"&gt;&lt;ul&gt;&lt;li&gt;首先拿一个用ImageNet&lt;b&gt;预训练好的VGGNet模型作为初始模型&lt;/b&gt;。然后再用MS COCO这样的有&lt;b&gt;多标签的数据集来对这个VGGNet做精细调整&lt;/b&gt;（fine-tune）。精细调整具体怎么做呢？就是将最后一个全连接层的输出输入到c分类的softmax中。c=256代表的是词汇表的数量。然后使用逐元素的逻辑回归作为损失函数：&lt;/li&gt;&lt;li&gt;&lt;b&gt;损失函数&lt;/b&gt;：假设有&lt;equation&gt;N&lt;/equation&gt;个训练样例，&lt;equation&gt;y_i=[y_{i1},y_{i2},...,y_{ic}]&lt;/equation&gt;是第i个图像对应的标签向量，如果&lt;equation&gt;y_{ij}&lt;/equation&gt;=1，表示图像中有该标签，反之则没有。&lt;equation&gt;p_i=[p_{i1},p_{i2},...,p_{ic}]&lt;/equation&gt;是对应的预测概率向量，则损失函数为：&lt;equation&gt;J=\frac{1}{N}\sum^N_{i=1}\sum^c_{j=1}log(1+exp(-y_{ij}p_{ij}))&lt;/equation&gt;。在精细调整的训练过程中只需要最小化这个损失函数值即可；&lt;/li&gt;&lt;li&gt;然后对于一张输入的图像，要将其分割成不同的局部。刚开始的时候是计划分割出上百个局部窗口，后来感到计算起来太耗费时间，就采取了归一化剪枝的算法将所有的方框分从m个簇，然后每个簇中保留最好的k个方框，最后加上原图，得到m*k+1个建议方框，将对应的局部输入到网络中。在使用中，作者令m=10，k=5；&lt;/li&gt;&lt;li&gt;对于局部方框的生成，作者们使用的方法是Multiscale Combinatorial Grouping (MCG)方法。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;实验结果：通过将图像信息表达为高等级的语义信息输入LSTM，该方法得到了一个比较显著的性能提升：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-505bc9a994886250e68e9f22e67a4fb3.png" data-rawwidth="1220" data-rawheight="736"&gt;这张表是论文方法在MSCOCO数据集上和其他很多方法，以及自己设定的基准模型之间的得分比较。注意除了困惑度（P）外，得分都是越高越好。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-5c67701be4a5dae18ab97658c661ca7d.png" data-rawwidth="1192" data-rawheight="546"&gt;这张表示论文方法在MSCOCO的5个标注和40个标注语句测试集上与如m-RNN等方法和人类得分的比较，注意论文方法在14个对比中，有13个的得分都是超过人类得分，拿到最高。&lt;/p&gt;&lt;p&gt;&lt;b&gt;注意&lt;/b&gt;：就如同我在上篇中所说，自动评价标准得分高于人类的分，并不代表实际标注语句就比人类标注语句水平高。&lt;b&gt;该论文没有如同谷歌NIC模型论文中一样，设置人工对生成的标注语句的分级评价，是一大遗憾&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;模型的比较思考&lt;/h2&gt;至此，3个模型及其论文介绍完毕。原本还有LRCN模型和斯坦福的NeuralTalk模型，后将其精简掉了。从这三个模型的进化我们可以看到一个比较清晰的脉络：一个创新之后，对这个创新中的局部进行优化，对局部之间的协作方式进行优化，对创新中说得不清晰或者不合理的部分敢于反思并探索，往往大的提升就在这些模糊的区域中了。&lt;h2&gt;代码实践&lt;/h2&gt;&lt;p&gt;&lt;b&gt;光说不练假把式&lt;/b&gt;。机器学习本来就是一个实践性很强的领域，工程能力是非常重要的一环。因此在我们专栏里面，比较推崇的就是知行合一的协作理念。在这个小节，会对CS231n课程的第三个大作业中的RNN图像标注作业部分内容进行解析，并简要介绍开源的NeuralTalk项目。&lt;/p&gt;&lt;h2&gt;CS231n #A3 RNN_Captioning&lt;/h2&gt;&lt;p&gt;首先，需要说一下的关于CS231n的几个课程作业，个人都非常推荐。希望入门深度学习的同学如能完成，则入门扎实了。我后续也会在专栏进行相关的解析。作业3相关情况请看我们的介绍：&lt;a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" data-editable="true" data-title="斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏" class=""&gt;斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏&lt;/a&gt;。由于本文主要是介绍图像标注问题，所以作业相关背景就不多说了。&lt;/p&gt;&lt;p&gt;完成RNN_Captioning作业首先需要运行jupyter notebook，然后打开&lt;b&gt;RNN_Captioning.ipynb&lt;/b&gt;文件。然后就可以看到整个作业文件是由Markdown文字说明块（Cell）和python代码块组成的，基本上是手把手教你完成该实验，在某些代码块之前，实验要求你要实现某些核心函数。&lt;/p&gt;&lt;p&gt;&lt;b&gt;代码块1-3：&lt;/b&gt;这3个代码块都不需要我们做什么，他们依次是在进行一些文件导入，初始化设置，导入MS COCO数据，显示数据集中的某些图像和标注语句。在代码块-1的下方，有对MSCOCO数据集的介绍：&lt;/p&gt;&lt;blockquote&gt;在本练习中，我们将使用微软COCO数据集2014版，该数据集已经成为图像标注的标准数据库。数据集包含80000张训练图像，40000张验证图像，每张图都有5个描述句子，句子是利用亚马逊的土耳其机器人招募人工做的。要下载数据集，到cs231n/datasets目录中运行get_coco_captioning.sh脚本。我们已经为你对数据进行了预处理并从中提取出了特征。使用在ImageNet上预训练的VGG-16网络，我们从网络的 fc7层提取出了所有图像的特征。这些特征被分别存储在train2014_vgg16_fc7.h5 和val2014_vgg16_fc7.h5两个文件中。为了减少处理时间和内存需求，还使用PCA将维度从4096减少到了512，这些数据存在train2014_vgg16_fc7_pca.h5val201h和4_vgg16_fc7_pca.h5l两个文件中。原始图像有20G，所以没有包含在这次的下载中。然而所有的图像都是从Flickr中获取，训练和验证图像的url都存在train2014_urls.txt和val2014_urls.txt，这样你就可以通过网络下载图像了。处理字符串是很低效的，所以练习中使用的是编码版的标注。每个单词都分配了一个整数ID，这样就能用数字序列来表示标注语句了。单词和ID之间的映射在文件coco2014_vocab.json中。你可以使用cs231n/coco_utils.py中的decode_captions来将装着整数ID的numpy数组转化为字符串。我们向字符表中加入了一些特殊的标记，在每个标注的开头加入&amp;lt;START&amp;gt; 结尾加入&amp;lt;END&amp;gt;，很少见的单词用 &amp;lt;UNK&amp;gt;替换。还有，因为小批量数据中的标注句子长度不同，所有在短的句子结束 &amp;lt;END&amp;gt; 后后面加上了 &amp;lt;NULL&amp;gt;标记，并且对 &amp;lt;NULL&amp;gt;标记不计算损失值和梯度。因为处理起来有点痛苦，所以我们已经帮你搞定了这些特殊标记的实现细节。使用load_coco_datah函数将所有的COCO数据进行加载。&lt;/blockquote&gt;&lt;p&gt;从上面的说明中我们可以知道几个要点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;助教在设计实验的时候，已经将图像输入到卷积神经网络中，提取出了特征数据并将其文件化，我们可以直接用了；&lt;/li&gt;&lt;li&gt;单词都是用整数ID来表示的。打开json文件我们可以看到：&lt;/li&gt;&lt;/ul&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-487ddc0cafce573205084ef7218d1329.png" data-rawwidth="604" data-rawheight="276"&gt;&lt;ul&gt;&lt;li&gt;一些特殊的符号用来表示句子的开始和结束，以及不常见的单词。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;接下来，我们需要开始在作业文件夹中的&lt;b&gt;cs231n/rnn_layers.py&lt;/b&gt;文件中实现一些核心函数&lt;b&gt;rnn_step_forward&lt;/b&gt;，不然代码块4运行是会报错的。rnn_step_forward函数实现的就是RNN模型一个时间戳的前向传播。我们先来看看代码块4。&lt;/p&gt;&lt;p&gt;代码块4：其实就是在&lt;b&gt;检验rnn_step_forward函数有没有正确实现&lt;/b&gt;。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;N, D, H = 3, 10, 4

x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)
prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)
Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)
Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)
b = np.linspace(-0.2, 0.4, num=H)

next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)
expected_next_h = np.asarray([
  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],
  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],
  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])

print 'next_h error: ', rel_error(expected_next_h, next_h)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;现在让我们用sublime text打开cs231n/rnn_layers.py文件找到rnn_step_forward函数：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def rnn_step_forward(x, prev_h, Wx, Wh, b):
  """
  Run the forward pass for a single timestep of a vanilla RNN that uses a tanh
  activation function.

  The input data has dimension D, the hidden state has dimension H, and we use
  a minibatch size of N.

  Inputs:
  - x: Input data for this timestep, of shape (N, D).
  - prev_h: Hidden state from previous timestep, of shape (N, H)
  - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)
  - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)
  - b: Biases of shape (H,)

  Returns a tuple of:
  - next_h: Next hidden state, of shape (N, H)
  - cache: Tuple of values needed for the backward pass.
  """
  next_h, cache = None, None
  #######################################################################
  # TODO: Implement a single forward step for the vanilla RNN. Store the next  #
  # hidden state and any values you need for the backward pass in the next_h   #
  # and cache variables respectively.                                          
#######################################################################
# implemente the function   #######################################################################
 #                          END OF YOUR CODE                      #######################################################################
  return next_h, cache
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;回顾课程，该函数主要要实现的就是RNN模型的下面计算：&lt;/p&gt;&lt;equation&gt;h_t=tanh(W_{hh}h_{t-1}+W_{xh}x_t)&lt;/equation&gt;&lt;p&gt;在上述函数定义中，x就是该时间点的输入。prev_h就是RNN上一个隐藏状态，即&lt;equation&gt;h_{t-1}&lt;/equation&gt;。Wx对应的就是&lt;equation&gt;W_{xh}&lt;/equation&gt;，Wh对应的就是&lt;equation&gt;W_{hh}&lt;/equation&gt;，b是偏置量。于是实现如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# stage computation
  # Step 1. mul1: Wh(H, H) dot prev_h(N, H) -&amp;gt; (N, H)
  mul1 = np.dot(prev_h, Wh)

  # Step 2. mul2: Wx(D, H) dot x(N, D) -&amp;gt; (N, H)
  mul2 = np.dot(x, Wx)

  # Step 3. add1: mul1 + mul2 -&amp;gt; (N, H)
  add1 = mul1 + mul2

  # Step 4. add2: add1(N, H) + b(H,) Broadcasting -&amp;gt; (N, H)
  add2 = add1 + b

  # Step 5. tanhed: apply tanh to add2 -&amp;gt; (N, H)
  tanhed = np.tanh(add2)
  next_h = tanhed

  # cache
  cache = (mul1, mul2, add1, add2, tanhed, x, Wx, Wh, prev_h.copy()) 
  # .copy is important!!!
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;大家看到这段实现可能会很奇怪，明明一行代码就能实现的事情，&lt;b&gt;为什么分成这么多步，还用了这么多中间变量&lt;/b&gt;？实际上，这种分段式的实现，是为了能够方便实现反向传播。为了说明这一点，接下来展示一下实现该步骤的反向传播函数rnn_step_backward。&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;def rnn_step_backward(dnext_h, cache):
  """
  Backward pass for a single timestep of a vanilla RNN.
  
  Inputs:
  - dnext_h: Gradient of loss with respect to next hidden state
  - cache: Cache object from the forward pass
  
  Returns a tuple of:
  - dx: Gradients of input data, of shape (N, D)
  - dprev_h: Gradients of previous hidden state, of shape (N, H)
  - dWx: Gradients of input-to-hidden weights, of shape (N, H) ? maybe wrong -&amp;gt; (D, H)
  - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)
  - db: Gradients of bias vector, of shape (H,)
  """
  dx, dprev_h, dWx, dWh, db = None, None, None, None, None
  #######################################################################
  # TODO: Implement the backward pass for a single step of a vanilla RNN.      #
  # HINT: For the tanh function, you can compute the local derivative in terms of the output value from tanh. 
#######################################################################
# implemente the function  
#######################################################################
  #                      END OF YOUR CODE                    
#######################################################################
  return dx, dprev_h, dWx, dWh, db
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我对该函数的实现如下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;# get the cache
  mul1, mul2, add1, add2, tanhed, x, Wx, Wh, prev_h = cache

  # backward pass
  # Back to step 5: backprop through tanh. dnext_h(N, H) taned(N, H)
  # d/dx (tanh x)^2 = 1 - (tanh x)^2
  dadd2 = (1.0 - tanhed * tanhed) * dnext_h

  # Back to step 4: z=x+y -&amp;gt; dz/dx(writted in dx) = 1, dz/dy(writted in dy) = 1
  dadd1 = 1.0 * dadd2 # -&amp;gt; (N, H)
  db = 1.0 * np.sum(dadd2, axis=0) # since db shape:(H,)

  # Back to step 3: z=x+y -&amp;gt; dz/dx(writted in dx) = 1, dz/dy(writted in dy) = 1
  dmul1 = 1.0 * dadd1 # -&amp;gt; (N, H)
  dmul2 = 1.0 * dadd1 # -&amp;gt; (N, H)

  # Back to step 2: x * y = z -&amp;gt; dx = y, dy = x
  # dWx = x * dmul2. x(N, D) dmul2(N, H) dWx should be (D, H)
  dWx = np.dot(x.T, dmul2)
  # dx = Wx * dmul2. Wx(D, H), dmul2(N, H) -&amp;gt; (N, D)
  dx = np.dot(dmul2, Wx.T)

  # Back to step 1: x * y = z -&amp;gt; dx = y, dy = x
  # dWh = prev_h * dmul1. prev_h(N, H), dmul1(N, H) -&amp;gt; (H, H)
  # which H is row ?
  dWh = np.dot(prev_h.T, dmul1)
  # dprev_h = Wh * dmul1. Wh(H, H), dmul1(N, H) -&amp;gt; (N, H)
  dprev_h = np.dot(dmul1, Wh.T)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;可以看见，这个反向传播求梯度和刚才前向传播的分步是一致的。&lt;/p&gt;&lt;p&gt;以上，就是CS231n作业3中用简单RNN来实现图像标注实验中RNN前向步进的实现，只是整个作业的一小部分，鉴于这个作业较长，且后续我会连载CS231n作业解析，所以就不全部讲解了。总体说来，完成作业后对于学习者对深度学习的理解，是很有帮助的。&lt;/p&gt;&lt;h2&gt;NeuralTalk项目&lt;/h2&gt;&lt;p&gt;在斯坦福计算机视觉实验室的论文《&lt;a href="http://arxiv.org/abs/1412.2306" data-editable="true" data-title="Deep Visual-Semantic Alignments for Generating Image Descriptions" class=""&gt;Deep Visual-Semantic Alignments for Generating Image Descriptions&lt;/a&gt;》中，作者&lt;a href="http://cs.stanford.edu/people/karpathy/" data-editable="true" data-title="Andrej Karpathy" class=""&gt;Andrej Karpathy&lt;/a&gt;给出了实验数据、代码和介绍：&lt;a href="http://cs.stanford.edu/people/karpathy/deepimagesent/" data-editable="true" data-title="Deep Visual-Semantic Alignments for Generating Image Descriptions"&gt;Deep Visual-Semantic Alignments for Generating Image Descriptions&lt;/a&gt;。现在去看页面，发现最初的基于Numpy的NerualTalk项目已经被废弃停止维护了，新的基于Torch的NerualTalk2已经发布。&lt;/p&gt;&lt;p&gt;鉴于&lt;a href="http://cs.stanford.edu/people/karpathy/" data-editable="true" data-title="Andrej Karpathy" class=""&gt;Andrej Karpathy&lt;/a&gt;已经从斯坦福计算机视觉实验室毕业，去了OpenAI，开始搞深度增强学习和tensorflow，所以这个项目是否会继续维护下去，还不得而知。&lt;/p&gt;&lt;p&gt;对于完成了CS231n作业的同学，建议可以先从老的基于Numpy的&lt;a href="https://github.com/karpathy/neuraltalk" data-editable="true" data-title="NerualTalk项目"&gt;NerualTalk项目&lt;/a&gt;入手学习。因为实际上作者自己也说：&lt;/p&gt;&lt;blockquote&gt; I am leaving it on Github for educational purposes.&lt;/blockquote&gt;&lt;p&gt;等对于老的NerualTalk项目比较熟悉了，可以考虑去阅读基于Torch的NerualTalk2，或者自己使用tensorflow来进行实现。&lt;/p&gt;&lt;h2&gt;图像标注问题展望&lt;/h2&gt;写到这里，其实我个人关于图像标注问题的看法已经比较清晰地包含在前面的文中了。想要更好地解决图像标注问题，需要：&lt;ul&gt;&lt;li&gt;更好的自动评价标准。这里个更好，是指的能够和人类评价相关性更高；&lt;/li&gt;&lt;li&gt;更大的数据集。图像更多，图像对应标注句子更多；&lt;/li&gt;&lt;li&gt;在图像分析部分，语言生成部分，或者两个部分的连接方式上出现新的模型或思路。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;总而言之，我个人对于图像标注问题比较乐观，在前面图像分类问题的深厚基础上，图像标注问题应该能够在1-2年内拿出一个接近或者达到人类标注水平的方法。&lt;/p&gt;&lt;h2&gt;作者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;欢迎对文中不妥之处批评指正；&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22520434&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Thu, 22 Sep 2016 16:40:31 GMT</pubDate></item><item><title>最前沿：深度增强学习再发力，家用机器人已近在眼前</title><link>https://zhuanlan.zhihu.com/p/22523121</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6f589df38509d14f839737645322a011_r.jpeg"&gt;&lt;/p&gt;&lt;h2&gt;1 前言&lt;/h2&gt;&lt;p&gt;如果问你“家用机器人真正实用化还需要多久？”我说的是那种可以端茶倒水，帮你干这干那的机器人，可能你的回答是10年，15年。但是现在，深度增强学习的不断发展，很可能将时间缩短到5年。&lt;/p&gt;&lt;p&gt;2016年9月16号，&lt;b&gt;Li Feifei&lt;/b&gt;组（了解ImageNet的知友们肯定都熟悉）放出了最新的Paper：&lt;/p&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/1609.05143" data-title="Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning " class="" data-editable="true"&gt;Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learning &lt;/a&gt;&lt;/p&gt;&lt;p&gt;做了什么事呢？使用深度增强学习实现目标驱动的视觉导航。说的简单一点就是机器人找东西，有一个地面机器人，让机器人去找一本书，或者去冰箱，机器人就自己去了，然后能找到物体停下。大家先看一下官方的视频：&lt;/p&gt;&lt;p&gt;&lt;video id="69269" data-swfurl="" poster="" data-sourceurl="http://v.youku.com/v_show/id_XMTczMTM5Mzk4OA==.html" data-name='Target-driven Visual Navigation in Indoor Scenes using Deep Reinforcement Learni—在线播放—优酷网，视频高清在线观看" &amp;gt;&amp;lt;/a&amp;gt;&amp;lt;meta name="irTitle" conte'&gt;&lt;/video&gt;&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;&lt;b&gt;实现机器人找东西有多困难呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;very hard! 下文我们会说一下传统机器人学的方法。&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;采用深度增强学习实现又有多大的意义？&lt;/b&gt;&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;先说结论。简单的说就是以前为了实现机器人找东西这个事情，我们需要做大量的工作，大量的hand-engineering，但是现在，采用深度增强学习，我们只需要使用神经网络。就是机器人只根据实时看到的画面还有目标选择动作，这和人类的行为很像。并且整个过程都是学习来的。这种方法具有革命性的意义。&lt;/p&gt;&lt;p&gt;还记得之前那篇文章吗？&lt;a href="https://zhuanlan.zhihu.com/p/21470871?refer=intelligentunit" class="" data-editable="true" data-title="最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能！ - 智能单元 - 知乎专栏"&gt;最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能！ - 智能单元 - 知乎专栏&lt;/a&gt; 那篇Paper作者研究如何通过迁移学习将不同场景学到的知识移植过来。特别是从虚拟到现实的迁移。但是，这篇文章想了个更简单的做法-------我们造一个非常仿真的环境不就完了。&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-92b55a9f3ef3e25426edcba6ab5ef6ff.png" data-rawwidth="1014" data-rawheight="662"&gt;&lt;p&gt;因此，这篇文章中，作者构建了一个非常好的仿真环境（如上图），并且通过在高度仿真的环境中训练，然后迁移到真实场景中。这种方法被证明是有效的。那么想象一下，如果构建了一个更复杂更真实的场景，然后让机器人在里面无限次的训练学习，掌握技能之后，再移植到现实世界。这将是game-changing的事情。让家用机器人能够端茶倒水将不再那么困难。&lt;/p&gt;&lt;p&gt;下面，我们来好好分析一下这篇文章的意义。最重要的是深度增强学习的意义。&lt;/p&gt;&lt;h2&gt;2 机器人找东西，传统的方法怎么做？&lt;/h2&gt;&lt;p&gt;假设我们面前有一台轮式机器人，有摄像头，可控制，上面还有一个机械臂。我们希望能够跟机器人说去帮我拿一个杯子过来。然后机器人能够自主的去厨房找到一个杯子，用机械臂拿起来，最后送到我手上。那么为了实现这么一个任务，我们首先需要实现的就是去找到杯子这个任务。我们再进一步假设房间里的东西都是静止的，然后要求机器人能够去找房间中的各种物品。&lt;/p&gt;&lt;p&gt;OK，在这种假设下，传统的机器人学告诉我们要怎么实现这个系统呢？&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 1：建图 Mapping和定位Localization也就是SLAM&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-05310424cd5a51deeb9a97d6484a8f1c.jpeg" data-rawwidth="448" data-rawheight="324"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-e08704b6dc925b575c2f1fe15e92be84.png" data-rawwidth="1024" data-rawheight="619"&gt;&lt;p&gt;要想找东西，总得先知道自己在哪，还要知道房间的结构好规划路线。因此，我们得先利用机器人上面的传感器比如摄像头、雷达等构建整个场景，因为要找东西，有的高，有的低，所以最好还是3d场景。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 2：构建语义地图Semantic Map&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/v2-7b91f9c057908d1b0f311266978921d2.jpeg" data-rawwidth="600" data-rawheight="424"&gt;&lt;p&gt;要找东西，当然要知道东西在哪里了。因此，这一步需要我们在上一步得到的地图上添加语义，也就是各种东西的位置信息。或者反过来也可以给3d地图里面的每个东西贴标签，这是冰箱，我就贴上冰箱的标志，那是门，那我就贴上门的标签。大家可以看到，这需要计算机视觉的物体检测（Object Detection）技术，比如我们可以使用YOLO或者Fast R-CNN算法在SLAM的同时进行物体检测，为物体贴上标签。比如上图所示，我们知道桌子，椅子，柜子的具体位置。做完这一步，我们不但知道整个房屋的构造，机器人自己的位置，也知道每一个物体的相对位置。&lt;/p&gt;&lt;p&gt;&lt;b&gt;Step 3：路径规划（Path Planning）&lt;/b&gt;&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/v2-6d2c266ed5d12dfd50c77b13bce14fb7.png" data-rawwidth="938" data-rawheight="526"&gt;&lt;p&gt;有了地图和定位，再加上目标位置，ok，万事俱备了。我们只要根据这个设计出一条最佳的路径，然后让机器人走就行了。这就是路径规划需要干的事情。&lt;/p&gt;&lt;p&gt;&lt;b&gt;小结一下&lt;/b&gt;&lt;/p&gt;&lt;p&gt;在传统的机器人学中，要实现机器人找东西这件事，需要完成上面三大任务。真是太复杂。单单SLAM都是一个很大的研究课题了。而这中间需要的计算量，需要的人力工程，都是非常多的。那么，这篇Paper是怎么做的呢？&lt;/p&gt;&lt;h2&gt;3 深度增强学习是怎么做的？&lt;/h2&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/v2-062f4429783928c286e33d62ddd488d4.png" data-rawwidth="1350" data-rawheight="700"&gt;深度增强学习的思路就完全不是机器人学的思路了，而是人的思路。上一节我们说又要建图，又要路径规划的。问题是：我们人需要这么做吗？不得不承认，我们人具备自动在大脑中构建3d场景的能力，但是对于找东西我们并不需要这么做。我们人就是大致知道东西的位置，然后往那个方向上走就行了。&lt;/p&gt;&lt;p&gt;&lt;b&gt;因此，深度增强学习要仅使用2D的视觉信息来完成找东西的任务！&lt;/b&gt;&lt;/p&gt;&lt;p&gt;如上图所示，机器人找东西的任务变成：看到图像，选择行走动作，直到找到想要的物体，停下。&lt;/p&gt;&lt;p&gt;&lt;b&gt;这篇文章的创新点在哪呢？&lt;/b&gt;&lt;/p&gt;&lt;p&gt;就是将目标图像作为输入！老实说这个创新点非常简单，也蛮低级的，而且仅针对这种目标能够具化的任务有效。但是对于机器人找东西而言，等于和人一样给个目标图像，然后让你去找到它。这样做的好处就是&lt;b&gt;神经网络具有通用性。&lt;/b&gt;之前，如果只用当前看到的图像作为输入，那么每找一个东西，都要单独训练一个网络。现在，把目标图像作为输入进行训练，那么就可以使这个网络不管输入什么目标都ok，也就具备了通用性。&lt;/p&gt;&lt;p&gt;这篇文章采用了&lt;b&gt;&lt;a href="https://arxiv.org/abs/1602.01783" data-title="A3C算法" class=""&gt;A3C算法&lt;/a&gt;&lt;/b&gt;，也就是Deepmind提出的当前深度增强学习最强的算法来训练，主要就是神经网络的模型变了，还有多线程训练的方式做了一点改变（也就是同时开启多个Agent训练多个场景多个任务）。甚至，还根据不同场景搞不同的神经网络来输出，这算不算通用我得打个问号。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/v2-dd93e3c95dcb22ec636baf710e73d287.png" data-rawwidth="1356" data-rawheight="1014"&gt;为了让观察的图像输入和目标图像输入能够和在一起，为两个图像采用了相同的预训练好的神经网络（使用ImageNet训练的深度残差网络）。&lt;/p&gt;&lt;p&gt;要理解整个训练过程，需要熟悉A3C算法，鉴于篇幅，这里就不细讲了。本专栏将陆续发布更多文章介绍深度增强学习的各种算法。&lt;/p&gt;&lt;p&gt;对于这个成果我们应该思考一个问题是：&lt;b&gt;神经网络是如何让机器人找东西的？&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;&lt;b&gt;4 深度神经网络通过学习学到了什么？&lt;/b&gt;&lt;/h2&gt;&lt;p&gt;它并没有记住物体的位置，更不知道房屋的结构。但它记住了在每一个位置，通向各个物体的行为方法。&lt;/p&gt;&lt;p&gt;比如说，冰箱在左前方5米，那么机器人在这个位置的时候根据看到的图像做出向左前方走的动作。它只是知道要这么走，它知道这个轨迹，但是并不知道其他。&lt;/p&gt;&lt;p&gt;反过来想，传统的机器人学方法可以通过记录轨迹来控制吗？&lt;/p&gt;&lt;p&gt;当然是可以，但是同样的，SLAM和语义地图还有路径规划都是避开不了的工程。而采用深度增强学习，我们是让机器人通过不断试错的方法来找到这条路径，而不是计算。&lt;/p&gt;&lt;h2&gt;5 小结与展望&lt;/h2&gt;&lt;p&gt;深度增强学习确实是一种非常模仿人类行为的思路。在真实环境中，我们确实很难让机器人通过试错来找到正确的行为方式，但是如果我们能够构建非常逼真的仿真环境的话，我们就很有希望在仿真环境中训练机器人，然后迁移到真实场景中。这样的话，让机器人通过自学习来掌握各种技能就真不是说说而已了。我怀疑DeepMind已经在构建非常好的仿真环境训练他们的机器人，也许DeepMind的下一个重量级工作就是深度增强学习的机器人了！&lt;/p&gt;&lt;h2&gt;声明：本文为原创文章，未经作者允许不得转载！&lt;/h2&gt;&lt;p&gt;本文图片来源于Paper及网络。&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22523121&amp;pixel&amp;useReferer"/&gt;</description><author>Flood Sung</author><pubDate>Wed, 21 Sep 2016 16:09:25 GMT</pubDate></item><item><title>看图说话的AI小朋友——图像标注趣谈（上）</title><link>https://zhuanlan.zhihu.com/p/22408033</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/7c2ef2d9f4d9244473a201b0b19318ec_r.jpg"&gt;&lt;/p&gt;&lt;b&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创，禁止未授权转载。&lt;/b&gt;&lt;blockquote&gt;&lt;b&gt;前言&lt;/b&gt;：近来&lt;b&gt;图像标注（Image Caption）&lt;/b&gt;问题的研究热度渐高。本文希望在把问题和研究介绍清楚的同时行文&lt;b&gt;通俗有趣&lt;/b&gt;，&lt;b&gt;让非专业读者也能一窥其妙&lt;/b&gt;。文中还提出了&lt;b&gt;开源构建一个基于守望先锋的中文图像标注数据集的构想&lt;/b&gt;，欢迎知友讨论参与。&lt;/blockquote&gt;&lt;h2&gt;内容列表：&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;&lt;b&gt;图像标注问题简介&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;图像标注是什么&lt;/li&gt;&lt;li&gt;当前水平&lt;/li&gt;&lt;li&gt;价值和意义&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;图像标注数据集&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;MSCOCO&lt;/li&gt;&lt;li&gt;Flickr8K和Flickr30K&lt;/li&gt;&lt;li&gt;PASCAL 1K&lt;/li&gt;&lt;li&gt;&lt;b&gt;创建一个守望先锋数据集&lt;/b&gt;？&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;图像标注评价标准&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;人类判断与自动评价标准&lt;/li&gt;&lt;li&gt;Perplexity&lt;/li&gt;&lt;li&gt;BLEU&lt;/li&gt;&lt;li&gt;ROUGE&lt;/li&gt;&lt;li&gt;METEOR&lt;/li&gt;&lt;li&gt;CIDEr &lt;b&gt;&lt;i&gt;注：上篇截止处&lt;/i&gt;&lt;/b&gt;&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;图像标注模型发展&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;百度的m-RNN&lt;/li&gt;&lt;li&gt;谷歌的NIC&lt;/li&gt;&lt;li&gt;斯坦福的NeuralTalk&lt;/li&gt;&lt;li&gt;目前的State of art&lt;/li&gt;&lt;li&gt;对几个模型的比较&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;代码实践&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;CS231n的LSTM_Captioning&lt;/li&gt;&lt;li&gt;基于Numpy的NerualTalk&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;b&gt;图像标注问题展望&lt;/b&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;模型图像部分、语言部分和连接方式上的更新&lt;/li&gt;&lt;li&gt;自动评价标注的更新&lt;/li&gt;&lt;li&gt;数据集的更新&lt;/li&gt;&lt;li&gt;小结&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;h2&gt;问题简介&lt;/h2&gt;&lt;p&gt;图像标注问题其本质是视觉到语言（Visual-to-Language，即V2L）的问题，解释起来很简单，就是四个字：&lt;b&gt;看图说话&lt;/b&gt;。就像老师要求小朋友们在看图说话作业中完成的任务一样，我们也希望算法能够&lt;b&gt;根据图像给出能够描述图像内容的自然语言语句&lt;/b&gt;。然而这种对于人类实在是小事一桩的小儿科级任务，在计算机视觉领域却不能不说是一个挑战：因为图像标注问题需要在两种不同形式的信息（图像信息到文本信息）之间进行“翻译”。&lt;/p&gt;&lt;p&gt;随着深度学习领域的发展，一种将深度卷积神经网络（Deep Convolutional Neural Network）和循环神经网络（Recurrent Neural Network）结合起来的方法在图像标注问题上取得了显著的进步。由于该方法的成功，使得基于该方法的对图像标注问题研究迅速地火热起来，在2016年的&lt;a href="http://cvpr2016.thecvf.com" data-title="IEEE国际计算机视觉与模式识别会议" class="" data-editable="true"&gt;IEEE国际计算机视觉与模式识别会议&lt;/a&gt;（即IEEE Conference on Computer Vision and Pattern Recognition，缩写为&lt;b&gt;CVPR&lt;/b&gt;）上专门有一个小型会议（session）的主题就是图像标注。&lt;/p&gt;&lt;p&gt;然而在“人工智能领域取得进展”这个问题上，显然大众和专业研究者们敏感程度是大不相同的，吐槽如下图所示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/d2195427d901f96541ef96446961233f.jpg" data-rawwidth="990" data-rawheight="300"&gt;&lt;b&gt;研究者&lt;/b&gt;：看我们根据图像生成了描述语句咯，不是以前的标签咯，厉不厉害？！~\(≧▽≦)/~&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/4906d07619a0b44d3f7fe726207df965.jpg" data-rawwidth="400" data-rawheight="297"&gt;&lt;b&gt;热心AI的大众&lt;/b&gt;：&lt;b&gt;→_→&lt;/b&gt;&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/e29fa599072b844a5cb31f542d90a3e9.jpg" data-rawwidth="600" data-rawheight="312"&gt;&lt;b&gt;暴雪&lt;/b&gt;：在“智能危机”结束之后，一群被放逐的&lt;b&gt;智能机器人&lt;/b&gt;感受到了被其称为“灵魂觉醒”的升华之道。他们在&lt;b&gt;冥思其存在本质和意义多年&lt;/b&gt;之后，渐渐相信他们不止是人工智能而已，和人类一样，他们&lt;b&gt;也有灵魂&lt;/b&gt;。——摘自&lt;a href="goog_1712963362" data-editable="true" data-title="守望先锋官网英雄"&gt;守望先锋官网英雄&lt;/a&gt;&lt;a href="http://ow.blizzard.cn/heroes/zenyatta" data-editable="true" data-title="禅雅塔简介"&gt;禅雅塔简介&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/aec9b9d222cfd8d2881c3df70db50455.jpg" data-rawwidth="400" data-rawheight="300"&gt;&lt;b&gt;热心AI的大众&lt;/b&gt;：和尚放大了！&lt;b&gt;恁死对面的源氏&lt;/b&gt;！世界需要更多的英雄！&lt;/p&gt;&lt;p&gt;之所以有这种反差，是大众对于人工智能认识的起点，几乎是人工智能研究者奋斗的终极目标：）&lt;/p&gt;&lt;p&gt;回到图像标注问题，那么现在该领域最厉害的方法，到底达到了什么样的水平？我的看法是：&lt;b&gt;貌似接近人类，其实差距尚大&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;为什么说貌似接近人类呢？这里引用几篇论文中表格：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/899efcd8d9b02c2c7f7c7bd5b1038468.png" data-rawwidth="782" data-rawheight="374"&gt;上面这个表格来自于目前图像标注领域结果最好的论文：《&lt;a href="http://arxiv.org/abs/1506.01144" data-title="What Value Do Explicit High Level Concepts Havein Vision to Language Problems?" class="" data-editable="true"&gt;What Value Do Explicit High Level Concepts Havein Vision to Language Problems?&lt;/a&gt;》，作者们来自澳大利亚Adelaide大学的计算机学院。上面表格是论文中作者们展示了自己的算法和其他算法在微软的COCO数据库的测试集上取得的测试结果。其中，5-Refs和40-Refs表示的是测试集中有两个数据集，一个数据集每张图像有5个参考标注（也就是人类输入的正确语句），一个数据集每张图像有40个参考标注。&lt;/p&gt;&lt;p&gt;B-N（N=1，2，3，4），M，R和CIDEr代表的是4中不同的对于算法的自动评价标准，后文会详细介绍，这里只需要&lt;b&gt;知道得分越高越好&lt;/b&gt;。参与比较的有包含论文方法在内的4种图像标注算法和人类水平。可以看见在&lt;b&gt;14个得分中，论文方法有13个得分都超过了人类得分&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;那么是不是论文方法就已经超越人类水平了呢？在我看来，答案是否定的&lt;/b&gt;。至于为什么，原因将在后文关于&lt;b&gt;图像标注算法评价标准的小节&lt;/b&gt;中揭晓。&lt;/p&gt;&lt;p&gt;虽说我个人认为最高水平的算法也尚未超过人类表现，但是其进展是毋庸置疑的，各种算法已经能够让人类评价者觉得非常不错的图像标注了。比如在上表中提到的LRCN模型，就能根据图像生成这样的描述：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/fc97868a916e972e0d36baefb8fa9df5.png" data-rawwidth="1580" data-rawheight="454"&gt;大家看了是不是觉得已经非常惊艳了？不要太激动，依旧存在一些惨不忍睹的：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/4c01fe630c5e0c43fab8f461add7c2e5.png" data-rawwidth="1424" data-rawheight="396"&gt;这些错误的图像标注案例是来自斯坦福视觉实验室的论文《&lt;a href="http://arxiv.org/abs/1412.2306" data-editable="true" data-title="Deep Visual-Semantic Alignments for Generating Image Descriptions" class=""&gt;Deep Visual-Semantic Alignments for Generating Image Descriptions&lt;/a&gt;》，第一作者是Andrej Karpathy，学习了斯坦福深度学习课程CS231n 2016的同学对作为讲师的他应该很熟悉了。我们可以看见最右边的图像标注语句尤其离谱，“在路中间站着一匹马”，大兄弟，感情这马🐴是透明的咯？&lt;/p&gt;&lt;p&gt;图像标注问题如果能够得到很好的解决，那么价值是显而易见的，可以应用到&lt;b&gt;图像检索，儿童教育和视力受损人士的生活辅助等方面&lt;/b&gt;。而从学术的角度来看，当前图像标注问题的研究，促使人工智能领域的两大领域，计算机视觉和自然语言处理很好地结合，这种跨子领域的结合能够催生出更让人惊艳的方法吗？我个人表示很期待。&lt;/p&gt;&lt;h2&gt;图像标注数据集&lt;/h2&gt;&lt;p&gt;到目前为止，深度学习依旧是一种需要大量数据来进行驱动的方法。小样本学习尚未有突破性的进展，所以数据对于基于深度学习的算法依旧非常重要。在图像标准问题研究的过程中，研究者们对于基准数据库的选择偏好也在发生变化，一些数据集运用的越来越广泛，而一些数据集则越来越少地被使用。本小节将基于图像标注问题，对这些数据集做简要的介绍和对比。&lt;/p&gt;&lt;h2&gt;Microsoft COCO Caption数据集&lt;/h2&gt;&lt;p&gt;Microsoft COCO Caption数据集的推出，是建立在Microsoft Common Objects in COntext
(COCO)数据集的工作基础上的。在论文《&lt;a href="http://arxiv.org/abs/1504.00325" data-editable="true" data-title="Microsoft COCO Captions: Data Collection and Evaluation Server"&gt;Microsoft COCO Captions: Data Collection and Evaluation Server&lt;/a&gt;》中，作者们详细介绍了他们基于MS COCO数据集构建MS COCO Caption数据集的工作。&lt;/p&gt;&lt;p&gt;简要地来说，就是对于原COCO数据集中约330,000张图像，使用亚马逊公司的“&lt;b&gt;土耳其机器人（Mechanical
Turk）&lt;/b&gt;”服务，&lt;b&gt;人工地&lt;/b&gt;为每张图像都生成了至少5句标注，标注语句总共超过了约150万句。至于亚马逊的“土耳其机器人”服务，其实也就是另一种形式的雇人拿钱干活而已。&lt;/p&gt;&lt;p&gt;实际上，COCO Caption数据集包含了两个数据集：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;第一个数据集是MS COCO c5。它包含的训练集、验证集合测试集图像和原始的MS COCO数据库是一致的，只不过每个图像都带有5个人工生成的标注语句。&lt;/li&gt;&lt;li&gt;第二个数据集是MS COCO c40。它只包含5000张图片，而且这些图像是从MS COCO数据集的测试集中随机选出的。和c5不同的是，它的每张图像都有用40个人工生成的标注语句。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;之所以要做MS COCO c40数据集，是因为如果有更多的参考标注语句，很多对于算法生成的标注的自动计算标准能够和人类判断有更高的相关性。下一步可能将MS COCO验证集中所有的图像都加上40个人工生成的标注语句。&lt;/p&gt;&lt;p&gt;作者们的另一个主要工作就是搭建了一个评价服务器，实现了当前最流行的评价标准（BLEU, METEOR, ROUGE and CIDEr）。使用MS COCO Caption数据集训练并用验证机调参后，研究者可以按照固定的JSON格式：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;[{
"image_id":int,
"caption" :str,
}]&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;向服务器上传自己算法对于测试集图像生成的标注语句，服务器将自动地给出各种评价标准的得分。要上传结果，需要在&lt;a href="https://www.codalab.org/competitions/3221" data-editable="true" data-title="CodaLab"&gt;CodaLab&lt;/a&gt;注册账号，且每个账号能够提交结果的次数是有限的。微软在Github上也提供了能够在本地对验证集数据生成标注进行评价的代码，地址&lt;a href="https://github.com/tylin/coco-caption" data-editable="true" data-title="在这里"&gt;在这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;简言之，MS COCO Caption数据集就是针对图像标注问题创建的，图像及其标注数量大，提供了现成的评价标准计算服务器和代码。就目前发表的高水平论文来看，MS COCO Caption数据集已经越来越成为研究者的首选。&lt;/p&gt;&lt;h2&gt;Flickr8K和30K&lt;/h2&gt;&lt;p&gt;Flickr8K和Flickr30K数据集的特性从它们的命名就能很方便地猜测出来：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;图像数据来源是雅虎的相册网站Flickr；&lt;/li&gt;&lt;li&gt;数据集中图像的数量分别是8,000张和30,000张（确切地说是31,783）；&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;这两个数据库中的图像大多展示的是人类在参与到某项活动中的情景。每张图像的对应人工标注依旧是5句话。这两个数据库本是同根生，所以其标注的语法比较类似。数据库也是按照标准的训练集、验证集合测试集来进行分块的。&lt;/p&gt;&lt;p&gt;相较于MS COCO Caption数据集，Flickr8K和Flickr30K数据集的明显劣势就在于其数据量不足。我个人“不乏恶意”地揣度：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;微软：他们搞图像标注都用什么数据库呀？
员工：Flickr数据集居多吧。
微软：数量多少呀？
员工：开始是8k，后来出了个30k，感觉够用了。
微软：搞个大新闻，让他们只有我们一个零头！做个330k的！
员工：好的老板，是的老板，微软大法好！&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在论文《&lt;a href="http://arxiv.org/abs/1411.5654" data-editable="true" data-title="Learning a Recurrent Visual Representation for Image Caption Generation"&gt;Learning a Recurrent Visual Representation for Image Caption Generation&lt;/a&gt;》中作者指出：&lt;/p&gt;&lt;blockquote&gt;We observed this fine-
tuning strategy is particularly helpful for MS COCO, but
does not give much performance gain on Flickr Datasets before it overfits. The Flickr datasets may not provide enough
training data to avoid overfitting. 翻译：我们观察到这个精细调整策略在使用MS COCO数据集训练的时候效果很好，但是在Flickr数据集上算法却没有很明显的提升。可能是因为Flickr数据集没有提供足够多的数据来防止算法过拟合吧。&lt;/blockquote&gt;&lt;p&gt;眼尖的知友会发现，上面这篇论文的作者中，有一位是微软的......于是我又忍不住“邪恶”脑洞一下：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;员工：老板，论文发了！
微软：不错，黑得有水平，不留痕迹。
员工：那您看......
微软：好说好说。&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;以上虽然是玩笑话，但是数据量上的劣势，确实使Flickr数据集正逐渐失宠，14年论文中几乎都使用，现在一些高水平论文仅在补充文档中展示甚至不采用。个人还是希望Flickr数据集能够有一个比较好的更新，比如来个333k的？&lt;/p&gt;&lt;h2&gt;PASCAL 1K&lt;/h2&gt;&lt;p&gt;该数据集的图像是大名鼎鼎的PASCAL VOC challenge图像数据集的一个子集，对于其20个分类，随机选出了50张图像，共1,000张图像。然后同样适用亚马逊公司的土耳其机器人服务为每张图像人工标注了5个描述语句。一般说来，这个数据集只是用来测试的。&lt;/p&gt;&lt;p&gt;在其他论文中，还有一些诸如&lt;b&gt;IAPR TC-12&lt;/b&gt;，&lt;b&gt;SBU&lt;/b&gt;等数据集，数据采集思路都大同小异，这里就不一一介绍了。&lt;/p&gt;&lt;h2&gt;创建基于守望先锋的图像标注数据集？&lt;/h2&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/6ee0e745fcdc2659f6a54f5409351e4b.jpg" data-rawwidth="1920" data-rawheight="1030"&gt;&lt;blockquote&gt;标注：来自东方的戴眼镜的某组织领导者正在殴打来自东方某半岛的女性直播。&lt;/blockquote&gt;&lt;p&gt;脑洞简言之：&lt;b&gt;开源搭建一个简单的图像标注页面，接受玩家的游戏截图投稿和对图像的中文标注，当数据收集达到目标数量后，数据集对所有参与贡献的人开放&lt;/b&gt;。做这件事的价值有两点：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;目前为止，个人&lt;b&gt;没有看到一个基于中文的公开图像标注数据集&lt;/b&gt;。&lt;/li&gt;&lt;li&gt;基于守望先锋的故事背景，能够&lt;b&gt;吸引更多的年轻人从对游戏中人工智能的兴趣进而对真正的人工智能研究感兴趣&lt;/b&gt;。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;为啥我对守望先锋情有独钟，主要是游戏基于后人工智能危机时代的世界观设定和故事吸引。看着网易乐呵呵地说守望先锋的销量超过了暗黑破坏神3，那么守望的销量也该有300万了吧？基于这个&lt;b&gt;庞大的玩家群体&lt;/b&gt;，如果这个数据集创建项目能够在玩家中有一定影响力，那么数据集的图像和标注数量应该比较客观。&lt;/p&gt;&lt;p&gt;当然，这只是我的一个小脑洞，&lt;b&gt;欢迎认可这个脑洞的知友在评论中留言讨论&lt;/b&gt;！我个人是真心希望推动这个脑洞成真。对了，个人倾向于&lt;b&gt;使用Ruby On Rails&lt;/b&gt;来搭建网站及后台。&lt;/p&gt;&lt;h2&gt;图像标注评价标准&lt;/h2&gt;&lt;p&gt;在简介中，提到虽然在多个评价标准的得分中，最新的方法的得分已经超过人类得分，但是我仍然不认为算法的真实水平超过人类，其原因就在于这些&lt;b&gt;自动评价标准（automatic evaluation metric）&lt;/b&gt;上！&lt;/p&gt;&lt;h2&gt;人类判断与自动评价标准&lt;/h2&gt;&lt;p&gt;简而言之，算法根据图像生成出来的标注语句质量高不高？和图像内容是不是相符？语法上有没有错误？&lt;b&gt;评价这些最靠谱最权威的还是咱们人类老爷&lt;/b&gt;！而各类的&lt;b&gt;自动评价标准的目前都是尽量让自己的计算结果能够和人类判断结果相关&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;论文《&lt;a href="http://www.aclweb.org/anthology/P/P14/P14-2074.xhtml" data-editable="true" data-title="Comparing Automatic Evaluation Measures for Image Description"&gt;Comparing Automatic Evaluation Measures for Image Description&lt;/a&gt;》关于这个问题就有很细致的分析。论文基于Flickr8K和E&amp;amp;K数据集，对BLEU、ROUGE、METEOR，TER几个评价标准与人类判断的相关性进行了研究，结果显示：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/c848d3b9934448272355b67eaf6ed21c.png" data-rawwidth="724" data-rawheight="482"&gt;论文中指出：与人类判断的相关性co-efficient在0.0–0.1是不相关，0.11–0.4是弱相关，0.41–0.7是中等相关，0.71–0.90是强相关，如果在0.91–1.0那就很完美了。所以论文的结论是首先推荐METEOR，或者使用ROUGE SU-4和Smoothed BLEU。PS：由于CIDEr标准是2015发布，所以这篇论文中没有体现。&lt;/p&gt;&lt;p&gt;而在谷歌2015年的论文《&lt;a href="http://arxiv.org/abs/1411.4555" data-title="Show and Tell: A Neural Image Caption Generator" class="" data-editable="true"&gt;Show and Tell: A Neural Image Caption Generator&lt;/a&gt;》中，更是三图胜千言：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/cda54a4f425a16b865032c9dcba879c5.png" data-rawwidth="772" data-rawheight="734"&gt;我们可以看到：在表1中，实验使用了微软的COCO数据集，3中评价标准的得分，谷歌NIC模型的得分和人类（Human）得分是不相仲伯的。在表2中，基于不同数据集统一计算BLEU-1得分，NIC的得分和人类得分也比较接近。是不是很牛了？&lt;/p&gt;&lt;p&gt;&lt;b&gt;&lt;u&gt;然而难能可贵的是人家马上就自己打自己的脸！&lt;/u&gt;&lt;/b&gt;在装了逼后，作者们&lt;b&gt;马上开始说实话&lt;/b&gt;。补充了一个基于人类判断的实验，邀请人类对于自己生成的标准语句进行评级，一共分成4个等级：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/79db143aa3a04a40d8f9b33078690b0e.png" data-rawwidth="1912" data-rawheight="458"&gt;如上图所示，分成“描述没有错误”、“描述中有点小错误”、“多少还是和图像相关”和“和图像无关”4个等级，分别得分从4到1。那么，真实的对比就来了：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/a9ba6674deaa9b1e05d971d4584854ca.png" data-rawwidth="884" data-rawheight="664"&gt;上图中，x坐标是BLEU得分，y坐标是表示积累分布（也就是说，输出的描述语句集合中，有百分只多少的得分大于当前的x）。其中：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Flickr-8k：NIC表示的是使用NIC模型在Flick8k测试集上跑的结果的得分曲线；&lt;/li&gt;&lt;li&gt;Pascal：NIC表示的是是使用NIC模型在Pascal测试集上跑的结果的得分曲线；&lt;/li&gt;&lt;li&gt;COCO-1k：NIC表示的是是使用NIC模型在COCO-1k测试集上跑的结果的得分曲线；&lt;/li&gt;&lt;li&gt;Flickr-8k：ref表示的是另一篇论文的结果的得分曲线，这里作为一个基准；&lt;/li&gt;&lt;li&gt;Flickr-8k：GT表示的是对Flickr-8k图像的人工标注语句同样进项人工分等级评价的结果。&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;由此可见：虽然自动裁判员BLEU-4认为NIC模型的得分超出了人类得分，但是如果让人类来当裁判员，NIC还差得远哪！这一结果也印证了上一篇论文中对于不同自动评价标准的分析。&lt;/p&gt;&lt;p&gt;当然，自动评价标准也在持续发展着，2015年发布的专门面向图像标注问题的自动评价标准CIDEr就做的更好些。&lt;/p&gt;&lt;p&gt;在刚才的人类判断与自动评价标准讨论中，几个标准的名称大家都应该比较熟悉了，接下来我就简单地介绍一下，并帮助大家理解它们是如何进行计算的：&lt;/p&gt;&lt;h2&gt;Perplexity&lt;/h2&gt;&lt;p&gt;首先，&lt;b&gt;这个perplexity该翻译成中文的哪个词&lt;/b&gt;就让我反复琢磨了好些天。查阅资料的过程中，大家要么就是不翻译（非我所认同），或者翻译为&lt;b&gt;复杂度&lt;/b&gt;、&lt;b&gt;混乱度&lt;/b&gt;或者&lt;b&gt;困惑度&lt;/b&gt;等。如何翻译先按下不表，所谓“信达雅”，第一是要把事情说明白，那么我们就先来把Perplexity理解了，再来选择译名不迟：&lt;/p&gt;&lt;p&gt;关于perplexity，在维基百科上有&lt;a href="https://en.wikipedia.org/wiki/Perplexity" data-editable="true" data-title="详细的解释"&gt;详细的解释&lt;/a&gt;，但是这里我想引用的是2014年百度的论文《&lt;a href="http://arxiv.org/abs/1410.1090" data-title="Explain Images with Multimodal Recurrent Neural Networks" class="" data-editable="true"&gt;Explain Images with Multimodal Recurrent Neural Networks&lt;/a&gt;》中对perplexity的定义公式。为什么引用这篇呢？这篇论文是我读的论文中最早提出将RNN和CNN结合起来用于图像标注的，以此表示敬意。作者们在简介中也是这么说的：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;To the best of our knowledge, this is the first work that incorporates the Recurrent
Neural Network in a deep multimodal architecture.&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;他们提出的&lt;b&gt;m-RNN&lt;/b&gt;模型也是后续论文方法的常用比较方法之一。在论文中，作者们结合图像标注任务，将perplexity定义为：&lt;/p&gt;&lt;equation&gt;log_2PPL(w_{1:L}|I)=-\frac{1}{L}\sum^L_{n=1}log_2P(w_n|w_{1:n-1},I)&lt;/equation&gt;&lt;p&gt;其中，&lt;equation&gt;L&lt;/equation&gt;是句子的长度，&lt;equation&gt;PPL(w_{1:L}|I)&lt;/equation&gt;就是根据图像&lt;equation&gt;I&lt;/equation&gt;给出的描述句子&lt;equation&gt;w_{1:L}&lt;/equation&gt;的perplexity。而&lt;equation&gt;P(w_n|w_{1:n-1},I)&lt;/equation&gt;是根据图像&lt;equation&gt;I&lt;/equation&gt;和前面的单词序列&lt;equation&gt;w_{1:n-1}&lt;/equation&gt;生成下一个单词&lt;equation&gt;w_n&lt;/equation&gt;的概率。对于此前没有接触过自然语言处理的同学（包括我）来说，此刻的感觉就是：&lt;b&gt;what fxxk ?!&lt;/b&gt;&lt;/p&gt;&lt;blockquote&gt;不要惊慌。——《&lt;a href="https://book.douban.com/subject/1394364/" data-editable="true" data-title="银河系漫游指南"&gt;银河系漫游指南&lt;/a&gt;》&lt;/blockquote&gt;&lt;p&gt;&lt;b&gt;举例子&lt;/b&gt;：下面我们用守望先锋的游戏画面来举个例子。假设知友们和玩家们很给力，前面提到的守望先锋图像标注数据集已经有了足够的数据量了，我们也成功地训练了一个图像标注模型，恩，就叫她&lt;b&gt;ATHENA&lt;/b&gt;吧！Athena是&lt;b&gt;Advanced Tactical Heroine Assistance&lt;/b&gt;的缩写！（劳资真是缩写拼凑小王子！）&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/eaa7e55ded2c3bdc3c9e44fb1f5492ac.jpg" data-rawwidth="699" data-rawheight="293"&gt;&lt;p&gt;我们将上面这张图输入模型，假设模型给出了图像标注句子：&lt;/p&gt;&lt;blockquote&gt;there are two screens on the table.&lt;/blockquote&gt;&lt;p&gt;恩，语句和图像内容相关度还不错，那么我们就计算一下PPL来评价下这个句子。首先我们应该将句子补充一下，&lt;b&gt;添加一个特殊的开始和结束符号&lt;/b&gt;：&lt;/p&gt;&lt;p&gt;&lt;b&gt;&amp;lt;STA&amp;gt;&lt;/b&gt;there are two screens on the table&lt;b&gt;&amp;lt;END&amp;gt;&lt;/b&gt;&lt;/p&gt;&lt;p&gt;我们知道，RNN生成句子的方式是采取一个单词一个单词预测的方式，那么假设咱们的词汇表里面有100个单词吧（挺少的），好了，现在从开始符号开始：&lt;/p&gt;&lt;ol&gt;&lt;li&gt;模型根据图像，从&amp;lt;STA&amp;gt;符号开始，对词汇表中100个单词分别给出了每个单词可能为&amp;lt;STA&amp;gt;下一个单词的可能性，其中由于there单词的可能性为0.6，高于其他单词，所以最终就选择了there；&lt;/li&gt;&lt;li&gt;模型根据图像和&amp;lt;STA&amp;gt;there序列，再次对词汇表中100个单词分别给出了每个单词可能为下一个单词的可能性，其中are的可能性为0.8，高于其他单词，所以选择了are；&lt;/li&gt;&lt;li&gt;以此类推，假设two的可能性为0.3，screens的可能性为0.4，on的可能性为0.3，the的可能性为0.5，table的可能性为0.6；&lt;/li&gt;&lt;li&gt;当table出现后，模型预测下一个为&amp;lt;END&amp;gt;结束符号，句子生成结束。&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;那么根据上面例子，&lt;equation&gt;L=7&lt;/equation&gt;，上面公式的右部就变成了：&lt;/p&gt;&lt;equation&gt;-\frac{1}{L}\sum^L_{n=1}log_2P(w_n|w_{1:n-1},I)=-\frac{1}{7}(log_2(0.6)+log_2(0.8)+log_2(0.3)+log_2(0.4)+log_2(0.3)+log_2(0.5)+log_2(0.6))&lt;/equation&gt;&lt;p&gt;由此可得：&lt;/p&gt;&lt;equation&gt;log_2PPL(w_{1:L}|I)=1.0845&lt;/equation&gt;&lt;p&gt;于是：&lt;/p&gt;&lt;equation&gt;PPL(w_{1:7}|I)=2^{1.0845}=2.12&lt;/equation&gt;&lt;p&gt;于是我们就得到了这个根据图像得出的标注语句的Perplexity值为2.12。那么我们的Athena的perplexity水平如何呢？这里引用一下百度这篇论文的结果表格：&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/6adb35fe9216103573556319ada4e870.png" data-rawwidth="920" data-rawheight="424"&gt;可以看见，m-RNN的PPL（即perplexity的缩写）值是6.92。那么这个分数值是低好一些还是高好一些呢？&lt;/p&gt;&lt;p&gt;我们利用刚才的例子重新算一遍，将每个单词的可能性都降低一些，就会发现perplexity值会升高。这就说明：&lt;b&gt;当模型对于下一个生成单词的确信程度降低时，perplexity值反而升高。我们当然是期望一个模型对于它预测的单词能比较有把握啊，所以perplexity值是越低越好&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;到了这里，我们就可以讨论perplexity到底可以怎么翻译了，其实我个人比较赞同&lt;a href="https://www.zhihu.com/people/4735cce127addcedc38470543c4ff409" data-hash="4735cce127addcedc38470543c4ff409" class="member_mention" data-editable="true" data-title="@硅谷王川" data-hovercard="p$b$4735cce127addcedc38470543c4ff409"&gt;@硅谷王川&lt;/a&gt;在文章中的一句话：&lt;/p&gt;&lt;blockquote&gt;换言之, 聊天机器人使用的语言模型, 如果&lt;b&gt;困惑度&lt;/b&gt;足够低,那么它就能够写出流利通顺和逻辑清晰的语句。借用韩愈老师在&amp;lt;师说&amp;gt;里的话:“&lt;b&gt;机器非生而知之者，孰能无惑？&lt;/b&gt;". 语言模型里进一步解惑的工具,则来自更多的数据和更精巧的算法。&lt;/blockquote&gt;&lt;p&gt;机器非生而知之者，孰能无惑？妙！所以我&lt;b&gt;个人认为将perplexity翻译为困惑度比较好&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;BLEU&lt;/h2&gt;&lt;p&gt;BLEU是&lt;b&gt;B&lt;/b&gt;i&lt;b&gt;l&lt;/b&gt;ingual &lt;b&gt;E&lt;/b&gt;valuation &lt;b&gt;U&lt;/b&gt;nderstudy的缩写。这个计算标准在图像标注结果评价中使用是很广泛的，但是它的设计初衷并不是针对图像标注问题，而是针对机器翻译问题，它是用于分析待评价的翻译语句和参考翻译语句之间n元组的相关性的。直白地来说，它的核心思想就是：&lt;b&gt;机器翻译语句与人类的专业翻译语句越接近就越好&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;一些需要说明的符号含义&lt;/b&gt;：对于图像&lt;equation&gt;I_i&lt;/equation&gt;，模型会生成对应的标注语句&lt;equation&gt;c_i&lt;/equation&gt;，自动评价标准能够根据参考标注语句（也就是人工标注的语句）的一个集合&lt;equation&gt;S_i=\{s_{i1},...,s_{im} \}\in S&lt;/equation&gt;，对待评价的标准语句&lt;equation&gt;c_i&lt;/equation&gt;的质量做出评价。标注语句都是用&lt;b&gt;n元组（n-gram）&lt;/b&gt;来表示的，一个n元组&lt;equation&gt;w_k\in \Omega&lt;/equation&gt;是一个由一个或者多个有顺序单词组成的序列。现在一般只探索n元组从1个单词到4个单词的情况。n元组&lt;equation&gt;w_k&lt;/equation&gt;在语句&lt;equation&gt;s_{ij}&lt;/equation&gt;中出现的次数被记为&lt;equation&gt;h_k(s_{ij})&lt;/equation&gt;，n元组&lt;equation&gt;w_k&lt;/equation&gt;在待评价语句&lt;equation&gt;c_i\in C&lt;/equation&gt;中出现的次数被记为&lt;equation&gt;h_k(c_i)&lt;/equation&gt;。&lt;/p&gt;&lt;p&gt;&lt;b&gt;明确上述符号含义后，BLEU的计算公式如下：&lt;/b&gt;&lt;/p&gt;&lt;p&gt;首先计算的是全局的&lt;b&gt;n元组&lt;/b&gt;精度：其中&lt;equation&gt;k&lt;/equation&gt;指的是长度为n的可能的n元组的集合数。&lt;/p&gt;&lt;equation&gt;CP_n(C,S)=\frac{\sum_i \sum_kmin(h_k(c_i),max_{j\in m}h_k(s_{ij}))}{\sum_i \sum_kh_k(c_i)}&lt;/equation&gt;&lt;p&gt;然后计算的是简洁性惩罚值：其中&lt;equation&gt;l_C&lt;/equation&gt;是待评价语句&lt;equation&gt;c_i&lt;/equation&gt;们的总长，&lt;equation&gt;l_S&lt;/equation&gt;是全局级别的有效参考句子的总长度。如果对于一个待评价语句有多个参考语句，那么就选择让简洁性惩罚最小的那个。&lt;/p&gt;&lt;equation&gt;b(C,S)=\left\{
\begin{array}{rcl}
1           &amp;amp;      &amp;amp; {if \qquad l_C      &amp;gt;      l_S}\\
e^{1-l_S/l_C}       &amp;amp;      &amp;amp; {if \qquad l_C \leq l_S}
\end{array} \right. &lt;/equation&gt;&lt;p&gt;最终计算BLEU分数：其中&lt;equation&gt;N=1,2,3,4&lt;/equation&gt;，对于所有的&lt;equation&gt;n&lt;/equation&gt;，&lt;equation&gt;w_n&lt;/equation&gt;都是常量。&lt;/p&gt;&lt;equation&gt;BLEU_N(C,S)=b(C,S)exp\left(\sum^N_{n=1}w_nlogCP_n(C,S) \right)&lt;/equation&gt;&lt;p&gt;怎么算？同学你看懂了吗？反正我刚开始是没看懂:(，直到我看了&lt;a href="https://en.wikipedia.org/wiki/BLEU" data-editable="true" data-title="维基百科上的例子" class=""&gt;维基百科上的例子&lt;/a&gt;如下：&lt;/p&gt;&lt;p&gt;&lt;b&gt;举例子&lt;/b&gt;：假设我们现在有1个模型生成的待评价句子和2个参考句子如下：&lt;/p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/1bf11b221e188a5f1bbedb83422960d6.png" data-rawwidth="762" data-rawheight="178"&gt;如果我们先来计算1元组the的精度的话，根据精度的公式：&lt;equation&gt;P=\frac{m}{w_t}=\frac{7}{7}=1&lt;/equation&gt;其中，&lt;equation&gt;m&lt;/equation&gt;怎么理解呢？就是如果一个单词是待评价句子中的，同时在在参考句子中也能找到的这个单词，那么这个单词在待评价语句中出现的次数就是&lt;equation&gt;m&lt;/equation&gt;，在现在例子中the在待评价语句中出现了，也在参考语句中出现了，所以符合条件，而在待评价语句中，the出现了7次，所以这里m=7。而&lt;equation&gt;w_t&lt;/equation&gt;是待评价句子总的单词数量，很显然&lt;equation&gt;w_t&lt;/equation&gt;的值也是7。现在就看到这个例子的奇葩之处了，看起来精度很完美，但是实际上翻译效果很差。&lt;b&gt;BLEU就是要解决这种问题&lt;/b&gt;：所以对于待评价句子中的任意一个单词，算法计算其在参考句子中出现的最大次数&lt;equation&gt;max_{j\in m}h_k(s_{ij})&lt;/equation&gt;，比如，the在参考1中出现了2次，在参考2中出现了1次，那么&lt;equation&gt;max_{j\in m}h_k(s_{ij})=2&lt;/equation&gt;。对于待评价句子，其中每个单词的出现次数&lt;equation&gt;h_k(c_i)&lt;/equation&gt;将被记为该单词的最大出现次数，比如对于the，值为7。而又因为&lt;equation&gt;min(h_k(c_i),max_{j\in m}h_k(s_{ij}))&lt;/equation&gt;，要在这两个值之间取最小值，所以值就是2了。于是，1元组the的精度分数&lt;equation&gt;CP_n(C,S)&lt;/equation&gt;就是&lt;b&gt;2/7&lt;/b&gt;了。然而在实际中，使用单个单词来比较并不是最理想的，所以BLEU使用n元组来计算，n值最高为4。1元组分数对于评价翻译并不足够。更长的元组得分对应的是语言的流畅性。接下来计算简洁性惩罚，&lt;b&gt;为啥还要引入一个简洁性惩罚呢&lt;/b&gt;？这是因为BLEU倾向于更短的句子，这样精度分数就会很高。为了解决这个问题，使用了乘以一个简洁性惩罚来防止很短的句子获得很高的分数。令&lt;equation&gt;l_S&lt;/equation&gt;为参考句子的总长度，&lt;equation&gt;l_C&lt;/equation&gt;是待评价句子的总长度，如果&lt;equation&gt;l_C&lt;/equation&gt;小于等于&lt;equation&gt;l_S&lt;/equation&gt;，那么惩罚生效，计算&lt;equation&gt;e^{1-l_S/l_C}&lt;/equation&gt;。反之，简洁性惩罚值为1。&lt;b&gt;如果有多个参考句子，那么就选取长度和待评价句子长度最接近的那个参考句子的长度&lt;/b&gt;。最后一步的计算相对比较清晰，就不过多解释了。&lt;p&gt;&lt;b&gt;一句话&lt;/b&gt;：&lt;b&gt;BLEU得分越高越好。&lt;/b&gt;&lt;/p&gt;&lt;h2&gt;ROUGE&lt;/h2&gt;&lt;p&gt;ROUGE是一个设计&lt;b&gt;用来评价文本摘要算法&lt;/b&gt;的自动评价标准集，其中有3个评价标准，分别是&lt;b&gt;ROUGE-N，ROUGE-L和ROUGE-S&lt;/b&gt;。下面逐个进行介绍：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;ROUGE_N&lt;/equation&gt;是第一个ROUGE标准。根据给出的待评价句子，它对所有的参考摘要计算一个简单的n元组召回：&lt;/p&gt;&lt;equation&gt;ROUGE_n(c_i,S_i)=\frac{\sum_j \sum_kmin(h_k(c_i),h_k(s_{ij}))}{\sum_i \sum_kh_k(s_{ij})}&lt;/equation&gt;&lt;p&gt;回顾一下：n元组&lt;equation&gt;w_k&lt;/equation&gt;在语句&lt;equation&gt;s_{ij}&lt;/equation&gt;中出现的次数被记为&lt;equation&gt;h_k(s_{ij})&lt;/equation&gt;，n元组&lt;equation&gt;w_k&lt;/equation&gt;在待评价语句&lt;equation&gt;c_i\in C&lt;/equation&gt;中出现的次数被记为&lt;equation&gt;h_k(c_i)&lt;/equation&gt;。所以上面&lt;equation&gt;ROUGE_N&lt;/equation&gt;的计算还是挺容易理解的。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;ROUGE_L&lt;/equation&gt;是基于longest common subsequence（LCS）的一种测量方法。所谓LCS，就是一个同时出现在两个句子中的单词集合，且单词出现的顺序也是相同的。和n元组不同的是，在单词之间可能还存在能够创建出LCS的单词。将比较的两个句子间的LCS的长度记为：&lt;equation&gt;l(c_i,s_{ij})&lt;/equation&gt;。ROUGE-L通过计算F-meansure（&lt;a href="https://en.wikipedia.org/wiki/F1_score" data-editable="true" data-title="F1 score"&gt;F1 score&lt;/a&gt;）来求得：&lt;/p&gt;&lt;equation&gt;R_l=max_j\frac{l(c_i,s_{ij})}{|s_{ij}|}&lt;/equation&gt;&lt;equation&gt;P_l=max_j\frac{l(c_i,s_{ij})}{|c_i|}&lt;/equation&gt;&lt;equation&gt;ROUGE_L(c_i,S_i)=\frac{(1+\beta^2)R_lP_l}{R_l+\beta^2P_l}&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;R_l&lt;/equation&gt;是召回，&lt;equation&gt;P_l&lt;/equation&gt;是精度，&lt;equation&gt;\beta&lt;/equation&gt;一般等于1.2，在这个计算中不需要管n元组。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;ROUGE_S&lt;/equation&gt;是最后一个标准，没有使用LCS或n元组，使用的是&lt;b&gt;跳跃二元组（skip bigram）&lt;/b&gt;。跳跃二元组是句子中有序的单词对，和LCS类似，在单词对之间，单词可能被跳过。比如一句有4个单词的句子，按照排列组合就可能有6种跳跃二元组。再次使用精度和召回率来计算F，将句子&lt;equation&gt;s_{ij}&lt;/equation&gt;中跳跃二元组的个数记为&lt;equation&gt;f_k(s_{ij})&lt;/equation&gt;，则计算公式如下：&lt;/p&gt;&lt;equation&gt;R_s=max_j\frac{\sum_kmin(f_k(c_i),f_k(s_{ij}))}{\sum_kf_k(s_{ij})}&lt;/equation&gt;&lt;equation&gt;P_s=max_j\frac{\sum_kmin(f_k(c_i),f_k(s_{ij}))}{\sum_kf_k(c_i)}&lt;/equation&gt;&lt;p&gt;&lt;equation&gt;GOUGE_S(c_i,S_i)=\frac{(1+\beta^2)R_sP_s}{R_s+\beta^2P_s}&lt;/equation&gt;跳跃二元组能够捕获到长距离的句子结构。在实践中，跳跃二元组计算的时候单词间最长距离为4。ROUGE-SU是在跳跃二元组基础上增加使用了1元组。&lt;/p&gt;&lt;p&gt;&lt;b&gt;一句话：ROUGE得分越高越好&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;METEOR&lt;/h2&gt;&lt;p&gt;&lt;b&gt;METEOR是用来评价机器翻译输出的标准&lt;/b&gt;。该方法基于一元组的精度和召回的调和平均（&lt;a href="https://en.wikipedia.org/wiki/Harmonic_mean" data-editable="true" data-title="Harmonic mean"&gt;Harmonic mean&lt;/a&gt;），召回的权重比精度要高一点。这个标准还有一些其他标准没有的特性，设计它是为了解决BLEU存在的一些问题。它与人类判断相关性高，而且和BLEU不同，它不仅在整个集合，而且在句子和分段级别，也能和人类判断的相关性高。在全集级别，它的相关性是0.964，BLEU是0.817。在句子级别，它的相关性最高到了0.403。&lt;/p&gt;&lt;p&gt;&lt;b&gt;METEOR的计算公式&lt;/b&gt;：其中m是&lt;b&gt;平面图（alignments）&lt;/b&gt;的集合，ch是&lt;b&gt;块（chunk）&lt;/b&gt;的数量，&lt;equation&gt;P_m&lt;/equation&gt;是精度，&lt;equation&gt;R_m&lt;/equation&gt;是召回率。&lt;/p&gt;&lt;p&gt;&lt;equation&gt;Pen=\gamma\left(\frac{ch}{m}  \right)^{\theta}&lt;/equation&gt;&lt;equation&gt;F_{mean}=\frac{P_mR_m}{\alpha P_m+(1-\alpha)R_m}&lt;/equation&gt;&lt;equation&gt;P_m=\frac{|m|}{\sum_k h_k(c_i)}&lt;/equation&gt;&lt;equation&gt;R_m=\frac{|m|}{\sum_k h_k(s_{ij})}&lt;/equation&gt;&lt;equation&gt;METEOR=(1-Pen)F_{mean}&lt;/equation&gt;&lt;b&gt;理解&lt;/b&gt;：看公式总是挺抽象的，下面我们还是看看来自维基百科的例子吧。计算的最基本单元是句子。算法首先从待评价字符串和参考字符串之间创建一个平面图如下：&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/96242ff6cfe2cbb92b23f755a78c28f7.png" data-rawwidth="1096" data-rawheight="174"&gt;所谓平面图，就是1元组之间的映射集。平面图有如下的一些限制：在待评价翻译中的每个1元组必须映射到参考翻译中的1个或0个一元组，然后根据这个定义创建平面图。&lt;b&gt;如果有两个平面图的映射数量相同，那么选择映射交叉数目较少的那个&lt;/b&gt;。也就是说，上面左侧平面图会被选择。状态会持续运行，在每个状态下只会向平面图加入那些在前一个状态中尚未匹配的1元组。一旦最终的平面图计算完毕，就开始计算METEOR得分：&lt;/p&gt;&lt;p&gt;1元组精度：&lt;equation&gt;P=\frac{m}{w_t}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;其中m是在参考句子中同样存在的，待评价句子中的一元组的数量。&lt;equation&gt;w_t&lt;/equation&gt;是待评价翻译中一元组的数量。&lt;/p&gt;&lt;p&gt;1元组召回率：&lt;equation&gt;R=\frac{m}{w_r}&lt;/equation&gt;&lt;/p&gt;&lt;p&gt;m同上，&lt;equation&gt;w_r&lt;/equation&gt;是参考翻译中一元组的数量。然后使用调和平均来计算F-mean，且召回的权重是精度的9倍。&lt;equation&gt;F_{mean}=\frac{10PR}{R+9P}&lt;/equation&gt;到目前为止，这个方法只对单个单词的一致性进行了衡量，却没有对参考翻译和待评价翻译中更大的分段进行衡量。为了将其计算在内，使用更长的n元组来计算对于平面图的惩罚p。在参考和待评价句子中的没有毗连的映射越多，惩罚就越高。为了计算惩罚，1元组被分组成最少可能的&lt;b&gt;块（chunks）&lt;/b&gt;。块的定义是在待评价语句和参考语句中毗邻的一元组集合。在待评价语句和参考语句之间的毗邻映射越长，块的数量就越少。一个待评价翻译如果和参考翻译相同，那么就只有一个块。惩罚p的计算如下：&lt;/p&gt;&lt;p&gt;&lt;equation&gt;p=0.5\left(\frac{c}{u_m} \right)^3&lt;/equation&gt;其中c就是块的数量，&lt;equation&gt;u_m&lt;/equation&gt;是被映射的一元组的数量。p可以减少F-mean的值。最后&lt;equation&gt;M=(1-p)F_{mean}&lt;/equation&gt;。&lt;b&gt;计算例子&lt;/b&gt;：这里偷个懒，直接截图维基百科，可以结合这个例子对照看自己的计算是否正确。&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/0c33f88db9521eab9c82afe1cf794ea3.png" data-rawwidth="1736" data-rawheight="1162"&gt;&lt;/p&gt;&lt;p&gt;&lt;b&gt;一句话：METEOR得分越高越好&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;CIDEr&lt;/h2&gt;CIDEr是专门设计出来用于图像标注问题的，它是通过对每个n元组进行&lt;b&gt;Term Frequency Inverse Document Frequency (TF-IDF) &lt;/b&gt;权重计算，来衡量图像标注的一致性的。一个n元组wk在出现在参考句子sij中的次数被记为&lt;equation&gt;h_k(s_{ij})&lt;/equation&gt; ，如果出现在待评价句子中，则被记为&lt;equation&gt;h_k(c_i)&lt;/equation&gt;。CIDEr为每个n元组wk都计算TF-IDF权重&lt;equation&gt;g_k(s_{ij})&lt;/equation&gt;：&lt;equation&gt;g_k(s_{ij})=\frac{h_k(s_{ij})}{\sum_{w_l\in \Omega}}log \left(\frac{|I|}{\sum_{I_p \in I}min(1,\sum_qh_k(s_{pq}))} \right)&lt;/equation&gt;其中&lt;equation&gt;\Omega&lt;/equation&gt;是所有n元组的词汇表，I是数据集中所有图像的集合。公式第一个部分计算的是每个n元组wk的TF，公式第二部分是使用IDF来计算&lt;equation&gt;w_k&lt;/equation&gt;的稀有程度。从直观上来说，如果一些n元组频繁地出现在描述图像的参考标注中，TF对于这些n元组将给出更高的权重，而IDF则降低那些在所有描述语句中都常常出现的n元组的权重。也就是说，IDF提供了一种测量单词显著性的方法，这就是将那些容易常常出现，但是对于视觉内容信息没有多大帮助的单词的重要性打折。IDF的计算方法是：分子为数据集中图像的数量I，分母为一些图像的数量，这些图像是其任意一个描述中出现了n元组&lt;equation&gt;w_k&lt;/equation&gt;的图像，然后再对着个分数求对数。对于长度为n的n元组的CIDEr-n分数是使用待评价句子和参考句子之间的平均相似性来计算的，其中精度和召回率都要占比例：&lt;equation&gt;CIDEr_n(c_i,S_i)=\frac{1}{m}\sum_j\frac{g^n(c_i)\cdot g^n(s_{ij})}{||g^n(c_i)|||| g^n(s_{ij})||}&lt;/equation&gt;其中，&lt;equation&gt;g^n(c_i)&lt;/equation&gt;是一个由&lt;equation&gt;g_k(c_i)&lt;/equation&gt;生成的向量，对应的是所有长度为n的n元组。&lt;equation&gt;||g^n(c_i)||&lt;/equation&gt;是向量的大小。而&lt;equation&gt;g^n(s_{ij})&lt;/equation&gt;的情况类似。更长的n元组是用来获取语法性质和更丰富的语义信息的。不同长度的n元组的得分计算如下：&lt;equation&gt;CIDEr(c_i,S_i)=\sum^N_{n=1}w_nCIDEr_n(c_i,S_i)&lt;/equation&gt;&lt;p&gt;标准权重&lt;equation&gt;w_n=1/N&lt;/equation&gt;，N=4比较常用。&lt;/p&gt;&lt;p&gt;&lt;b&gt;CIDEr-D&lt;/b&gt;是修改版本，为的是让CIDEr对于gaming问题更加鲁棒。什么是Gaming问题？它是一种现象，就是一个句子经过人工判断得分很低，但是在自动计算标准中却得分很高的情况。为了避免这种情况，CIDEr-D增加了&lt;b&gt;截断（clipping）和基于长度的高斯惩罚&lt;/b&gt;：&lt;/p&gt;&lt;equation&gt;CIDEr\text{-}D_n(c_i,S_i)=\frac{10}{m}\sum_j e^{\frac{-(l(c_i)-l(s_{ij}))^2}{2\sigma^2}} \frac{min(g^n(c_i),g^n(s_{ij}))\cdot g^n(s_{ij})}{||g^n(c_i)|||| g^n(s_{ij})||}&lt;/equation&gt;其中，&lt;equation&gt;l(c_i)&lt;/equation&gt;和&lt;equation&gt;l(s_{ij})&lt;/equation&gt;分别表示的是待评价句子和参考句子的长度，&lt;equation&gt;\sigma&lt;/equation&gt;=6，分子为10是为了让得分和其他标准比较相似。最终：&lt;equation&gt;CIDEr\text{-}D(c_i,S_i)=\sum^N_{n=1}w_nCIDEr\text{-}D_n(c_i,S_i)&lt;/equation&gt;&lt;b&gt;一句话：CIDEr得分越高越好&lt;/b&gt;。&lt;h2&gt;下篇预告&lt;/h2&gt;&lt;p&gt;由于文章篇幅较长，顾分为上下篇发布。在上篇中，&lt;b&gt;介绍了图像标准问题，以及研究问题的数据集，人工和自动评价标准，并脑洞了开源创建守望先锋图像标注数据集的想法&lt;/b&gt;。&lt;/p&gt;&lt;p&gt;在下篇中，将&lt;b&gt;介绍比较几个有代表性的图像标注方法，一些代码实践，和对于图像标准问题的思考&lt;/b&gt;。&lt;/p&gt;&lt;h2&gt;作者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;欢迎大家对文中的不当之处留言批评指正，共同学习提高；&lt;/li&gt;&lt;li&gt;由于希望写得有趣和通俗些，所以解释和比喻较多，行文稍显冗余，不知阅读效果如何？请批评；&lt;/li&gt;&lt;li&gt;真心想做一个基于守望先锋游戏画面的图像标注数据集，为什么特别指明用Ruby呢？因为朋友天天安利我说Ruby on rails做网站好，我对网站这一块不太懂，就信任小伙伴咯，不希望因为这个问题引战:)&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22408033&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Tue, 20 Sep 2016 12:04:02 GMT</pubDate></item><item><title>[原创翻译]循环神经网络惊人的有效性（下）</title><link>https://zhuanlan.zhihu.com/p/22230074</link><description>&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/2029a1ea020883b4d065592d14acc96e_r.jpg"&gt;&lt;/p&gt;版权声明：本文&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元"&gt;智能单元&lt;/a&gt;首发，本人原创翻译，禁止未授权转载。 &lt;blockquote&gt;译者注：在CS231n课程笔记止步于CNN，没有循环神经网络（RNN和LSTM），实为憾事。经知友推荐，将&lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" data-title="The Unreasonable Effectiveness of Recurrent Neural Networks" class="" data-editable="true"&gt;The Unreasonable Effectiveness of Recurrent Neural Networks&lt;/a&gt;一文翻译完毕，作为补充。感谢@&lt;a href="https://www.zhihu.com/people/hmonkey" class="" data-editable="true" data-title="猴子"&gt;猴子&lt;/a&gt;，&lt;a href="https://www.zhihu.com/people/e7fcc05b0cf8a90a3e676d0206f888c9" data-hash="e7fcc05b0cf8a90a3e676d0206f888c9" class="member_mention" data-hovercard="p$b$e7fcc05b0cf8a90a3e676d0206f888c9"&gt;@堃堃&lt;/a&gt;和&lt;a href="https://www.zhihu.com/people/f11e78650e8185db2b013af42fd9a481" data-hash="f11e78650e8185db2b013af42fd9a481" class="member_mention" data-hovercard="p$b$f11e78650e8185db2b013af42fd9a481"&gt;@李艺颖&lt;/a&gt;的校对。&lt;/blockquote&gt;&lt;h2&gt;目录&lt;/h2&gt;&lt;ul&gt;&lt;li&gt;循环神经网络&lt;/li&gt;&lt;li&gt;字母级别的语言模型&lt;/li&gt;&lt;li&gt;RNN的乐趣&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Paul Graham生成器&lt;/li&gt;&lt;li&gt;莎士比亚&lt;/li&gt;&lt;li&gt;维基百科&lt;/li&gt;&lt;li&gt;几何代数&lt;/li&gt;&lt;li&gt;Linux源码&lt;/li&gt;&lt;li&gt;生成婴儿姓名&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;理解训练过程 &lt;i&gt;&lt;b&gt;译者注：下篇起始处&lt;/b&gt;&lt;/i&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;训练时输出文本的进化&lt;/li&gt;&lt;li&gt;RNN中的预测与神经元激活可视化&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;源代码&lt;/li&gt;&lt;li&gt;拓展阅读&lt;/li&gt;&lt;li&gt;结论&lt;/li&gt;&lt;li&gt;译者反馈&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;理解训练过程&lt;/h2&gt;&lt;p&gt;我们已经看见训练结束后的结果令人印象深刻，但是它到底是如何运作的呢？现在跑两个小实验来一探究竟。&lt;/p&gt;&lt;h3&gt;训练时输出文本的进化&lt;/h3&gt;&lt;p&gt;首先，观察模型在训练时输出文本的不断进化是很有意思的。例如，我使用托尔斯泰的《战争与和平》来训练LSTM，并在训练过程中每迭代100次就输出一段文本。在第100次迭代时，模型输出的文本是随机排列的：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;tyntd-iafhatawiaoihrdemot  lytdws  e ,tfti, astai f ogoh eoase rrranbyne 'nhthnee e 
plia tklrgd t o idoe ns,smtt   h ne etie h,hregtrs nigtike,aoaenns lng
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;但是至少可以看到它学会了单词是被空格所分割的，只是有时候它使用了两个连续空格。它还没学到逗号后面总是有个空格。在迭代到第300次的时候，可以看到模型学会使用引号和句号。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;"Tmont thithey" fomesscerliund
Keushey. Thom here
sheulke, anmerenith ol sivh I lalterthend Bleipile shuwy fil on aseterlome
coaniogennc Phe lism thond hon at. MeiDimorotion in ther thize."
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;单词被空格所分割，模型开始知道在句子末尾使用句号。在第500次迭代时：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;we counter. He stutn co des. His stanted out one ofler that concossions and was 
to gearang reay Jotrets and with fre colt otf paitt thin wall. Which das stimn 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;模型开始学会使用最短和最常用的单词，比如“we”、“He”、“His”、“Which”、“and”等。从第700次迭代开始，可以看见更多和英语单词形似的文本：&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;Aftair fall unsuch that the hall for Prince Velzonski's that me of
her hearly, and behs to so arwage fiving were to it beloge, pavu say falling misfort 
how, and Gogition is so overelical and ofter.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在第1200次迭代，我们可以看见使用引号、问好和感叹号，更长的单词也出现了。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;"Kite vouch!" he repeated by her
door. "But I would be done and quarts, feeling, then, son is people...."
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;在迭代到2000次的时候，模型开始正确的拼写单词，引用句子和人名。&lt;/p&gt;&lt;pre&gt;&lt;code lang="text"&gt;"Why do what that day," replied Natasha, and wishing to himself the fact the
princess, Princess Mary was easier, fed in had oftened him.
Pierre aking his soul came to the packs and drove up his father-in-law women.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;从上述结果中可见，模型首先发现的是一般的单词加空格结构，然后开始学习单词；从短单词开始，然后学习更长的单词。由多个单词组成的话题和主题词要到训练后期才会出现。&lt;/p&gt;&lt;h2&gt;RNN中的预测与神经元激活可视化&lt;/h2&gt;&lt;p&gt;另一个有趣的实验内容就是将模型对于字符的预测可视化。下面的图示是我们对用维基百科内容训练的RNN模型输入验证集数据（蓝色和绿色的行）。在每个字母下面我们列举了模型预测的概率最高的5个字母，并用深浅不同的红色着色。深红代表模型认为概率很高，白色代表模型认为概率较低。注意有时候模型对于预测的字母是非常有信心的。比如在&lt;a href="http://www/" data-editable="true" data-title="www 的页面"&gt;http://www&lt;/a&gt;. 序列中就是。&lt;/p&gt;&lt;p&gt;输入字母序列也被着以蓝色或者绿色，这代表的是RNN隐层表达中的某个随机挑选的神经元是否被&lt;em&gt;激活&lt;/em&gt;。绿色代表非常兴奋，蓝色代表不怎么兴奋。LSTM中细节也与此类似，隐藏状态向量中的值是[-1, 1]，这就是经过各种操作并使用tanh计算后的LSTM细胞状态。直观地说，这就是当RNN阅读输入序列时，它的“大脑”中的某些神经元的激活率。不同的神经元关注的是不同的模式。在下面我们会看到4种不同的神经元，我认为比较有趣和能够直观理解（当然也有很多不能直观理解）。&lt;/p&gt;&lt;p&gt;————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/2ae42139518681b8cbbe4f854fdef515.jpg" data-rawwidth="1639" data-rawheight="825"&gt;本图中高亮的神经元看起来对于URL的开始与结束非常敏感。LSTM看起来是用这个神经元来记忆自己是不是在一个URL中。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/63ac60c5e0fc60017e45360516c95682.jpg" data-rawwidth="1688" data-rawheight="827"&gt;高亮的神经元看起来对于markdown符号[[]]的开始与结束非常敏感。有趣的是，一个[符号不足以激活神经元，必须等到两个[[同时出现。而判断有几个[的任务看起来是由另一个神经元完成的。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic4.zhimg.com/c41f6b3d378e74b45698950a95b32d29.jpg" data-rawwidth="1685" data-rawheight="398"&gt;这是一个在[[]]中线性变化的神经元。换句话说，在[[]]中，它的激活是为RNN提供了一个以时间为准的坐标系。RNN可以使用该信息来根据字符在[[]]中出现的早晚来决定其出现的频率（也许？）。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic1.zhimg.com/27c8e4245a2e3726f6faa3af58c15603.jpg" data-rawwidth="1668" data-rawheight="185"&gt;这是一个进行局部动作的神经元：它大部分时候都很安静，直到出现www序列中的第一个w后，就突然关闭了。RNN可能是使用这个神经元来计算www序列有多长，这样它就知道是该输出有一个w呢，还是开始输出URL了。&lt;/p&gt;&lt;p&gt;——————————————————————————————————————————&lt;/p&gt;&lt;p&gt;当然，由于RNN的隐藏状态是一个巨大且分散的高维度表达，所以上面这些结论多少有一点手动调整。上面的这些可视化图片是用定制的HTML/CSS/Javascript实现的，如果你想实现类似的，可以查看&lt;a href="http://cs.stanford.edu/people/karpathy/viscode.zip" data-editable="true" data-title="这里"&gt;这里&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;我们可以进一步简化可视化效果：不显示预测字符仅仅显示文本，文本的着色代表神经元的激活情况。可以看到大部分的细胞做的事情不是那么直观能理解，但是其中5%看起来是学到了一些有趣并且能理解的算法：&lt;/p&gt;&lt;p&gt;—————————————————————————————————————————&lt;/p&gt;&lt;p&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic2.zhimg.com/d86db8d5322c96a366e59572e01a5685.png" data-rawwidth="990" data-rawheight="865"&gt;&lt;img rel="noreferrer" src="https://images.weserv.nl/?url=ssl:pic3.zhimg.com/25eea1bd327845168fab51e66c78d77d.png" data-rawwidth="766" data-rawheight="865"&gt;—————————————————————————————————————————&lt;/p&gt;&lt;p&gt;在预测下个字符的过程中优雅的一点是：我们不用进行任何的硬编码。比如，不用去实现判断我们到底是不是在一个引号之中。我们只是使用原始数据训练LSTM，然后它自己决定这是个有用的东西于是开始跟踪。换句话说，其中一个单元自己在训练中变成了引号探测单元，只因为这样有助于完成最终任务。这也是深度学习模型（更一般化地说是端到端训练）强大能力的一个简洁有力的证据。&lt;/p&gt;&lt;h2&gt;源代码&lt;/h2&gt;&lt;p&gt;我想这篇博文能够让你认为训练一个字符级别的语言模型是一件有趣的事儿。你可以使用我在Github上的&lt;a href="https://github.com/karpathy/char-rnn" data-editable="true" data-title="char rnn代码"&gt;char rnn代码&lt;/a&gt;训练一个自己的模型。它使用一个大文本文件训练一个字符级别的模型，可以输出文本。如果你有GPU，那么会在比CPU上训练快10倍。如果你训练结束得到了有意思的结果，请联系我。如果你看Torch/Lua代码看的头疼，别忘了它们只不过是这个&lt;a href="https://gist.github.com/karpathy/d4dee566867f8291f086" data-editable="true" data-title="100行项目"&gt;100行项目&lt;/a&gt;的高端版。&lt;/p&gt;&lt;p&gt;&lt;em&gt;题外话&lt;/em&gt;。代码是用&lt;a href="http://torch.ch/" data-editable="true" data-title="Torch7"&gt;Torch7&lt;/a&gt;写的，它最近变成我最爱的深度学习框架了。我开始学习Torch/LUA有几个月了，这并不简单（花了很多时间学习Github上的原始Torch代码，向项目创建者提问来解决问题），但是一旦你搞懂了，它就会给你带来很大的弹性和加速。之前我使用的是Caffe和Theano，虽然Torch虽然还不完美，但是我相信它的抽象和哲学层次比前两个高。在我看来，一个高效的框架应有以下特性：&lt;/p&gt;&lt;ul&gt;&lt;li&gt;有丰富函数（例如切片，数组/矩阵操作等）的，对底层CPU/GPU透明的张量库。&lt;/li&gt;&lt;li&gt;一整个基于脚本语言（比如Python）的分离的代码库，能够对张量进行操作，实现所有深度学习内容（前向、反向传播，计算图等）。&lt;/li&gt;&lt;li&gt;分享预训练模型非常容易（Caffe做得很好，其他的不行）。&lt;/li&gt;&lt;li&gt;最关键的：没有编译过程！或者至少不要像Theano现在这样！深度学习的趋势是更大更复杂的网络，这些网络都有随着时间展开的复杂计算流程。编译时间不能太长，不然开发过程将充满痛苦。其次，编译导致开发者放弃解释能力，不能高效地进行调试。如果在流程开发完成后有个&lt;em&gt;选项&lt;/em&gt;能进行编译，那也可以。&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;拓展阅读&lt;/h2&gt;&lt;p&gt;在结束本篇博文前，我想把RNN放到更广的背景中，提供一些当前的研究方向。RNN现在在深度学习领域引起了不小的兴奋。和卷积神经网络一样，它出现已经有十多年了，但是直到最近它的潜力才被逐渐发掘出来，这是因为我们的计算能力日益强大。下面是当前的一些进展（肯定不完整，而且很多工作可以追溯的1990年）：&lt;/p&gt;&lt;p&gt;在NLP/语音领域，RNN将&lt;a href="http://www.jmlr.org/proceedings/papers/v32/graves14.pdf" data-editable="true" data-title="语音转化为文字"&gt;语音转化为文字&lt;/a&gt;，进行&lt;a href="http://arxiv.org/abs/1409.3215" data-editable="true" data-title="机器翻译"&gt;机器翻译&lt;/a&gt;，生成&lt;a href="http://www.cs.toronto.edu/~graves/handwriting.html" data-editable="true" data-title="手写文本"&gt;手写文本&lt;/a&gt;，当然也是强大的语言模型 (Sutskever等) (Graves) (Mikolov等)。字符级别和单词级别的模型都有，目前看来是单词级别的模型更领先，但是这只是暂时的。&lt;/p&gt;&lt;p&gt;计算机视觉。RNN迅速地在计算机视觉领域中被广泛运用。比如，使用RNN用于&lt;a href="http://arxiv.org/abs/1411.4389" data-editable="true" data-title="视频分类"&gt;视频分类&lt;/a&gt;，&lt;a href="http://arxiv.org/abs/1411.4555" data-editable="true" data-title="图像标注"&gt;图像标注&lt;/a&gt;（其中有我自己的工作和其他一些），&lt;a href="http://arxiv.org/abs/1505.00487" data-editable="true" data-title="视频标注"&gt;视频标注&lt;/a&gt;和最近的&lt;a href="http://arxiv.org/abs/1505.02074" data-editable="true" data-title="视觉问答"&gt;视觉问答&lt;/a&gt;。在计算机视觉领域，我个人最喜欢的RNN论文是《&lt;a href="http://arxiv.org/abs/1406.6247" data-editable="true" data-title="Recurrent Models of Visual Attention"&gt;Recurrent Models of Visual Attention&lt;/a&gt;》，之所以推荐它，是因为它高层上的指导方向和底层的建模方法（对图像短时间观察后的序列化处理），和建模难度低（REINFORCE算法规则是增强学习里面策略梯度方法中的一个特例，使得能够用非微分的计算来训练模型（在该文中是对图像四周进行快速查看））。我相信这种用CNN做原始数据感知，RNN在顶层做快速观察策略的混合模型将会在感知领域变得越来越流行，尤其是在那些不单单是对物体简单分类的复杂任务中将更加广泛运用。&lt;/p&gt;&lt;p&gt;归纳推理，记忆和注意力（Inductive Reasoning, Memories and Attention）。另一个令人激动的研究方向是要解决普通循环网络自身的局限。RNN的一个问题是它不具有归纳性：它能够很好地记忆序列，但是从其表现上来看，它不能很好地在正确的方向上对其进行归纳（一会儿会举例让这个更加具体一些）。另一个问题是RNN在运算的每一步都将表达数据的尺寸和计算量联系起来，而这并非必要。比如，假设将隐藏状态向量尺寸扩大为2倍，那么由于矩阵乘法操作，在每一步的浮点运算量就要变成4倍。理想状态下，我们希望保持大量的表达和记忆（比如存储全部维基百科或者很多中间变量），但同时每一步的运算量不变。&lt;/p&gt;&lt;p&gt;在该方向上第一个具有说服力的例子来自于DeepMind的&lt;a href="http://arxiv.org/abs/1410.5401" data-editable="true" data-title="神经图灵机（Neural Turing Machines）"&gt;神经图灵机（Neural Turing Machines）&lt;/a&gt;论文。该论文展示了一条路径：模型可以在巨大的外部存储数组和较小的存储寄存器集（将其看做工作的存储器）之间进行读写操作，而运算是在存储寄存器集中进行。更关键的一点是，神经图灵机论文提出了一个非常有意思的存储解决机制，该机制是通过一个（soft和全部可微分的）注意力模型来实现的。&lt;em&gt;译者注：这里的soft取自softmax&lt;/em&gt;。基于概率的“软”注意力机制（soft attention）是一个强有力的建模特性，已经在面向机器翻译的《&lt;a href="http://arxiv.org/abs/1409.0473" data-editable="true" data-title="Neural Machine Translation by Jointly Learning to Align and Translate"&gt; Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;》一文和面向问答的《&lt;a href="http://arxiv.org/abs/1503.08895" data-editable="true" data-title="Memory Networks"&gt;Memory Networks&lt;/a&gt;》中得以应用。实际上，我想说的是：&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;注意力概念是近期神经网络领域中最有意思的创新。&lt;/p&gt;&lt;/blockquote&gt;&lt;p&gt;现在我不想更多地介绍细节，但是软注意力机制存储器寻址是非常方便的，因为它让模型是完全可微的。不好的一点就是牺牲了效率，因为每一个可以关注的地方都被关注了（虽然是“软”式的）。想象一个C指针并不指向一个特定的地址，而是对内存中所有的地址定义一个分布，然后间接引用指针，返回一个与指向内容的权重和（这将非常耗费计算资源）。这让很多研究者都从软注意力模式转向硬注意力模式，而硬注意力模式是指对某一个区域内的内容固定关注（比如，对某些单元进行读写操作而不是所有单元进行读写操作）。这个模型从设计哲学上来说肯定更有吸引力，可扩展且高效，但不幸的是模型就不是可微分的了。这就导致了对于增强学习领域技术的引入（比如REINFORCE算法），因为增强学习领域中的研究者们非常熟悉不可微交互的概念。这项工作现在还在进展中，但是硬注意力模型已经被发展出来了，在《&lt;a href="http://arxiv.org/abs/1503.01007" data-editable="true" data-title="Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"&gt; Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets&lt;/a&gt;》，《&lt;a href="http://arxiv.org/abs/1505.00521" data-editable="true" data-title="Reinforcement Learning Neural Turing Machines"&gt; Reinforcement Learning Neural Turing Machines&lt;/a&gt;》，《&lt;a href="http://arxiv.org/abs/1502.03044" data-editable="true" data-title="Show Attend and Tell"&gt;Show Attend and Tell&lt;/a&gt;》三篇文章中均有介绍。&lt;/p&gt;&lt;p&gt;研究者。如果你想在RNN方面继续研究，我推荐&lt;a href="http://www.cs.toronto.edu/~graves/" data-editable="true" data-title="Alex Graves"&gt;Alex Graves&lt;/a&gt;，&lt;a href="http://www.cs.toronto.edu/~ilya/" data-editable="true" data-title="Ilya Sutskever"&gt;Ilya Sutskever&lt;/a&gt;和&lt;a href="http://www.rnnlm.org/" data-editable="true" data-title="Tomas Mikolov"&gt;Tomas Mikolov&lt;/a&gt;三位研究者。想要知道更多增强学习和策略梯度方法（REINFORCE算法是其中一个特例），可以学习&lt;a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html" data-editable="true" data-title="David Silver的课程"&gt;David Silver的课程&lt;/a&gt;，或&lt;a href="http://www.cs.berkeley.edu/~pabbeel/" data-editable="true" data-title="Pieter Abbeel的课程"&gt;Pieter Abbeel的课程&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;代码。如果你想要继续训练RNN，我听说Theano上的&lt;a href="https://github.com/fchollet/keras" data-editable="true" data-title="keras"&gt;keras&lt;/a&gt;或&lt;a href="https://github.com/IndicoDataSolutions/Passage" data-editable="true" data-title="passage"&gt;passage&lt;/a&gt;还不错。我使用Torch写了一个&lt;a href="https://github.com/karpathy/char-rnn" data-editable="true" data-title="项目"&gt;项目&lt;/a&gt;，也用numpy实现了一个可以前向和后向传播的LSTM。你还可以在Github上看看我的&lt;a href="https://github.com/karpathy/neuraltalk" data-editable="true" data-title="NeuralTalk" class=""&gt;NeuralTalk&lt;/a&gt;项目，是用RNN/LSTM来进行图像标注。或者看看Jeff Donahue用&lt;a href="http://jeffdonahue.com/lrcn/" data-editable="true" data-title="Caffe"&gt;Caffe&lt;/a&gt;实现的项目。&lt;/p&gt;&lt;h2&gt;结论&lt;/h2&gt;&lt;p&gt;我们已经学习了RNN，知道了它如何工作，以及为什么它如此重要。我们还利用不同的数据集将RNN训练成字母级别的语言模型，观察了它是如何进行这个过程的。可以预见，在未来将会出现对RNN的巨大创新，我个人认为它们将成为智能系统的关键组成部分。&lt;/p&gt;&lt;p&gt;最后，为了给文章增添一点格调，我使用本篇博文对RNN进行了训练。然而由于博文的长度很短，不足以很好地训练RNN。但是返回的一段文本如下（使用低的温度设置来返回更典型的样本）：&lt;/p&gt;&lt;pre&gt;&lt;code lang="python"&gt;I've the RNN with and works, but the computed with program of the 
RNN with and the computed of the RNN with with and the code
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;是的，这篇博文就是讲RNN和它如何工作的，所以显然模型是有用的：）下次见！&lt;/p&gt;&lt;h2&gt;译者反馈&lt;/h2&gt;&lt;ol&gt;&lt;li&gt;翻译不到位的地方，欢迎知友们评论批评指正；&lt;/li&gt;&lt;li&gt;关于Torch和TensorFlow，AK本人现在在OpenAI工作主要是在用TF了，但是他对于Torch还是有很强的倾向性。这在他最新的博文中可以看到；&lt;/li&gt;&lt;li&gt;在计算机视觉方面，个人对于&lt;b&gt;图像标注&lt;/b&gt;比较感兴趣，正在入坑。欢迎有同样兴趣的知友投稿讨论；&lt;/li&gt;&lt;li&gt;想要加入翻译小组的同学，&lt;b&gt;请连续3次在评论中对我们最新的翻译做出认真的批评和指正，而后我们会小组内投票决定是否吸纳新成员：）&lt;/b&gt;这个小小的门槛是为了方便我们找到真正喜爱机器学习和翻译的同学。&lt;/li&gt;&lt;/ol&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22230074&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Tue, 13 Sep 2016 19:09:57 GMT</pubDate></item><item><title>智能单元专栏目录</title><link>https://zhuanlan.zhihu.com/p/22339097</link><description>&lt;a href="https://zhuanlan.zhihu.com/intelligentunit" class="" data-editable="true" data-title="智能单元 - 知乎专栏"&gt;智能单元 - 知乎专栏&lt;/a&gt;长期原创和翻译&lt;b&gt;深度学习和深度增强学习&lt;/b&gt;等领域高质量文章，并&lt;b&gt;接受知友投稿&lt;/b&gt;，投稿前请阅读&lt;a href="https://zhuanlan.zhihu.com/p/21917736?refer=intelligentunit" data-editable="true" data-title="专栏投稿说明" class=""&gt;投稿说明&lt;/a&gt;。&lt;h2&gt;最前沿系列&lt;/h2&gt;&lt;blockquote&gt;最前沿系列解读我们认为的深度学习领域有巨大影响的论文和成果。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21320865?refer=intelligentunit" class="" data-editable="true" data-title="最前沿：史蒂夫的人工智能大挑战 - 智能单元 - 知乎专栏"&gt;最前沿：史蒂夫的人工智能大挑战&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21362413?refer=intelligentunit" class="" data-editable="true" data-title="最前沿：让计算机学会学习Let Computers Learn to Learn - 智能单元 - 知乎专栏"&gt;最前沿：让计算机学会学习Let Computers Learn to Learn&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21470871?refer=intelligentunit" data-editable="true" data-title="最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能" class=""&gt;最前沿：从虚拟到现实，迁移深度增强学习让机器人革命成为可能&lt;/a&gt;！&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22143664?refer=intelligentunit" data-editable="true" data-title="最前沿：深度学习训练方法大革新，反向传播训练不再唯一 - 智能单元 - 知乎专栏"&gt;最前沿：深度学习训练方法大革新，反向传播训练不再唯一 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22523121?refer=intelligentunit" data-editable="true" data-title="最前沿：深度增强学习再发力，家用机器人已近在眼前" class=""&gt;最前沿：深度增强学习再发力，家用机器人已近在眼前&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22604627?refer=intelligentunit" data-editable="true" data-title="最前沿：围棋之后，AI玩FPS游戏也能秀人类一脸了！ " class=""&gt;最前沿：围棋之后，AI玩FPS游戏也能秀人类一脸了！ &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22758556?refer=intelligentunit" data-editable="true" data-title="最前沿 之 谷歌的协作机械臂 " class=""&gt;最前沿：谷歌的协作机械臂 &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;CS231n课程翻译系列&lt;/h2&gt;&lt;blockquote&gt;斯坦度CS231n：面向视觉识别的卷积神经网络，入门深度学习利器。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21930884?refer=intelligentunit" class="" data-editable="true" data-title="贺完结！CS231n官方笔记授权翻译总集篇发布"&gt;贺完结！CS231n官方笔记授权翻译总集篇发布&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20870307?refer=intelligentunit" class="" data-editable="true" data-title="获得授权翻译斯坦福CS231n课程笔记系列"&gt;获得授权翻译斯坦福CS231n课程笔记系列&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20878530?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：Python Numpy教程" class=""&gt;CS231n课程笔记翻译：Python Numpy教程&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20894041?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：图像分类笔记（上）" class=""&gt;CS231n课程笔记翻译：图像分类笔记（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20900216?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：图像分类笔记（下） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：图像分类笔记（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20918580?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：线性分类笔记（上） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：线性分类笔记（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20945670?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：线性分类笔记（中）" class=""&gt;CS231n课程笔记翻译：线性分类笔记（中）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：线性分类笔记（下） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：线性分类笔记（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21354230?refer=intelligentunit" class="" data-editable="true" data-title="知友智靖远关于CS231n课程字幕翻译的倡议 "&gt;知友智靖远关于CS231n课程字幕翻译的倡议 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21360434?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：最优化笔记（上）" class=""&gt;CS231n课程笔记翻译：最优化笔记（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21387326?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：最优化笔记（下） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：最优化笔记（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21407711?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：反向传播笔记 - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：反向传播笔记 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21441838?refer=intelligentunit" data-editable="true" data-title="斯坦福CS231n课程作业# 1简介 - 智能单元 - 知乎专栏"&gt;斯坦福CS231n课程作业# 1简介 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记1（上）" class=""&gt;CS231n课程笔记翻译：神经网络笔记1（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21513367?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记1（下） - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：神经网络笔记1（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21560667?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记 2 - 智能单元 - 知乎专栏"&gt;CS231n课程笔记翻译：神经网络笔记 2 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21741716?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记3（上）" class=""&gt;CS231n课程笔记翻译：神经网络笔记3（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21798784?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：神经网络笔记3（下）" class=""&gt;CS231n课程笔记翻译：神经网络笔记3（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21941485?refer=intelligentunit" data-editable="true" data-title="斯坦福CS231n课程作业# 2简介 " class=""&gt;斯坦福CS231n课程作业# 2简介 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22038289?refer=intelligentunit" data-editable="true" data-title="CS231n课程笔记翻译：卷积神经网络笔记 - 猴子的文章 - 知乎专栏"&gt;CS231n课程笔记翻译：卷积神经网络笔记 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21946525?refer=intelligentunit" class="" data-editable="true" data-title="斯坦福CS231n课程作业# 3简介 - 智能单元 - 知乎专栏"&gt;斯坦福CS231n课程作业# 3简介 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22282421?refer=intelligentunit" data-editable="true" data-title="Andrej Karpathy的回信和Quora活动邀请 - 智能单元 - 知乎专栏"&gt;Andrej Karpathy的回信和Quora活动邀请&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22232836?refer=intelligentunit" data-editable="true" data-title="知行合一码作业，深度学习真入门 - 智能单元 - 知乎专栏"&gt;知行合一码作业，深度学习真入门 &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;深度增强学习系列&lt;/h2&gt;&lt;blockquote&gt;分享深度增强学习相关的资料，文章，代码及教程。&lt;/blockquote&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20885568?refer=intelligentunit" class="" data-editable="true" data-title="Deep Reinforcement Learning 深度增强学习资源 (持续更新） - 智能单元 - 知乎专栏"&gt;Deep Reinforcement Learning 深度增强学习资源 (持续更新） &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20893777?refer=intelligentunit" data-editable="true" data-title="深度解读AlphaGo " class=""&gt;深度解读AlphaGo &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/20924929?refer=intelligentunit" data-editable="true" data-title="从OpenAI看深度学习研究前沿 - 智能单元 - 知乎专栏"&gt;从OpenAI看深度学习研究前沿 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21262246?refer=intelligentunit" data-editable="true" data-title="DQN 从入门到放弃1 DQN与增强学习 - 智能单元 - 知乎专栏"&gt;DQN 从入门到放弃1 DQN与增强学习&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21263408?refer=intelligentunit" data-editable="true" data-title="你是这样获取人工智能AI前沿信息的吗？ - 智能单元 - 知乎专栏"&gt;你是这样获取人工智能AI前沿信息的吗？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21296798?refer=intelligentunit" class="" data-editable="true" data-title="解密Google Deepmind AlphaGo围棋算法：真人工智能来自于哪里？ - 智能单元 - 知乎专栏"&gt;解密Google Deepmind AlphaGo围棋算法：真人工智能来自于哪里？&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21292697?refer=intelligentunit" data-editable="true" data-title="DQN 从入门到放弃2 增强学习与MDP - 智能单元 - 知乎专栏"&gt;DQN 从入门到放弃2 增强学习与MDP &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21340755?refer=intelligentunit" data-editable="true" data-title="DQN 从入门到放弃3 价值函数与Bellman方程 " class=""&gt;DQN 从入门到放弃3 价值函数与Bellman方程 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit" class="" data-editable="true" data-title="DQN 从入门到放弃4 动态规划与Q-Learning - 智能单元 - 知乎专栏"&gt;DQN 从入门到放弃4 动态规划与Q-Learning &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21421729?refer=intelligentunit" data-editable="true" data-title="DQN从入门到放弃5 深度解读DQN算法 - 智能单元 - 知乎专栏"&gt;DQN从入门到放弃5 深度解读DQN算法 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21434933?refer=intelligentunit" data-editable="true" data-title="DQN实战篇1 从零开始安装Ubuntu, Cuda, Cudnn, Tensorflow, OpenAI Gym " class=""&gt;DQN实战篇1 从零开始安装Ubuntu, Cuda, Cudnn, Tensorflow, OpenAI Gym &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21477488?refer=intelligentunit" data-editable="true" data-title="150行代码实现DQN算法玩CartPole " class=""&gt;150行代码实现DQN算法玩CartPole &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21547911?refer=intelligentunit" data-editable="true" data-title="DQN从入门到放弃6 DQN的各种改进 - 智能单元 - 知乎专栏"&gt;DQN从入门到放弃6 DQN的各种改进 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21609472?refer=intelligentunit" data-editable="true" data-title="DQN从入门到放弃7 连续控制DQN算法-NAF " class=""&gt;DQN从入门到放弃7  连续控制DQN算法-NAF &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21725498?refer=intelligentunit" data-editable="true" data-title="深度增强学习之Policy Gradient方法1 - 智能单元 - 知乎专栏"&gt;深度增强学习之Policy Gradient方法1 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21369441?refer=intelligentunit" data-editable="true" data-title="深度增强学习暑期学校 PPT 详解1 - 智能单元 - 知乎专栏"&gt;深度增强学习暑期学校 PPT 详解1 &lt;/a&gt;&lt;/li&gt;&lt;h2&gt;原创系列&lt;/h2&gt;&lt;blockquote&gt;共同学习的翻译小组奉上值得一翻的文章。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22107715?refer=intelligentunit" data-editable="true" data-title="[原创翻译]循环神经网络惊人的有效性（上）" class=""&gt;[原创翻译]循环神经网络惊人的有效性（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22230074?refer=intelligentunit" data-editable="true" data-title="[原创翻译]循环神经网络惊人的有效性（下）" class=""&gt;[原创翻译]循环神经网络惊人的有效性（下）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22408033?refer=intelligentunit" data-editable="true" data-title="看图说话的AI小朋友——图像标注趣谈（上） - 智能单元 - 知乎专栏"&gt;看图说话的AI小朋友——图像标注趣谈（上）&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22520434?refer=intelligentunit" data-editable="true" data-title="看图说话的AI小朋友——图像标注趣谈（下） - 智能单元 - 知乎专栏" class=""&gt;看图说话的AI小朋友——图像标注趣谈（下）&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;h2&gt;知友投稿系列&lt;/h2&gt;&lt;blockquote&gt;欢迎知友投稿，请看投稿说明。&lt;/blockquote&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21917736?refer=intelligentunit" data-editable="true" data-title="智能单元专栏投稿说明" class=""&gt;智能单元专栏投稿说明&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21341440?refer=intelligentunit" class="" data-editable="true" data-title="「无中生有」计算机视觉探奇 - 欲穷千里目 - 知乎专栏"&gt;「无中生有」计算机视觉探奇 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22308032?refer=intelligentunit" data-editable="true" data-title="关于图像语义分割的总结和感悟 - 困兽的文章 - 知乎专栏"&gt;关于图像语义分割的总结和感悟 &lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/22457562?refer=intelligentunit" data-editable="true" data-title="逻辑与神经之间的桥 - 甄景贤的文章 - 知乎专栏"&gt;逻辑与神经之间的桥 &lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;END&lt;/p&gt;&lt;img rel="noreferrer" src="https://ga-beacon.appspot.com/UA-41015557-4/page-name?dt=https://zhuanlan.zhihu.com/p/22339097&amp;pixel&amp;useReferer"/&gt;</description><author>杜客</author><pubDate>Sun, 11 Sep 2016 21:13:02 GMT</pubDate></item></channel></rss>